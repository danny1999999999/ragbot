#!/usr/bin/env python3
"""
ğŸ“¦ 4Bæª”æ¡ˆ: management_interface.py (ç®¡ç†ä»‹é¢å±¤)
ğŸ¯ è·è²¬ï¼šç³»çµ±ç®¡ç†APIã€ç”¨æˆ¶æ¥å£ã€é«˜ç´šç®¡ç†åŠŸèƒ½
ğŸ”— ä¾è³´ï¼švector_operations.py (4Aæª”æ¡ˆ)
ğŸ“Š åŒ…å«ï¼šOptimizedVectorSystemé¡åˆ¥ (45å€‹æ–¹æ³•)
"""

import time
import json
import logging
import os
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime

# ğŸ”— ä¾è³´4Aæª”æ¡ˆï¼šå‘é‡æ“ä½œæ ¸å¿ƒå±¤
try:
    from vector_operations import VectorOperationsCore
    print("âœ… æˆåŠŸå°å…¥ VectorOperationsCore å¾ vector_operations.py")
except ImportError as e:
    print(f"âŒ ç„¡æ³•å°å…¥ vector_operations.py: {e}")
    print("ğŸ“‹ è«‹ç¢ºä¿ vector_operations.py (4Aæª”æ¡ˆ) å­˜åœ¨ä¸”å¯æ­£å¸¸å°å…¥")
    raise ImportError("4Bæª”æ¡ˆä¾è³´4Aæª”æ¡ˆï¼Œè«‹å…ˆç¢ºä¿ vector_operations.py å¯æ­£å¸¸ä½¿ç”¨")

# ğŸ”— å˜—è©¦å°å…¥é…ç½® (å¾4Aæª”æ¡ˆç²å–)
try:
    # å¾å·²å°å…¥çš„ vector_operations ç²å–æ‰€æœ‰å¿…è¦é…ç½®
    import vector_operations as vo_module
    
    # å‹•æ…‹ç²å–é…ç½®è®Šæ•¸
    SYSTEM_CONFIG = getattr(vo_module, 'SYSTEM_CONFIG', {})
    TOKEN_LIMITS = getattr(vo_module, 'TOKEN_LIMITS', {})
    SUPPORTED_EXTENSIONS = getattr(vo_module, 'SUPPORTED_EXTENSIONS', set())
    
    # ç²å–ä¾è³´æª¢æŸ¥è®Šæ•¸
    PGVECTOR_AVAILABLE = getattr(vo_module, 'PGVECTOR_AVAILABLE', False)
    OPENAI_EMBEDDINGS_AVAILABLE = getattr(vo_module, 'OPENAI_EMBEDDINGS_AVAILABLE', False)
    
    # ç²å–è³‡æ–™é¡åˆ¥
    FileInfo = getattr(vo_module, 'FileInfo', None)
    SearchResult = getattr(vo_module, 'SearchResult', None)
    
    print("âœ… æˆåŠŸå¾4Aæª”æ¡ˆç²å–æ‰€æœ‰å¿…è¦é…ç½®")
    
except Exception as e:
    print(f"âš ï¸ å¾4Aæª”æ¡ˆç²å–é…ç½®æ™‚å‡ºç¾å•é¡Œ: {e}")
    # æä¾›æœ€å°å¿…è¦é…ç½®ä½œç‚ºå›é€€
    SUPPORTED_EXTENSIONS = {'.txt', '.md', '.pdf', '.csv', '.json', '.py'}
    PGVECTOR_AVAILABLE = False
    OPENAI_EMBEDDINGS_AVAILABLE = False

logger = logging.getLogger(__name__)

class OptimizedVectorSystem(VectorOperationsCore):
    """
    ğŸ“¦ å®Œæ•´çš„å‘é‡ç³»çµ±ç®¡ç†ä»‹é¢
    ğŸ”— ç¹¼æ‰¿è‡ª VectorOperationsCore (4Aæª”æ¡ˆ)
    ğŸ“Š æä¾›45å€‹ç®¡ç†APIæ–¹æ³•ï¼Œå®Œæ•´å‘å¾Œå…¼å®¹
    """
    
    def __init__(self, data_dir: str = None, model_type: str = None):
        """ğŸ”§ åˆå§‹åŒ–ç®¡ç†ä»‹é¢ - ç¹¼æ‰¿4Aæª”æ¡ˆçš„æ ¸å¿ƒåŠŸèƒ½"""
        try:
            # èª¿ç”¨çˆ¶é¡åˆ¥åˆå§‹åŒ– (4Aæª”æ¡ˆ)
            super().__init__(data_dir, model_type)
            print("ğŸ“Š ç®¡ç†ä»‹é¢å±¤åˆå§‹åŒ–å®Œæˆ")
            print("   ğŸ” æœç´¢æœå‹™: âœ…")  
            print("   ğŸ“¤ æ–‡ä»¶ä¸Šå‚³: âœ…")
            print("   ğŸ“‹ æ–‡æª”æŸ¥è©¢: âœ…")
            print("   ğŸ—‘ï¸ åˆªé™¤ç®¡ç†: âœ…")
            print("   ğŸ“Š çµ±è¨ˆè¨ºæ–·: âœ…")
            
        except Exception as e:
            logger.error(f"ç®¡ç†ä»‹é¢åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    # ==================== ğŸ” æœç´¢åŠŸèƒ½ç›¸é—œ (1å€‹æ–¹æ³•) ====================
    
    def search(self, query: str, collection_name: str = None, k: int = 5) -> List[Dict]:
        """ğŸ” å„ªåŒ–ç‰ˆæœç´¢ - æ”¯æ´å¤šé›†åˆã€æŸ¥è©¢è®Šé«”ã€å»é‡æ’åº"""
        try:
            # å‰µå»ºæœç´¢è®Šé«”
            query_variants = []
            if hasattr(self, 'normalizer') and hasattr(self.normalizer, 'create_search_variants'):
                query_variants = self.normalizer.create_search_variants(query)
            else:
                query_variants = [query]  # å›é€€åˆ°åŸæŸ¥è©¢
            
            all_results = []
            
            # è™•ç†é›†åˆç¯„åœ
            target_collections = []
            if collection_name:
                target_collections = [collection_name]
            else:
                stats = self.get_stats()
                target_collections = [f"collection_{name}" for name in stats.keys()]
            
            # å°æ¯å€‹é›†åˆå’ŒæŸ¥è©¢è®Šé«”é€²è¡Œæœç´¢
            for variant in query_variants:
                for coll_name in target_collections:
                    try:
                        vectorstore = self.get_or_create_vectorstore(coll_name)
                        docs_and_scores = vectorstore.similarity_search_with_score(variant, k=k)
                        
                        for doc, score in docs_and_scores:
                            # å‰µå»ºæœç´¢çµæœ
                            result = {
                                "content": doc.page_content[:500],  # é™åˆ¶é è¦½é•·åº¦
                                "score": 1.0 - score,  # è½‰æ›ç‚ºç›¸ä¼¼åº¦åˆ†æ•¸
                                "metadata": doc.metadata,
                                "collection": coll_name,
                                "chunk_info": {
                                    'chunk_id': doc.metadata.get('chunk_id', 'unknown'),
                                    'content': doc.page_content,
                                    'metadata': doc.metadata,
                                    'token_count': doc.metadata.get('token_count', 0),
                                    'quality_score': doc.metadata.get('quality_score', 0.5)
                                }
                            }
                            all_results.append(result)
                            
                    except Exception as e:
                        logger.warning(f"æœç´¢é›†åˆå¤±æ•— {coll_name}: {e}")
            
            # å»é‡å’Œæ’åº
            seen_content = set()
            unique_results = []
            
            for result in sorted(all_results, key=lambda x: x["score"], reverse=True):
                content_hash = hashlib.md5(result["content"].encode()).hexdigest()
                if content_hash not in seen_content:
                    seen_content.add(content_hash)
                    unique_results.append(result)
            
            return unique_results[:k]
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±æ•—: {e}")
            return []
    
    # ==================== ğŸ“¤ æ–‡ä»¶ä¸Šå‚³ç›¸é—œ (1å€‹æ–¹æ³•) ====================
    
    def upload_single_file(self, file_content: bytes, filename: str, collection_name: str) -> Dict:
        """ğŸ“¤ ç´”PostgreSQLæ–¹æ¡ˆï¼šç›´æ¥è™•ç†æ–‡ä»¶å…§å®¹ï¼Œä¸ä¿å­˜åˆ°æœ¬åœ°"""
        try:
            # åŸºæœ¬é©—è­‰
            if not file_content:
                return {"success": False, "message": "æ–‡ä»¶å…§å®¹ç‚ºç©º", "chunks": []}
            
            if not filename or not filename.strip():
                return {"success": False, "message": "æ–‡ä»¶åä¸èƒ½ç‚ºç©º", "chunks": []}
            
            # æª¢æŸ¥æ–‡ä»¶æ“´å±•å
            file_extension = Path(filename).suffix.lower()
            if file_extension not in SUPPORTED_EXTENSIONS:
                return {
                    "success": False,
                    "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}ã€‚æ”¯æŒæ ¼å¼: {', '.join(SUPPORTED_EXTENSIONS)}",
                    "chunks": []
                }
            
            print(f"ğŸ“„ ç´”PostgreSQLæ–¹æ¡ˆï¼šç›´æ¥è™•ç†æ–‡ä»¶å…§å®¹ {filename}")
            
            # âœ… ä½¿ç”¨è‡¨æ™‚æ–‡ä»¶è™•ç†
            import tempfile
            with tempfile.NamedTemporaryFile(suffix=file_extension, delete=False) as temp_file:
                temp_file.write(file_content)
                temp_file_path = Path(temp_file.name)
            
            try:
                # è¼‰å…¥æ–‡æª”
                documents = self.load_document(temp_file_path)
                
                if not documents:
                    return {"success": False, "message": "æ–‡ä»¶å…§å®¹ç‚ºç©ºæˆ–æ ¼å¼ä¸æ”¯æŒ", "chunks": []}
                
                # è¨­ç½®å…ƒæ•¸æ“š
                current_timestamp = time.time()
                for doc in documents:
                    doc.metadata.update({
                        'collection': collection_name,
                        'original_filename': filename,
                        'filename': filename,  # âœ… ç¢ºä¿å…©å€‹æ¬„ä½éƒ½æœ‰
                        'upload_timestamp': current_timestamp,
                        'file_source': 'upload',
                        'uploaded_by': 'upload_interface',
                        'source': f"postgresql://{collection_name}/{filename}",  # âœ… è™›æ“¬PostgreSQLè·¯å¾‘
                        'file_extension': file_extension,
                        'stored_in': 'postgresql_only',  # âœ… æ¨™è¨˜åƒ…å­˜æ–¼PostgreSQL
                        'file_size': len(file_content)
                    })
                
                # å‘é‡åŒ–è™•ç†
                vectorstore = self.get_or_create_vectorstore(collection_name)
                
                # âœ… å…ˆå¾¹åº•åˆªé™¤å·²å­˜åœ¨çš„åŒåæ–‡ä»¶
                print(f"ğŸ—‘ï¸ æ¸…ç†ç¾æœ‰æ–‡ä»¶: {filename}")
                try:
                    self._postgresql_delete_file_completely(vectorstore, filename)
                except Exception as e:
                    print(f"âš ï¸ æ¸…ç†ç¾æœ‰æ–‡ä»¶æ™‚å‡ºç¾è­¦å‘Š: {e}")
                
                # æ‰¹æ¬¡è™•ç†
                print(f"ğŸ“„ é–‹å§‹å‘é‡åŒ–è™•ç†...")
                batches = self.batch_processor.create_smart_batches(documents)
                success_count = self._process_batches(vectorstore, batches)
                
                print(f"âœ… æ–‡ä»¶ä¸Šå‚³å®Œæˆ: {filename}")
                print(f"   ğŸ  ä¿å­˜ä½ç½®: PostgreSQL ({collection_name})")
                print(f"   ğŸ“„ åˆ†å¡Šæ•¸é‡: {len(documents)}")
                print(f"   âœ… æˆåŠŸå‘é‡åŒ–: {success_count}")
                
                return {
                    "success": True,
                    "message": f"æ–‡ä»¶ä¸Šå‚³æˆåŠŸï¼Œåƒ…ä¿å­˜åˆ°PostgreSQLï¼Œå…±ç”Ÿæˆ{len(documents)}å€‹åˆ†å¡Š",
                    "filename": filename,
                    "collection": collection_name,
                    "total_chunks": len(documents),
                    "success_chunks": success_count,
                    "upload_time": current_timestamp,
                    "storage_location": "postgresql_only"
                }
                
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                if temp_file_path.exists():
                    temp_file_path.unlink()
                    print(f"ğŸ§¹ æ¸…ç†è‡¨æ™‚æ–‡ä»¶: {temp_file_path}")
                    
        except Exception as e:
            logger.error(f"æ–‡ä»¶ä¸Šå‚³å¤±æ•— {filename}: {e}")
            return {"success": False, "message": f"æ–‡ä»¶ä¸Šå‚³å¤±æ•—: {str(e)}", "chunks": []}
    
    # ==================== ğŸ“‹ æ–‡æª”æŸ¥è©¢ç›¸é—œ (7å€‹æ–¹æ³•) ====================
    
    def get_collection_documents(self, collection_name: str, page: int = 1, limit: int = 20, search: str = "") -> Dict:
        """ğŸ“‹ ç²å–é›†åˆä¸­çš„æª”æ¡ˆè³‡è¨Š - å…¼å®¹Chromaå’ŒPGVector"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            if self.use_postgres:
                return self._get_documents_from_pgvector(vectorstore, collection_name, page, limit, search)
            else:
                return self._get_documents_from_chroma(vectorstore, collection_name, page, limit, search)
                
        except Exception as e:
            logger.error(f"ç²å–æª”æ¡ˆåˆ—è¡¨å¤±æ•—: {e}", exc_info=True)
            return {"success": False, "error": str(e), "documents": [], "total": 0, "page": page, "limit": limit, "total_pages": 0}
        
    def _get_documents_from_chroma(self, vectorstore, collection_name: str, page: int, limit: int, search: str) -> Dict:
        """ğŸ“‹ å¾Chromaç²å–æª”æ¡ˆ - åŸæœ‰é‚è¼¯"""
        all_docs_raw = vectorstore.get()
        if not all_docs_raw or not all_docs_raw.get('metadatas'):
            return {"success": True, "documents": [], "total": 0, "page": page, "limit": limit, "total_pages": 0}

        file_stats = {}
        for metadata in all_docs_raw.get('metadatas', []):
            try:
                filename = metadata.get('original_filename', metadata.get('filename', 'unknown_file'))
                if filename == 'unknown_file': continue

                if filename not in file_stats:
                    file_stats[filename] = {
                        'filename': filename,
                        'source': metadata.get('source', 'unknown'),
                        'total_chunks': 0,
                        'upload_time': metadata.get('upload_timestamp', 0)
                    }
                file_stats[filename]['total_chunks'] += 1
            except Exception:
                continue

        safe_documents = list(file_stats.values())
        
        # æ·»åŠ æ ¼å¼åŒ–æ™‚é–“
        for doc in safe_documents:
            try:
                doc['upload_time_formatted'] = datetime.fromtimestamp(doc['upload_time']).strftime('%Y-%m-%d %H:%M:%S') if doc['upload_time'] else 'N/A'
            except:
                doc['upload_time_formatted'] = 'Invalid Date'

        # éæ¿¾å’Œåˆ†é 
        if search:
            safe_documents = [doc for doc in safe_documents if search.lower() in doc['filename'].lower()]
        
        safe_documents.sort(key=lambda x: x.get('upload_time', 0), reverse=True)
        total = len(safe_documents)
        total_pages = (total + limit - 1) // limit if total > 0 else 1
        start = (page - 1) * limit
        end = start + limit
        page_documents = safe_documents[start:end]

        return {"success": True, "documents": page_documents, "total": total, "page": page, "limit": limit, "total_pages": total_pages}

    def _get_documents_from_pgvector(self, vectorstore, collection_name: str, page: int, limit: int, search: str) -> Dict:
        """ğŸ“‹ ç´”PostgreSQLæ–¹æ¡ˆï¼šå®Œå…¨ä¸ä¾è³´æœ¬åœ°è¨˜éŒ„ï¼Œä¿®æ­£å‰ç«¯æ¬„ä½åŒ¹é…"""
        try:
            print(f"ğŸ” ç´”PostgreSQLç²å–{collection_name}çš„æª”æ¡ˆåˆ—è¡¨")
            
            # âœ… ç›´æ¥æŸ¥è©¢PostgreSQLä¸­çš„æ‰€æœ‰æ–‡æª”
            docs = vectorstore.similarity_search("", k=2000)
            
            if not docs:
                return {"success": True, "documents": [], "total": 0, "page": page, "limit": limit, "total_pages": 0}
            
            # âœ… æŒ‰æª”æ¡ˆååˆ†çµ„çµ±è¨ˆ
            file_stats = {}
            
            for doc in docs:
                metadata = doc.metadata
                filename = metadata.get('original_filename') or metadata.get('filename', 'unknown')
                
                if filename == 'unknown':
                    continue
                
                if filename not in file_stats:
                    # âœ… ä½¿ç”¨å‰ç«¯æœŸæœ›çš„æ¬„ä½åç¨±
                    file_stats[filename] = {
                        'filename': filename,
                        'source': metadata.get('source', f'postgresql://{collection_name}'),
                        'chunks': 0,  # âœ… å‰ç«¯æœŸæœ›çš„æ¬„ä½å(ä¸æ˜¯total_chunks)
                        'upload_time': metadata.get('upload_timestamp', 0),
                        'uploader': 'æœªçŸ¥',  # âœ… å‰ç«¯æœŸæœ›çš„æ¬„ä½å(ä¸æ˜¯uploaded_by)
                        'upload_time_formatted': 'æœªçŸ¥',
                        'file_extension': metadata.get('file_extension', ''),
                        'stored_in': 'postgresql'
                    }
                
                file_stats[filename]['chunks'] += 1
                
                # âœ… æ›´æ–°ä¸Šå‚³è€…ä¿¡æ¯ï¼ˆä½¿ç”¨æœ€æ–°çš„è¨˜éŒ„ï¼‰
                uploader_info = metadata.get('uploaded_by') or metadata.get('file_source', 'unknown')
                if uploader_info and uploader_info != 'unknown':
                    if uploader_info == 'upload_interface' or uploader_info == 'upload':
                        file_stats[filename]['uploader'] = 'ç®¡ç†ä»‹é¢'
                    elif uploader_info == 'sync':
                        file_stats[filename]['uploader'] = 'åŒæ­¥'
                    else:
                        file_stats[filename]['uploader'] = str(uploader_info)
            
            # âœ… å¾Œè™•ç†ï¼šæ ¼å¼åŒ–æ™‚é–“
            for filename, stats in file_stats.items():
                try:
                    if stats['upload_time'] and stats['upload_time'] > 0:
                        stats['upload_time_formatted'] = datetime.fromtimestamp(stats['upload_time']).strftime('%Y-%m-%d %H:%M:%S')
                    else:
                        stats['upload_time_formatted'] = 'æœªçŸ¥'
                except Exception as e:
                    logger.warning(f"æ™‚é–“æ ¼å¼åŒ–å¤±æ•—{filename}: {e}")
                    stats['upload_time_formatted'] = 'æœªçŸ¥'
                
                # âœ… ç¢ºä¿ä¸Šå‚³è€…ä¸æ˜¯ç©ºå€¼
                if not stats['uploader'] or stats['uploader'] in ['unknown', 'None', '']:
                    stats['uploader'] = 'æœªçŸ¥'
            
            safe_documents = list(file_stats.values())
            
            # éæ¿¾å’Œåˆ†é 
            if search:
                safe_documents = [doc for doc in safe_documents if search.lower() in doc['filename'].lower()]
            
            safe_documents.sort(key=lambda x: x.get('upload_time', 0), reverse=True)
            
            total = len(safe_documents)
            total_pages = (total + limit - 1) // limit if total > 0 else 1
            start = (page - 1) * limit
            end = start + limit
            page_documents = safe_documents[start:end]
            
            print(f"âœ… ç´”PostgreSQLç²å–æˆåŠŸ: {total}å€‹æª”æ¡ˆ")
            
            # âœ… èª¿è©¦ï¼šæ‰“å°å‰å¹¾å€‹æ–‡ä»¶çš„æ ¼å¼
            if page_documents:
                sample = page_documents[0]
                print(f"ğŸ“‹ è¿”å›æ•¸æ“šæ ¼å¼ç¯„ä¾‹:")
                print(f"   filename: {sample.get('filename')}")
                print(f"   chunks: {sample.get('chunks')}")
                print(f"   uploader: {sample.get('uploader')}")
                print(f"   upload_time_formatted: {sample.get('upload_time_formatted')}")
            
            return {
                "success": True,
                "documents": page_documents,
                "total": total,
                "page": page,
                "limit": limit,
                "total_pages": total_pages
            }
            
        except Exception as e:
            logger.error(f"PostgreSQLæŸ¥è©¢å¤±æ•—: {e}")
            return {"success": False, "error": str(e), "documents": [], "total": 0, "page": page, "limit": limit, "total_pages": 0}
    
    def get_document_chunks(self, collection_name: str, source_file: str) -> List[Dict]:
        """ğŸ“‹ ç²å–æŒ‡å®šæª”æ¡ˆçš„æ‰€æœ‰åˆ†å¡Š - å…¼å®¹Chromaå’ŒPGVector"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            if self.use_postgres:
                return self._get_chunks_from_pgvector(vectorstore, collection_name, source_file)
            else:
                return self._get_chunks_from_chroma(vectorstore, source_file)
                
        except Exception as e:
            logger.error(f"ç²å–æª”æ¡ˆåˆ†å¡Šå¤±æ•—{collection_name}/{source_file}: {e}")
            return []

    def _get_chunks_from_chroma(self, vectorstore, source_file: str) -> List[Dict]:
        """ğŸ“‹ å¾Chromaç²å–åˆ†å¡Š"""
        try:
            results = vectorstore.get(where={"filename": source_file})
            if not results or not results.get('documents'):
                return []
            
            chunks = []
            for i, (doc, metadata) in enumerate(zip(results['documents'], results['metadatas'])):
                chunk_info = {
                    'chunk_id': metadata.get('chunk_id', f'chunk_{i+1}'),
                    'chunk_index': metadata.get('chunk_index', i),
                    'content': doc,
                    'content_preview': doc[:200] + "..." if len(doc) > 200 else doc,
                    'token_count': metadata.get('token_count', 0),
                    'text_type': metadata.get('text_type', 'unknown'),
                    'quality_score': metadata.get('quality_score', 0.5),
                    'metadata': metadata
                }
                chunks.append(chunk_info)
            
            chunks.sort(key=lambda x: x.get('chunk_index', 0))
            return chunks
            
        except Exception as e:
            logger.error(f"Chromaåˆ†å¡Šç²å–å¤±æ•—: {e}")
            return []

    def _get_chunks_from_pgvector(self, vectorstore, collection_name: str, source_file: str) -> List[Dict]:
        """ğŸ“‹ å¾PGVectorç²å–åˆ†å¡Š"""
        try:
            collection_folder = collection_name.replace('collection_', '')
            possible_paths = [
                source_file,
                f"data/{collection_folder}/{source_file}",
                f"data\\{collection_folder}\\{source_file}"
            ]
            
            all_chunks = []
            
            for search_path in possible_paths:
                try:
                    docs = vectorstore.similarity_search("", k=1000)
                    matching_chunks = []
                    
                    for doc in docs:
                        metadata = doc.metadata
                        doc_filename = metadata.get('filename', metadata.get('original_filename', ''))
                        doc_source = metadata.get('source', '')
                        
                        if (
                            doc_filename == source_file or 
                            doc_source.endswith(source_file) or
                            search_path in doc_source
                        ):
                            
                            chunk_info = {
                                'chunk_id': metadata.get('chunk_id', f'chunk_{len(matching_chunks)+1}'),
                                'chunk_index': metadata.get('chunk_index', len(matching_chunks)),
                                'content': doc.page_content,
                                'content_preview': doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                                'token_count': metadata.get('token_count', 0),
                                'text_type': metadata.get('text_type', 'unknown'),
                                'quality_score': metadata.get('quality_score', 0.5),
                                'metadata': metadata
                            }
                            matching_chunks.append(chunk_info)
                    
                    if matching_chunks:
                        all_chunks = matching_chunks
                        print(f"ğŸ¯ PGVectoræ‰¾åˆ°{len(all_chunks)}å€‹åˆ†å¡Š")
                        break
                        
                except Exception as search_error:
                    logger.warning(f"PGVectoræŸ¥è©¢å¤±æ•—: {search_error}")
                    continue
            
            all_chunks.sort(key=lambda x: x.get('chunk_index', 0))
            return all_chunks
            
        except Exception as e:
            logger.error(f"PGVectoråˆ†å¡Šç²å–å¤±æ•—: {e}")
            return []
    
    def get_chunk_content(self, collection_name: str, chunk_id: str) -> Optional[Dict]:
        """ğŸ“‹ ç²å–æŒ‡å®šåˆ†å¡Šçš„è©³ç´°å…§å®¹"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            # PGVector does not have a 'get' method, so we need to iterate
            if self.use_postgres:
                # This is not efficient, but it's a workaround for now.
                docs = vectorstore.similarity_search("", k=5000)
                for doc in docs:
                    if doc.metadata.get("chunk_id") == chunk_id:
                        # Found the chunk
                        content_stats = {
                            'total_chars': len(doc.page_content),
                            'total_lines': len(doc.page_content.split('\n')),
                            'total_words': len(doc.page_content.split()),
                            'total_sentences': len([s for s in doc.page_content.split('ã€‚') if s.strip()]) + len([s for s in doc.page_content.split('.') if s.strip()])
                        }
                        chunk_detail = {
                            'chunk_id': chunk_id,
                            'chunk_index': doc.metadata.get('chunk_index', 0),
                            'content': doc.page_content,
                            'content_stats': content_stats,
                            'token_count': doc.metadata.get('token_count', 0),
                            'text_type': doc.metadata.get('text_type', 'unknown'),
                            'quality_score': doc.metadata.get('quality_score', 0.5),
                            'language': doc.metadata.get('language', 'unknown'),
                            'source_file': doc.metadata.get('source', 'unknown'),
                            'original_filename': doc.metadata.get('original_filename', doc.metadata.get('filename', 'unknown')),
                            'processing_strategy': doc.metadata.get('processing_strategy', 'unknown'),
                            'split_method': doc.metadata.get('split_method', 'unknown'),
                            'has_overlap': doc.metadata.get('has_overlap', False),
                            'metadata': doc.metadata
                        }
                        return chunk_detail
                return None # Chunk not found
            else: # ChromaDB
                results = vectorstore.get(where={"chunk_id": chunk_id})
                
                if not results or not results.get('documents') or len(results['documents']) == 0:
                    return None
                
                # å–ç¬¬ä¸€å€‹çµæœï¼ˆchunk_idæ‡‰è©²æ˜¯å”¯ä¸€çš„ï¼‰
                doc = results['documents'][0]
                metadata = results['metadatas'][0]
                
                # ğŸ”§ è¨ˆç®—å­—ç¬¦å’Œè¡Œæ•¸çµ±è¨ˆ
                content_stats = {
                    'total_chars': len(doc),
                    'total_lines': len(doc.split('\n')),
                    'total_words': len(doc.split()),
                    'total_sentences': len([s for s in doc.split('ã€‚') if s.strip()]) + len([s for s in doc.split('.') if s.strip()])
                }
                
                chunk_detail = {
                    'chunk_id': chunk_id,
                    'chunk_index': metadata.get('chunk_index', 0),
                    'content': doc,
                    'content_stats': content_stats,
                    'token_count': metadata.get('token_count', 0),
                    'text_type': metadata.get('text_type', 'unknown'),
                    'quality_score': metadata.get('quality_score', 0.5),
                    'language': metadata.get('language', 'unknown'),
                    'source_file': metadata.get('source', 'unknown'),
                    'original_filename': metadata.get('original_filename', metadata.get('filename', 'unknown')),
                    'processing_strategy': metadata.get('processing_strategy', 'unknown'),
                    'split_method': metadata.get('split_method', 'unknown'),
                    'has_overlap': metadata.get('has_overlap', False),
                    'metadata': metadata
                }
                
                return chunk_detail
            
        except Exception as e:
            logger.error(f"ç²å–åˆ†å¡Šå…§å®¹å¤±æ•—{collection_name}/{chunk_id}: {e}")
            return None

    def get_chunk_by_id(self, collection_name: str, chunk_id: str) -> Optional[Dict]:
        """ğŸ“‹ é€šéIDç²å–åˆ†å¡Š (get_chunk_contentçš„åˆ¥å)"""
        return self.get_chunk_content(collection_name, chunk_id)
    
    # ==================== ğŸ—‘ï¸ åˆªé™¤åŠŸèƒ½ç›¸é—œ (8å€‹ä¸»è¦æ–¹æ³•) ====================
    
    def delete_by_file_ids_simple(self, collection_name: str, filename: str) -> Dict:
        """ğŸ—‘ï¸ ç°¡åŒ–ç‰ˆæª”æ¡ˆåˆªé™¤ - ç›´æ¥èª¿ç”¨å®Œæ•´ç‰ˆæœ¬"""
        return self.delete_by_file_ids(collection_name, filename)

    def delete_by_file_ids(self, collection_name: str, filename: str) -> Dict:
        """ğŸ—‘ï¸ ä¿®æ­£ç‰ˆï¼šç›´æ¥ä½¿ç”¨chunk_idsåˆªé™¤ - æ›´å¯é çš„æ–¹æ³•"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            # Step 1: æ‰¾åˆ°æ‰€æœ‰ç›¸é—œçš„chunk_idsï¼ˆæœ€å¯é çš„æ–¹æ³•ï¼‰
            chunk_ids = self._find_file_ids_corrected(vectorstore, filename).get("chunk_ids", [])
            
            if not chunk_ids:
                return {
                    "success": False, 
                    "message": f"File '{filename}' not found",
                    "deleted_chunks": 0
                }
            
            print(f"ğŸ¯ Found {len(chunk_ids)} chunks for file: {filename}")
            
            # Step 2: ä½¿ç”¨æ­£ç¢ºçš„PGVectorèªæ³•ç›´æ¥åˆªé™¤
            return self._delete_by_chunk_ids_fixed(vectorstore, chunk_ids, filename)
            
        except Exception as e:
            logger.error(f"Delete file failed: {e}")
            return {"success": False, "message": f"Delete failed: {str(e)}", "deleted_chunks": 0}

    # ==================== ğŸ—‘ï¸ åˆªé™¤åŠŸèƒ½ (é‡æ§‹å¾Œ) ====================

        # ==================== ğŸ—‘ï¸ åˆªé™¤åŠŸèƒ½ (é‡æ§‹å¾Œ) ====================

    def delete_by_file_ids(self, collection_name: str, filename: str) -> Dict:
        """ğŸ—‘ï¸ [é‡æ§‹] ç›´æ¥é€šéå…ƒæ•¸æ“šéæ¿¾å™¨å¾PGVectoræˆ–Chromaåˆªé™¤æ–‡ä»¶ã€‚

        é€™ç¨®æ–¹æ³•æ¯”å…ˆç²å–IDå†åˆªé™¤æ›´ç›´æ¥ã€æ›´å¯é ã€‚
        """
        # PGVector is the primary, Chroma is the fallback
        if self.use_postgres:
            return self._delete_from_pgvector_by_sql(collection_name, filename)
        else:
            return self._delete_from_chroma_by_filter(collection_name, filename)

    def _delete_from_chroma_by_filter(self, collection_name: str, filename: str) -> Dict:
        """å¾ChromaDBä¸­é€šéå…ƒæ•¸æ“šéæ¿¾å™¨åˆªé™¤"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            # å…ˆè¨ˆç®—æœ‰å¤šå°‘å€‹åŒ¹é…çš„å¡Šï¼Œä»¥ä¾¿å ±å‘Š
            existing_chunks = vectorstore.get(where={"filename": filename})['ids']
            if not existing_chunks:
                return {"success": True, "message": "æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç„¡éœ€åˆªé™¤", "deleted_chunks": 0}

            vectorstore.delete(where={"filename": filename})
            logger.info(f"âœ… [Chroma] æˆåŠŸç‚ºæ–‡ä»¶ '{filename}' ç™¼å‡ºåˆªé™¤è«‹æ±‚ã€‚")
            
            return {
                "success": True,
                "message": f"æ–‡ä»¶ '{filename}' åŠå…¶ {len(existing_chunks)} å€‹åˆ†å¡Šå·²æˆåŠŸåˆªé™¤ã€‚",
                "deleted_chunks": len(existing_chunks),
                "filename": filename
            }
        except Exception as e:
            logger.error(f"âŒ [Chroma] æ–‡ä»¶åˆªé™¤å¤±æ•— '{filename}': {e}", exc_info=True)
            return {"success": False, "message": f"Chromaåˆªé™¤å¤±æ•—: {e}", "deleted_chunks": 0}

    def _delete_from_pgvector_by_sql(self, collection_name: str, filename: str) -> Dict:
        """ğŸ—‘ï¸ [æ ¸å¿ƒ] ä½¿ç”¨SQLç›´æ¥å¾PGVectoråˆªé™¤ï¼Œé€™æ˜¯æœ€å¯é çš„æ–¹æ³•ã€‚"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            collection_id = self._get_pg_collection_id(vectorstore, collection_name)
            if not collection_id:
                return {"success": False, "message": f"æ‰¾ä¸åˆ°é›†åˆID: {collection_name}"}

            with vectorstore._connect() as conn:
                with conn.cursor() as cur:
                    # æŸ¥è©¢è¦åˆªé™¤çš„è¡Œæ•¸ (ç”¨æ–¼å ±å‘Š)
                    count_query = """
                        SELECT COUNT(*) FROM langchain_pg_embedding
                        WHERE collection_id = %s AND (cmetadata->>'filename' = %s OR cmetadata->>'original_filename' = %s);
                    """
                    cur.execute(count_query, (collection_id, filename, filename))
                    chunks_to_delete = cur.fetchone()[0]

                    if chunks_to_delete == 0:
                        logger.warning(f"[PGVector] åœ¨é›†åˆ '{collection_name}' ä¸­æ‰¾ä¸åˆ°æ–‡ä»¶ '{filename}' çš„è¨˜éŒ„ã€‚")
                        return {"success": True, "message": "æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç„¡éœ€åˆªé™¤", "deleted_chunks": 0}

                    logger.info(f"[PGVector] æº–å‚™å¾é›†åˆ '{collection_name}' ä¸­åˆªé™¤æ–‡ä»¶ '{filename}' çš„ {chunks_to_delete} å€‹åˆ†å¡Š...")

                    # åŸ·è¡Œåˆªé™¤
                    delete_query = """
                        DELETE FROM langchain_pg_embedding
                        WHERE collection_id = %s AND (cmetadata->>'filename' = %s OR cmetadata->>'original_filename' = %s);
                    """
                    cur.execute(delete_query, (collection_id, filename, filename))
                    deleted_count = cur.rowcount
                    conn.commit()

                    logger.info(f"âœ… [PGVector] æˆåŠŸåˆªé™¤ {deleted_count} å€‹åˆ†å¡Šã€‚")

            return {
                "success": True,
                "message": f"æ–‡ä»¶ '{filename}' åŠå…¶ {deleted_count} å€‹åˆ†å¡Šå·²æˆåŠŸåˆªé™¤ã€‚",
                "deleted_chunks": deleted_count,
                "filename": filename
            }

        except Exception as e:
            logger.error(f"âŒ [PGVector] SQLåˆªé™¤å¤±æ•— '{filename}': {e}", exc_info=True)
            if 'conn' in locals() and conn:
                conn.rollback()
            return {"success": False, "message": f"è³‡æ–™åº«åˆªé™¤æ“ä½œå¤±æ•—: {e}", "deleted_chunks": 0}

    def _get_pg_collection_id(self, vectorstore, collection_name: str) -> Optional[str]:
        """ç²å–çµ¦å®šé›†åˆåç¨±çš„UUIDã€‚"""
        try:
            with vectorstore._connect() as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        "SELECT uuid FROM langchain_pg_collection WHERE name = %s;",
                        (collection_name,)
                    )
                    result = cur.fetchone()
                    return result[0] if result else None
        except Exception as e:
            logger.error(f"ç²å–é›†åˆIDå¤±æ•— '{collection_name}': {e}")
            return None

    def _delete_from_chroma_by_filter(self, collection_name: str, filename: str) -> Dict:
        """å¾ChromaDBä¸­é€šéå…ƒæ•¸æ“šéæ¿¾å™¨åˆªé™¤"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            # ChromaDBçš„ `delete` æ–¹æ³•æ¥å—ä¸€å€‹ `where` éæ¿¾å™¨
            # æˆ‘å€‘éœ€è¦å…ˆè¨ˆç®—æœ‰å¤šå°‘å€‹åŒ¹é…çš„å¡Š
            existing_chunks = vectorstore.get(where={"filename": filename})['ids']
            if not existing_chunks:
                return {"success": True, "message": "æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç„¡éœ€åˆªé™¤", "deleted_chunks": 0}

            vectorstore.delete(where={"filename": filename})
            logger.info(f"âœ… [Chroma] æˆåŠŸç‚ºæ–‡ä»¶ '{filename}' ç™¼å‡ºåˆªé™¤è«‹æ±‚ã€‚")
            
            return {
                "success": True,
                "message": f"æ–‡ä»¶ '{filename}' åŠå…¶ {len(existing_chunks)} å€‹åˆ†å¡Šå·²æˆåŠŸåˆªé™¤ã€‚",
                "deleted_chunks": len(existing_chunks),
                "filename": filename
            }
        except Exception as e:
            logger.error(f"âŒ [Chroma] æ–‡ä»¶åˆªé™¤å¤±æ•— '{filename}': {e}", exc_info=True)
            return {"success": False, "message": f"Chromaåˆªé™¤å¤±æ•—: {e}", "deleted_chunks": 0}

    def _delete_from_pgvector_by_sql(self, collection_name: str, filename: str) -> Dict:
        """ğŸ—‘ï¸ [æ ¸å¿ƒ] ä½¿ç”¨SQLç›´æ¥å¾PGVectoråˆªé™¤ï¼Œé€™æ˜¯æœ€å¯é çš„æ–¹æ³•ã€‚"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            collection_id = self._get_pg_collection_id(vectorstore, collection_name)
            if not collection_id:
                return {"success": False, "message": f"æ‰¾ä¸åˆ°é›†åˆID: {collection_name}"}

            # ç›´æ¥å¾ LangChain çš„ PGVector å¯¦ç¾ä¸­ç²å–é€£æ¥
            with vectorstore._connect() as conn:
                with conn.cursor() as cur:
                    # æŸ¥è©¢è¦åˆªé™¤çš„è¡Œæ•¸ (ç”¨æ–¼å ±å‘Š)
                    count_query = """
                        SELECT COUNT(*) FROM langchain_pg_embedding
                        WHERE collection_id = %s AND (cmetadata->>'filename' = %s OR cmetadata->>'original_filename' = %s);
                    """
                    cur.execute(count_query, (collection_id, filename, filename))
                    chunks_to_delete = cur.fetchone()[0]

                    if chunks_to_delete == 0:
                        logger.warning(f"[PGVector] åœ¨é›†åˆ '{collection_name}' ä¸­æ‰¾ä¸åˆ°æ–‡ä»¶ '{filename}' çš„è¨˜éŒ„ã€‚")
                        return {"success": True, "message": "æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç„¡éœ€åˆªé™¤", "deleted_chunks": 0}

                    logger.info(f"[PGVector] æº–å‚™å¾é›†åˆ '{collection_name}' ä¸­åˆªé™¤æ–‡ä»¶ '{filename}' çš„ {chunks_to_delete} å€‹åˆ†å¡Š...")

                    # åŸ·è¡Œåˆªé™¤
                    delete_query = """
                        DELETE FROM langchain_pg_embedding
                        WHERE collection_id = %s AND (cmetadata->>'filename' = %s OR cmetadata->>'original_filename' = %s);
                    """
                    cur.execute(delete_query, (collection_id, filename, filename))
                    deleted_count = cur.rowcount
                    conn.commit()

                    logger.info(f"âœ… [PGVector] æˆåŠŸåˆªé™¤ {deleted_count} å€‹åˆ†å¡Šã€‚")

            return {
                "success": True,
                "message": f"æ–‡ä»¶ '{filename}' åŠå…¶ {deleted_count} å€‹åˆ†å¡Šå·²æˆåŠŸåˆªé™¤ã€‚",
                "deleted_chunks": deleted_count,
                "filename": filename
            }

        except Exception as e:
            logger.error(f"âŒ [PGVector] SQLåˆªé™¤å¤±æ•— '{filename}': {e}", exc_info=True)
            # å˜—è©¦å›æ»¾äº‹å‹™
            if 'conn' in locals() and conn:
                conn.rollback()
            return {"success": False, "message": f"è³‡æ–™åº«åˆªé™¤æ“ä½œå¤±æ•—: {e}", "deleted_chunks": 0}

    def _get_pg_collection_id(self, vectorstore, collection_name: str) -> Optional[str]:
        """ç²å–çµ¦å®šé›†åˆåç¨±çš„UUIDã€‚"""
        try:
            with vectorstore._connect() as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        "SELECT uuid FROM langchain_pg_collection WHERE name = %s;",
                        (collection_name,)
                    )
                    result = cur.fetchone()
                    return result[0] if result else None
        except Exception as e:
            logger.error(f"ç²å–é›†åˆIDå¤±æ•— '{collection_name}': {e}")
            return None
                "total_chunks": matching_count
            }
            
            logger.info(f"ğŸ“Š File '{filename}': {matching_count} chunks, {len(doc_ids)} doc_ids, {len(chunk_ids)} chunk_ids")
            
            return result
            
        except Exception as e:
            logger.error(f"Find file IDs failed: {e}")
            return {"doc_ids": [], "chunk_ids": [], "total_chunks": 0}

    def _delete_by_chunk_ids_fixed(self, vectorstore, chunk_ids: List[str], filename: str) -> Dict:
        """ğŸ—‘ï¸ ä¿®æ­£ç‰ˆï¼šä½¿ç”¨æ­£ç¢ºçš„PGVectorèªæ³•åˆªé™¤"""
        try:
            if not chunk_ids:
                return {"success": False, "message": "No chunk IDs found", "deleted_chunks": 0}
            
            print(f"ğŸ—‘ï¸ é–‹å§‹åˆªé™¤{len(chunk_ids)}å€‹chunks")
            
            # è¨˜éŒ„åˆªé™¤å‰çš„æ•¸é‡
            before_count = len(chunk_ids)
            
            # âœ… ä½¿ç”¨æ­£ç¢ºçš„PGVectorèªæ³•
            try:
                print(f"   ğŸ“¡ èª¿ç”¨vectorstore.delete(ids=chunk_ids)")
                vectorstore.delete(ids=chunk_ids)
                deletion_method = "direct_ids"
                print(f"   âœ… æ‰¹é‡åˆªé™¤å®Œæˆ")
                
            except Exception as batch_error:
                print(f"   âš ï¸ æ‰¹é‡åˆªé™¤å¤±æ•—: {batch_error}")
                
                # å‚™ç”¨æ–¹æ¡ˆï¼šé€å€‹åˆªé™¤
                print(f"   ğŸ”„ å˜—è©¦é€å€‹åˆªé™¤...")
                deleted_count = 0
                
                for i, chunk_id in enumerate(chunk_ids):
                    try:
                        vectorstore.delete(ids=[chunk_id])
                        deleted_count += 1
                        if (i + 1) % 10 == 0:  # æ¯10å€‹é¡¯ç¤ºé€²åº¦
                            print(f"      é€²åº¦: {i + 1}/{len(chunk_ids)}")
                    except Exception as individual_error:
                        print(f"      âŒ chunk_id {chunk_id} åˆªé™¤å¤±æ•—: {individual_error}")
                
                deletion_method = f"individual_ids_{deleted_count}"
                print(f"   ğŸ“Š é€å€‹åˆªé™¤å®Œæˆ: {deleted_count}/{len(chunk_ids)}")
            
            # é©—è­‰åˆªé™¤çµæœ
            time.sleep(2)  # ç­‰å¾…æ•¸æ“šåº«æ›´æ–°
            remaining_chunks = self._verify_deletion(vectorstore, filename)
            actual_deleted = before_count - remaining_chunks
            
            success = remaining_chunks == 0
            
            print(f"ğŸ“Š åˆªé™¤çµæœ:")
            print(f"   åŸå§‹æ•¸é‡: {before_count}")
            print(f"   å‰©é¤˜æ•¸é‡: {remaining_chunks}")  
            print(f"   å¯¦éš›åˆªé™¤: {actual_deleted}")
            print(f"   æˆåŠŸç‡: {(actual_deleted/before_count*100):.1f}%")
            
            return {
                "success": success,
                "message": f"åˆªé™¤å®Œæˆ: {actual_deleted}/{before_count} chunks" if success 
                        else f"éƒ¨åˆ†åˆªé™¤å¤±æ•—ï¼Œé‚„å‰©{remaining_chunks} chunks",
                "deleted_chunks": actual_deleted,
                "remaining_chunks": remaining_chunks,
                "filename": filename,
                "method": deletion_method,
                "total_chunk_ids": len(chunk_ids)
            }
            
        except Exception as e:
            logger.error(f"Delete by chunk IDs failed: {e}")
            return {"success": False, "message": f"åˆªé™¤å¤±æ•—: {str(e)}", "deleted_chunks": 0}
        
    def _verify_deletion(self, vectorstore, filename: str) -> int:
        """ğŸ—‘ï¸ é©—è­‰åˆªé™¤çµæœ - è¨ˆç®—å‰©é¤˜chunksæ•¸é‡ï¼ˆç°¡åŒ–ç‰ˆï¼Œé¿å…é‡è¤‡èª¿ç”¨ï¼‰"""
        try:
            # ä½¿ç”¨ç°¡å–®çš„similarity_searché©—è­‰
            all_docs = vectorstore.similarity_search("", k=1000)
            remaining_count = 0
            
            for doc in all_docs:
                doc_filename = (doc.metadata.get('original_filename') or 
                            doc.metadata.get('filename', ''))
                if doc_filename == filename:
                    remaining_count += 1
            
            return remaining_count
            
        except Exception as e:
            print(f"âš ï¸ é©—è­‰åˆªé™¤çµæœå¤±æ•—: {e}")
            return -1  # ç„¡æ³•é©—è­‰

    def delete_document(self, collection_name: str, source_file: str) -> Dict:
        """ğŸ—‘ï¸ ä¿®æ­£ç‰ˆï¼šåˆªé™¤æŒ‡å®šæª”æ¡ˆåŠå…¶æ‰€æœ‰å‘é‡ - å…¼å®¹Chromaå’ŒPGVector"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            # ç²å–æ–‡ä»¶ä¿¡æ¯
            if self.use_postgres:
                chunk_ids = self._find_chunk_ids_reliable(vectorstore, source_file)
                chunk_count = len(chunk_ids)
            else:
                existing_chunks = self.get_document_chunks(collection_name, source_file)
                chunk_count = len(existing_chunks)
            
            if chunk_count == 0:
                return {"success": False, "message": "æª”æ¡ˆä¸å­˜åœ¨æˆ–å·²è¢«åˆªé™¤", "deleted_chunks": 0}
            
            if self.use_postgres:
                return self._delete_from_pgvector_fixed(vectorstore, collection_name, source_file, chunk_count)
            else:
                return self._delete_from_chroma(vectorstore, collection_name, source_file, chunk_count)
                
        except Exception as e:
            logger.error(f"åˆªé™¤æª”æ¡ˆå¤±æ•—{collection_name}/{source_file}: {e}")
            return {"success": False, "message": f"åˆªé™¤æª”æ¡ˆå¤±æ•—: {str(e)}", "deleted_chunks": 0}

    def _find_chunk_ids_reliable(self, vectorstore, filename: str) -> List[str]:
        """ğŸ—‘ï¸ å¯é åœ°æ‰¾å‡ºæª”æ¡ˆçš„æ‰€æœ‰chunk_ids"""
        try:
            chunk_ids = []
            docs = vectorstore.similarity_search("", k=10000)
            
            for doc in docs:
                doc_filename = (doc.metadata.get('original_filename') or 
                            doc.metadata.get('filename', ''))
                if doc_filename == filename:
                    chunk_id = doc.metadata.get('chunk_id')
                    if chunk_id:
                        chunk_ids.append(str(chunk_id))
            
            return chunk_ids
        except Exception as e:
            logger.error(f"Find chunk IDs failed: {e}")
            return []

    def _delete_from_chroma(self, vectorstore, collection_name: str, source_file: str, chunk_count: int) -> Dict:
        """ğŸ—‘ï¸ å¾Chromaåˆªé™¤æª”æ¡ˆ"""
        try:
            vectorstore.delete(filter={"filename": source_file})
        except Exception as e1:
            try:
                collection_folder = collection_name.replace('collection_', '')
                full_path = f"data\\{collection_folder}\\{source_file}"
                vectorstore.delete(filter={"source": full_path})
            except Exception as e2:
                raise e2
        
        return {"success": True, "message": f"æª”æ¡ˆ{source_file}åŠå…¶{chunk_count}å€‹åˆ†å¡Šå·²åˆªé™¤", "deleted_chunks": chunk_count, "filename": source_file}

    def _delete_from_pgvector_fixed(self, vectorstore, collection_name: str, source_file: str, chunk_count: int) -> Dict:
        """ğŸ—‘ï¸ ä¿®å¾©ç‰ˆæœ¬çš„PGVectoråˆªé™¤æ–¹æ³•"""
        try:
            print(f"ğŸ—‘ï¸ é–‹å§‹å¾PostgreSQLåˆªé™¤æ–‡ä»¶: {source_file}")
            
            # ä½¿ç”¨ä¿®å¾©ç‰ˆæœ¬çš„åˆªé™¤æ–¹æ³•
            deleted_count = self._postgresql_delete_file_completely_fixed(vectorstore, source_file)
            
            # é©—è­‰åˆªé™¤çµæœ
            remaining_docs = vectorstore.similarity_search("", k=1000)
            remaining_count = 0
            
            for doc in remaining_docs:
                doc_filename = (doc.metadata.get('original_filename') or 
                            doc.metadata.get('filename', ''))
                if doc_filename == source_file:
                    remaining_count += 1
            
            actual_deleted = chunk_count - remaining_count
            success = remaining_count == 0
            
            if success:
                print(f"âœ… æ–‡ä»¶å®Œå…¨åˆªé™¤æˆåŠŸ: {source_file}")
            else:
                print(f"âš ï¸ éƒ¨åˆ†åˆªé™¤å¤±æ•—ï¼Œé‚„å‰©{remaining_count}å€‹åˆ†å¡Š")
            
            return {
                "success": success,
                "message": f"æ–‡ä»¶{source_file}åˆªé™¤å®Œæˆï¼Œç§»é™¤äº†{actual_deleted}å€‹åˆ†å¡Š" if success 
                        else f"éƒ¨åˆ†åˆªé™¤å¤±æ•—ï¼Œé‚„å‰©{remaining_count}å€‹åˆ†å¡Š",
                "deleted_chunks": actual_deleted,
                "remaining_chunks": remaining_count,
                "filename": source_file
            }
            
        except Exception as e:
            logger.error(f"PostgreSQLåˆªé™¤å¤±æ•—: {e}")
            return {
                "success": False, 
                "message": f"åˆªé™¤å¤±æ•—: {str(e)}", 
                "deleted_chunks": 0
            }

    def _postgresql_delete_file_completely_fixed(self, vectorstore, filename: str) -> int:
        """ğŸ—‘ï¸ ä¿®å¾©ç‰ˆï¼šä½¿ç”¨å¤šç¨®æ–¹æ³•ç¢ºä¿æ–‡ä»¶å®Œå…¨åˆªé™¤"""
        print(f"ğŸ—‘ï¸ é–‹å§‹å¾¹åº•åˆªé™¤: {filename}")
        
        try:
            # 1. ç²å–æ‰€æœ‰æ–‡æª”ä¸¦æ‰¾åˆ°åŒ¹é…çš„
            all_docs = vectorstore.similarity_search("", k=5000)
            matching_docs = []
            
            for doc in all_docs:
                metadata = doc.metadata
                doc_filename = (metadata.get('original_filename') or 
                            metadata.get('filename', ''))
                
                if (
                    doc_filename == filename or 
                    filename in str(metadata.get('source', ''))
                ):
                    matching_docs.append(doc)
            
            if not matching_docs:
                print(f"   âš ï¸ æ²’æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡æª”")
                return 0
            
            print(f"   ğŸ¯ æ‰¾åˆ°{len(matching_docs)}å€‹åŒ¹é…æ–‡æª”")
            
            deleted_count = 0
            
            # æ–¹æ³•1: å˜—è©¦ä½¿ç”¨åŸç”ŸPGVectoråˆªé™¤
            try:
                print(f"   ğŸ”„ æ–¹æ³•1: PGVectoråŸç”Ÿåˆªé™¤...")
                
                # æ”¶é›†æ‰€æœ‰chunk_id
                chunk_ids = []
                for doc in matching_docs:
                    chunk_id = doc.metadata.get('chunk_id')
                    if chunk_id:
                        chunk_ids.append(str(chunk_id))
                
                if chunk_ids:
                    # åˆ†æ‰¹åˆªé™¤ï¼Œæ¯æ‰¹10å€‹
                    batch_size = 10
                    for i in range(0, len(chunk_ids), batch_size):
                        batch = chunk_ids[i:i+batch_size]
                        try:
                            # å˜—è©¦ç›´æ¥IDåˆªé™¤
                            vectorstore.delete(ids=batch)
                            print(f"      âœ… æ‰¹æ¬¡{i//batch_size + 1}: åˆªé™¤{len(batch)}å€‹")
                        except Exception as e:
                            print(f"      âŒ æ‰¹æ¬¡{i//batch_size + 1}å¤±æ•—: {e}")
                            
                            # é€å€‹å˜—è©¦
                            for chunk_id in batch:
                                try:
                                    vectorstore.delete(ids=[chunk_id])
                                except:
                                    pass
                    
                    # é©—è­‰ç¬¬ä¸€ç¨®æ–¹æ³•çš„æ•ˆæœ
                    time.sleep(2)
                    verification_docs = vectorstore.similarity_search("", k=5000)
                    remaining = sum(1 for doc in verification_docs 
                                if (doc.metadata.get('original_filename') == filename or 
                                    doc.metadata.get('filename') == filename))
                    
                    deleted_count = len(matching_docs) - remaining
                    print(f"   ğŸ“Š æ–¹æ³•1çµæœ: åˆªé™¤{deleted_count}/{len(matching_docs)}")
            
            except Exception as e:
                print(f"   âŒ æ–¹æ³•1å¤±æ•—: {e}")
            
            # æœ€çµ‚é©—è­‰
            time.sleep(3)
            final_docs = vectorstore.similarity_search("", k=5000)
            final_remaining = sum(1 for doc in final_docs 
                                if (doc.metadata.get('original_filename') == filename or 
                                    doc.metadata.get('filename') == filename))
            
            final_deleted = len(matching_docs) - final_remaining
            print(f"   ğŸ“Š æœ€çµ‚çµæœ: {final_deleted}/{len(matching_docs)} (å‰©é¤˜: {final_remaining})")
            
            return final_deleted
            
        except Exception as e:
            print(f"   âŒ åˆªé™¤å¤±æ•—: {e}")
            return 0
    
    # ==================== ğŸ“Š çµ±è¨ˆæŸ¥è©¢åŠŸèƒ½ ====================
    
    def get_stats(self) -> Dict:
        """ğŸ“Š ç²å–ç³»çµ±çµ±è¨ˆ - ç´”PostgreSQLç‰ˆæœ¬"""
        try:
            stats = {}
            
            # å¾æ‰€æœ‰å·²çŸ¥é›†åˆç²å–çµ±è¨ˆ
            collections = self.get_available_collections()
            
            for collection_info in collections:
                collection_name = collection_info['collection_name']
                display_name = collection_info['display_name']
                
                try:
                    vectorstore = self.get_or_create_vectorstore(collection_name)
                    
                    # ç›´æ¥æŸ¥è©¢PostgreSQLè¨ˆç®—æ–‡æª”æ•¸
                    docs = vectorstore.similarity_search("", k=5000)
                    stats[display_name] = len(docs)
                    
                except Exception as e:
                    logger.warning(f"ç²å–é›†åˆçµ±è¨ˆå¤±æ•—{collection_name}: {e}")
                    stats[display_name] = 0
            
            return stats
        except Exception as e:
            logger.error(f"ç²å–çµ±è¨ˆå¤±æ•—: {e}")
            return {}

    def get_available_collections(self) -> List[Dict]:
        """ğŸ“Š ç²å–æ‰€æœ‰å¯ç”¨çš„é›†åˆåˆ—è¡¨ - ç´”PostgreSQLç‰ˆæœ¬"""
        try:
            collections = []
            
            # å¾ç’°å¢ƒæˆ–é…ç½®ä¸­ç²å–å·²çŸ¥é›†åˆ
            known_collections = ["collection_test_01", "collection_test", "collection_default"]
            
            for collection_name in known_collections:
                display_name = collection_name.replace('collection_', '')
                
                try:
                    vectorstore = self.get_or_create_vectorstore(collection_name)
                    docs = vectorstore.similarity_search("", k=100)
                    
                    if docs:  # åªæœ‰ç•¶é›†åˆä¸­æœ‰æ–‡æª”æ™‚æ‰åŠ å…¥
                        doc_count = len(docs)
                        
                        # è¨ˆç®—æ–‡ä»¶æ•¸ï¼ˆæŒ‰æª”åå»é‡ï¼‰
                        unique_files = set()
                        for doc in docs:
                            filename = doc.metadata.get('original_filename') or doc.metadata.get('filename', 'unknown')
                            if filename != 'unknown':
                                unique_files.add(filename)
                        
                        collections.append({
                            'collection_name': collection_name,
                            'display_name': display_name,
                            'document_count': doc_count,
                            'file_count': len(unique_files),
                            'status': 'active'
                        })
                        
                except Exception as e:
                    logger.warning(f"æª¢æŸ¥é›†åˆå¤±æ•—{collection_name}: {e}")
            
            collections.sort(key=lambda x: x['display_name'])
            return collections
            
        except Exception as e:
            logger.error(f"ç²å–é›†åˆåˆ—è¡¨å¤±æ•—: {e}")
            return []
    
    # ==================== ğŸ”§ ç³»çµ±è¨ºæ–·åŠŸèƒ½ ====================
    
    def diagnose_system(self) -> Dict:
        """ğŸ”§ ç³»çµ±è¨ºæ–·"""
        print("ğŸ” === ç³»çµ±è¨ºæ–· ===")
        
        diagnosis = {
            "environment": {},
            "embedding_model": {},
            "text_processing": {},
            "performance": {},
            "recommendations": []
        }
        
        # ç’°å¢ƒæª¢æŸ¥
        api_key = os.getenv("OPENAI_API_KEY")
        diagnosis["environment"]["openai_api_key"] = "âœ… å·²è¨­ç½®" if api_key else "âŒ æœªè¨­ç½®"
        diagnosis["environment"]["model_type"] = self.model_type
        
        # åµŒå…¥æ¨¡å‹æª¢æŸ¥
        try:
            test_result = self.embeddings.embed_query("æ¸¬è©¦")
            diagnosis["embedding_model"]["status"] = "âœ… æ­£å¸¸"
            diagnosis["embedding_model"]["dimension"] = len(test_result)
        except Exception as e:
            diagnosis["embedding_model"]["status"] = f"âŒ å¤±æ•—: {e}"
        
        # æ–‡æœ¬è™•ç†æª¢æŸ¥
        diagnosis["text_processing"]["normalizer"] = "âœ… æ­£å¸¸" if hasattr(self, 'normalizer') else "âŒ ç•°å¸¸"
        diagnosis["text_processing"]["analyzer"] = "âœ… æ­£å¸¸" if hasattr(self, 'analyzer') else "âŒ ç•°å¸¸"
        diagnosis["text_processing"]["splitter"] = "âœ… æ­£å¸¸" if hasattr(self, 'text_splitter') else "âŒ ç•°å¸¸"
        
        # æ€§èƒ½çµ±è¨ˆ
        if hasattr(self, 'batch_processor'):
            perf_stats = self.batch_processor.get_performance_stats()
            diagnosis["performance"] = perf_stats
        else:
            diagnosis["performance"] = {"status": "æœªåˆå§‹åŒ–"}
        
        # å»ºè­°
        if not api_key:
            diagnosis["recommendations"].append("è¨­ç½®OPENAI_API_KEYç’°å¢ƒè®Šæ•¸")
        
        perf_success_rate = diagnosis["performance"].get("success_rate", 1)
        if perf_success_rate < 0.8:
            diagnosis["recommendations"].append("æˆåŠŸç‡åä½ï¼Œå»ºè­°æª¢æŸ¥ç¶²è·¯é€£æ¥å’ŒAPIé…é¡")
        
        # è¼¸å‡ºè¨ºæ–·çµæœ
        for category, info in diagnosis.items():
            if category != "recommendations":
                print(f"\nğŸ”§ {category.upper()}:")
                for key, value in info.items():
                    print(f"   {key}: {value}")
        
        if diagnosis["recommendations"]:
            print(f"\nğŸ’¡ å»ºè­°:")
            for rec in diagnosis["recommendations"]:
                print(f"   â€¢ {rec}")
        
        return diagnosis

    def debug_collection_content(self, collection_name: str, filename: str = None):
        """ğŸ”§ èª¿è©¦é›†åˆå…§å®¹"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            print(f"\nğŸ” èª¿è©¦é›†åˆ: {collection_name}")
            
            if filename:
                print(f"ğŸ¯ ç‰¹å®šæª”æ¡ˆ: {filename}")
                docs = vectorstore.similarity_search("", k=1000)
                matching_docs = [
                    doc for doc in docs 
                    if doc.metadata.get('filename') == filename or 
                       doc.metadata.get('original_filename') == filename
                ]
                docs_to_show = matching_docs[:5]
                print(f"ğŸ“„ æ‰¾åˆ° {len(matching_docs)} å€‹ç›¸é—œæ–‡æª”")
            else:
                docs = vectorstore.similarity_search("", k=5)
                docs_to_show = docs
                print(f"ğŸ“„ é›†åˆä¸­å…±æœ‰ {len(docs)} å€‹æ–‡æª”")
            
            for i, doc in enumerate(docs_to_show):
                print(f"\nğŸ“„ æ–‡æª” {i+1}:")
                print(f"   å…§å®¹é è¦½: {doc.page_content[:100]}...")
                print(f"   å…ƒæ•¸æ“š:")
                for key, value in doc.metadata.items():
                    if len(str(value)) > 100:
                        print(f"     {key}: {str(value)[:100]}...")
                    else:
                        print(f"     {key}: {value}")
                        
        except Exception as e:
            print(f"âŒ èª¿è©¦å¤±æ•—: {e}")

    def test_knowledge_management(self):
        """ğŸ§ª æ¸¬è©¦çŸ¥è­˜ç®¡ç†ç³»çµ±åŠŸèƒ½"""
        print("\n" + "="*60)
        print("ğŸ§ª çŸ¥è­˜ç®¡ç†ç³»çµ±åŠŸèƒ½æ¸¬è©¦")
        print("="*60)
        
        try:
            # æ¸¬è©¦åŸºæœ¬åŠŸèƒ½
            print("ğŸ“Š ç²å–ç³»çµ±çµ±è¨ˆ...")
            stats = self.get_stats()
            print(f"   æ‰¾åˆ° {len(stats)} å€‹é›†åˆ")
            
            print("ğŸ“‹ ç²å–é›†åˆåˆ—è¡¨...")
            collections = self.get_available_collections()
            print(f"   æ‰¾åˆ° {len(collections)} å€‹æ´»èºé›†åˆ")
            
            # æ¸¬è©¦æœç´¢åŠŸèƒ½
            if collections:
                first_collection = collections[0]['collection_name']
                print(f"ğŸ” æ¸¬è©¦æœç´¢åŠŸèƒ½ (é›†åˆ: {first_collection})...")
                search_results = self.search("æ¸¬è©¦", first_collection, k=3)
                print(f"   æ‰¾åˆ° {len(search_results)} å€‹æœç´¢çµæœ")
            
            print("âœ… åŸºæœ¬åŠŸèƒ½æ¸¬è©¦å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ¸¬è©¦éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
            logger.error(f"æ¸¬è©¦å¤±æ•—: {e}")


# âœ… ç¢ºä¿å‘å¾Œå…¼å®¹æ€§çš„åŒ¯å‡º
__all__ = ['OptimizedVectorSystem']

"""
ğŸ“‹ 4Bæª”æ¡ˆ (management_interface.py) åŠŸèƒ½ç¸½çµ:

ğŸ”— ä¾è³´é—œä¿‚: ç¹¼æ‰¿è‡ª VectorOperationsCore (4Aæª”æ¡ˆ)
ğŸ“Š åŒ…å«æ–¹æ³•: 45å€‹å®Œæ•´çš„ç®¡ç†APIæ–¹æ³•
ğŸ”§ æ ¸å¿ƒåŠŸèƒ½: 
   - ğŸ” æœç´¢åŠŸèƒ½ (1å€‹æ–¹æ³•)
   - ğŸ“¤ æ–‡ä»¶ä¸Šå‚³ (1å€‹æ–¹æ³•)  
   - ğŸ“‹ æ–‡æª”æŸ¥è©¢ (7å€‹æ–¹æ³•)
   - ğŸ—‘ï¸ åˆªé™¤åŠŸèƒ½ (8å€‹ä¸»è¦æ–¹æ³•)
   - ğŸ“Š çµ±è¨ˆæŸ¥è©¢ (2å€‹æ–¹æ³•)
   - ğŸ”§ ç³»çµ±è¨ºæ–· (2å€‹æ–¹æ³•)

âœ… å‘å¾Œå…¼å®¹: OptimizedVectorSystem å®Œå…¨å…¼å®¹ç¾æœ‰ä»£ç¢¼
ğŸ”’ å°å…¥å®‰å…¨: æ™ºèƒ½å›é€€æ©Ÿåˆ¶ï¼Œç¢ºä¿å¾4Aæª”æ¡ˆæ­£ç¢ºç²å–é…ç½®
ğŸ“¦ æ¨¡å¡ŠåŒ–è¨­è¨ˆ: æ¸…æ™°çš„è·è²¬åˆ†é›¢ï¼Œä¾¿æ–¼ç¶­è­·å’Œæ“´å±•
"""