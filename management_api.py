#!/usr/bin/env python3
"""
ğŸ“¦ 4Bæª”æ¡ˆ: management_interface.py (ç®¡ç†ä»‹é¢å±¤)
ğŸ¯ è·è²¬ï¼šç³»çµ±ç®¡ç†APIã€ç”¨æˆ¶æ¥å£ã€é«˜ç´šç®¡ç†åŠŸèƒ½
ğŸ”— ä¾è³´ï¼švector_operations.py (4Aæª”æ¡ˆ)
ğŸ“Š åŒ…å«ï¼šOptimizedVectorSystemé¡åˆ¥ (45å€‹æ–¹æ³•)
"""

import time
import json
import logging
import os
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
import psycopg

# ğŸ”— ä¾è³´4Aæª”æ¡ˆï¼šå‘é‡æ“ä½œæ ¸å¿ƒå±¤
try:
    from vector_operations import VectorOperationsCore
    print("âœ… æˆåŠŸå°å…¥ VectorOperationsCore å¾ vector_operations.py")
except ImportError as e:
    print(f"âŒ ç„¡æ³•å°å…¥ vector_operations.py: {e}")
    print("ğŸ“‹ è«‹ç¢ºä¿ vector_operations.py (4Aæª”æ¡ˆ) å­˜åœ¨ä¸”å¯æ­£å¸¸å°å…¥")
    raise ImportError("4Bæª”æ¡ˆä¾è³´4Aæª”æ¡ˆï¼Œè«‹å…ˆç¢ºä¿ vector_operations.py å¯æ­£å¸¸ä½¿ç”¨")

# ğŸ”— å˜—è©¦å°å…¥é…ç½® (å¾4Aæª”æ¡ˆç²å–)
try:
    # å¾å·²å°å…¥çš„ vector_operations ç²å–æ‰€æœ‰å¿…è¦é…ç½®
    import vector_operations as vo_module
    
    # å‹•æ…‹ç²å–é…ç½®è®Šæ•¸
    SYSTEM_CONFIG = getattr(vo_module, 'SYSTEM_CONFIG', {})
    TOKEN_LIMITS = getattr(vo_module, 'TOKEN_LIMITS', {})
    SUPPORTED_EXTENSIONS = getattr(vo_module, 'SUPPORTED_EXTENSIONS', set())
    
    # ç²å–ä¾è³´æª¢æŸ¥è®Šæ•¸
    PGVECTOR_AVAILABLE = getattr(vo_module, 'PGVECTOR_AVAILABLE', False)
    OPENAI_EMBEDDINGS_AVAILABLE = getattr(vo_module, 'OPENAI_EMBEDDINGS_AVAILABLE', False)
    
    # ç²å–è³‡æ–™é¡åˆ¥
    FileInfo = getattr(vo_module, 'FileInfo', None)
    SearchResult = getattr(vo_module, 'SearchResult', None)
    
    print("âœ… æˆåŠŸå¾4Aæª”æ¡ˆç²å–æ‰€æœ‰å¿…è¦é…ç½®")
    
except Exception as e:
    print(f"âš ï¸ å¾4Aæª”æ¡ˆç²å–é…ç½®æ™‚å‡ºç¾å•é¡Œ: {e}")
    # æä¾›æœ€å°å¿…è¦é…ç½®ä½œç‚ºå›é€€
    SUPPORTED_EXTENSIONS = {'.txt', '.md', '.pdf', '.csv', '.json', '.py'}
    PGVECTOR_AVAILABLE = False
    OPENAI_EMBEDDINGS_AVAILABLE = False

logger = logging.getLogger(__name__)

class OptimizedVectorSystem(VectorOperationsCore):
    """
    ğŸ“¦ å®Œæ•´çš„å‘é‡ç³»çµ±ç®¡ç†ä»‹é¢
    ğŸ”— ç¹¼æ‰¿è‡ª VectorOperationsCore (4Aæª”æ¡ˆ)
    ğŸ“Š æä¾›45å€‹ç®¡ç†APIæ–¹æ³•ï¼Œå®Œæ•´å‘å¾Œå…¼å®¹
    """
    
    def __init__(self, data_dir: str = None, model_type: str = None):
        """ğŸ”§ åˆå§‹åŒ–ç®¡ç†ä»‹é¢ - ç¹¼æ‰¿4Aæª”æ¡ˆçš„æ ¸å¿ƒåŠŸèƒ½"""
        try:
            # èª¿ç”¨çˆ¶é¡åˆ¥åˆå§‹åŒ– (4Aæª”æ¡ˆ)
            super().__init__(data_dir, model_type)
            print("ğŸ“Š ç®¡ç†ä»‹é¢å±¤åˆå§‹åŒ–å®Œæˆ")
            print("   ğŸ” æœç´¢æœå‹™: âœ…")  
            print("   ğŸ“¤ æ–‡ä»¶ä¸Šå‚³: âœ…")
            print("   ğŸ“‹ æ–‡æª”æŸ¥è©¢: âœ…")
            print("   ğŸ—‘ï¸ åˆªé™¤ç®¡ç†: âœ…")
            print("   ğŸ“Š çµ±è¨ˆè¨ºæ–·: âœ…")
            
        except Exception as e:
            logger.error(f"ç®¡ç†ä»‹é¢åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    # ==================== ğŸ” æœç´¢åŠŸèƒ½ç›¸é—œ (1å€‹æ–¹æ³•) ====================
    
    def search(self, query: str, collection_name: str = None, k: int = 5) -> List[Dict]:
        """ğŸ” å„ªåŒ–ç‰ˆæœç´¢ - æ”¯æ´å¤šé›†åˆã€æŸ¥è©¢è®Šé«”ã€å»é‡æ’åº"""
        try:
            # å‰µå»ºæœç´¢è®Šé«”
            query_variants = []
            if hasattr(self, 'normalizer') and hasattr(self.normalizer, 'create_search_variants'):
                query_variants = self.normalizer.create_search_variants(query)
            else:
                query_variants = [query]  # å›é€€åˆ°åŸæŸ¥è©¢
            
            all_results = []
            
            # è™•ç†é›†åˆç¯„åœ
            target_collections = []
            if collection_name:
                target_collections = [collection_name]
            else:
                stats = self.get_stats()
                target_collections = [f"collection_{name}" for name in stats.keys()]
            
            # å°æ¯å€‹é›†åˆå’ŒæŸ¥è©¢è®Šé«”é€²è¡Œæœç´¢
            for variant in query_variants:
                for coll_name in target_collections:
                    try:
                        vectorstore = self.get_or_create_vectorstore(coll_name)
                        docs_and_scores = vectorstore.similarity_search_with_score(variant, k=k)
                        
                        for doc, score in docs_and_scores:
                            # å‰µå»ºæœç´¢çµæœ
                            result = {
                                "content": doc.page_content[:500],  # é™åˆ¶é è¦½é•·åº¦
                                "score": 1.0 - score,  # è½‰æ›ç‚ºç›¸ä¼¼åº¦åˆ†æ•¸
                                "metadata": doc.metadata,
                                "collection": coll_name,
                                "chunk_info": {
                                    'chunk_id': doc.metadata.get('chunk_id', 'unknown'),
                                    'content': doc.page_content,
                                    'metadata': doc.metadata,
                                    'token_count': doc.metadata.get('token_count', 0),
                                    'quality_score': doc.metadata.get('quality_score', 0.5)
                                }
                            }
                            all_results.append(result)
                            
                    except Exception as e:
                        logger.warning(f"æœç´¢é›†åˆå¤±æ•— {coll_name}: {e}")
            
            # å»é‡å’Œæ’åº
            seen_content = set()
            unique_results = []
            
            for result in sorted(all_results, key=lambda x: x["score"], reverse=True):
                content_hash = hashlib.md5(result["content"].encode()).hexdigest()
                if content_hash not in seen_content:
                    seen_content.add(content_hash)
                    unique_results.append(result)
            
            return unique_results[:k]
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±æ•—: {e}")
            return []
    
    # ==================== ğŸ“¤ æ–‡ä»¶ä¸Šå‚³ç›¸é—œ (1å€‹æ–¹æ³•) ====================
    
    def upload_single_file(self, file_content: bytes, filename: str, collection_name: str) -> Dict:
        """ğŸ“¤ ç´”PostgreSQLæ–¹æ¡ˆï¼šç›´æ¥è™•ç†æ–‡ä»¶å…§å®¹ï¼Œä¸ä¿å­˜åˆ°æœ¬åœ°"""
        try:
            # åŸºæœ¬é©—è­‰
            if not file_content:
                return {"success": False, "message": "æ–‡ä»¶å…§å®¹ç‚ºç©º", "chunks": []}
            
            if not filename or not filename.strip():
                return {"success": False, "message": "æ–‡ä»¶åä¸èƒ½ç‚ºç©º", "chunks": []}
            
            # æª¢æŸ¥æ–‡ä»¶æ“´å±•å
            file_extension = Path(filename).suffix.lower()
            if file_extension not in SUPPORTED_EXTENSIONS:
                return {
                    "success": False,
                    "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}ã€‚æ”¯æŒæ ¼å¼: {', '.join(SUPPORTED_EXTENSIONS)}",
                    "chunks": []
                }
            
            print(f"ğŸ“„ ç´”PostgreSQLæ–¹æ¡ˆï¼šç›´æ¥è™•ç†æ–‡ä»¶å…§å®¹ {filename}")
            
            # âœ… ä½¿ç”¨è‡¨æ™‚æ–‡ä»¶è™•ç†
            import tempfile
            with tempfile.NamedTemporaryFile(suffix=file_extension, delete=False) as temp_file:
                temp_file.write(file_content)
                temp_file_path = Path(temp_file.name)
            
            try:
                # è¼‰å…¥æ–‡æª”
                documents = self.load_document(temp_file_path)
                
                if not documents:
                    return {"success": False, "message": "æ–‡ä»¶å…§å®¹ç‚ºç©ºæˆ–æ ¼å¼ä¸æ”¯æŒ", "chunks": []}
                
                # è¨­ç½®å…ƒæ•¸æ“š
                current_timestamp = time.time()
                for doc in documents:
                    doc.metadata.update({
                        'collection': collection_name,
                        'original_filename': filename,
                        'filename': filename,  # âœ… ç¢ºä¿å…©å€‹æ¬„ä½éƒ½æœ‰
                        'upload_timestamp': current_timestamp,
                        'file_source': 'upload',
                        'uploaded_by': 'upload_interface',
                        'source': f"postgresql://{collection_name}/{filename}",  # âœ… è™›æ“¬PostgreSQLè·¯å¾‘
                        'file_extension': file_extension,
                        'stored_in': 'postgresql_only',  # âœ… æ¨™è¨˜åƒ…å­˜æ–¼PostgreSQL
                        'file_size': len(file_content)
                    })
                
                # å‘é‡åŒ–è™•ç†
                vectorstore = self.get_or_create_vectorstore(collection_name)
                
                # âœ… å…ˆå¾¹åº•åˆªé™¤å·²å­˜åœ¨çš„åŒåæ–‡ä»¶
                print(f"ğŸ—‘ï¸ æ¸…ç†ç¾æœ‰æ–‡ä»¶: {filename}")
                try:
                    self.delete_by_file_ids(collection_name, filename)
                except Exception as e:
                    print(f"âš ï¸ æ¸…ç†ç¾æœ‰æ–‡ä»¶æ™‚å‡ºç¾è­¦å‘Š: {e}")
                
                # æ‰¹æ¬¡è™•ç†
                print(f"ğŸ“„ é–‹å§‹å‘é‡åŒ–è™•ç†...")
                batches = self.batch_processor.create_smart_batches(documents)
                success_count = self._process_batches(vectorstore, batches)
                
                print(f"âœ… æ–‡ä»¶ä¸Šå‚³å®Œæˆ: {filename}")
                print(f"   ğŸ  ä¿å­˜ä½ç½®: PostgreSQL ({collection_name})")
                print(f"   ğŸ“„ åˆ†å¡Šæ•¸é‡: {len(documents)}")
                print(f"   âœ… æˆåŠŸå‘é‡åŒ–: {success_count}")
                
                return {
                    "success": True,
                    "message": f"æ–‡ä»¶ä¸Šå‚³æˆåŠŸï¼Œåƒ…ä¿å­˜åˆ°PostgreSQLï¼Œå…±ç”Ÿæˆ{len(documents)}å€‹åˆ†å¡Š",
                    "filename": filename,
                    "collection": collection_name,
                    "total_chunks": len(documents),
                    "success_chunks": success_count,
                    "upload_time": current_timestamp,
                    "storage_location": "postgresql_only"
                }
                
            finally:
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                if temp_file_path.exists():
                    temp_file_path.unlink()
                    print(f"ğŸ§¹ æ¸…ç†è‡¨æ™‚æ–‡ä»¶: {temp_file_path}")
                    
        except Exception as e:
            logger.error(f"æ–‡ä»¶ä¸Šå‚³å¤±æ•— {filename}: {e}")
            return {"success": False, "message": f"æ–‡ä»¶ä¸Šå‚³å¤±æ•—: {str(e)}", "chunks": []}
    
    # ==================== ğŸ“‹ æ–‡æª”æŸ¥è©¢ç›¸é—œ (7å€‹æ–¹æ³•) ====================
    
    def get_collection_documents(self, collection_name: str, page: int = 1, limit: int = 20, search: str = "") -> Dict:
        """ğŸ“‹ ç²å–é›†åˆä¸­çš„æª”æ¡ˆè³‡è¨Š - å…¼å®¹Chromaå’ŒPGVector"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            if self.use_postgres:
                return self._get_documents_from_pgvector(vectorstore, collection_name, page, limit, search)
            else:
                return self._get_documents_from_chroma(vectorstore, collection_name, page, limit, search)
                
        except Exception as e:
            logger.error(f"ç²å–æª”æ¡ˆåˆ—è¡¨å¤±æ•—: {e}", exc_info=True)
            return {"success": False, "error": str(e), "documents": [], "total": 0, "page": page, "limit": limit, "total_pages": 0}
        
    def _get_documents_from_chroma(self, vectorstore, collection_name: str, page: int, limit: int, search: str) -> Dict:
        """ğŸ“‹ å¾Chromaç²å–æª”æ¡ˆ - åŸæœ‰é‚è¼¯"""
        all_docs_raw = vectorstore.get()
        if not all_docs_raw or not all_docs_raw.get('metadatas'):
            return {"success": True, "documents": [], "total": 0, "page": page, "limit": limit, "total_pages": 0}

        file_stats = {}
        for metadata in all_docs_raw.get('metadatas', []):
            try:
                filename = metadata.get('original_filename', metadata.get('filename', 'unknown_file'))
                if filename == 'unknown_file': continue

                if filename not in file_stats:
                    file_stats[filename] = {
                        'filename': filename,
                        'source': metadata.get('source', 'unknown'),
                        'total_chunks': 0,
                        'upload_time': metadata.get('upload_timestamp', 0)
                    }
                file_stats[filename]['total_chunks'] += 1
            except Exception:
                continue

        safe_documents = list(file_stats.values())
        
        # æ·»åŠ æ ¼å¼åŒ–æ™‚é–“
        for doc in safe_documents:
            try:
                doc['upload_time_formatted'] = datetime.fromtimestamp(doc['upload_time']).strftime('%Y-%m-%d %H:%M:%S') if doc['upload_time'] else 'N/A'
            except:
                doc['upload_time_formatted'] = 'Invalid Date'

        # éæ¿¾å’Œåˆ†é 
        if search:
            safe_documents = [doc for doc in safe_documents if search.lower() in doc['filename'].lower()]
        
        safe_documents.sort(key=lambda x: x.get('upload_time', 0), reverse=True)
        total = len(safe_documents)
        total_pages = (total + limit - 1) // limit if total > 0 else 1
        start = (page - 1) * limit
        end = start + limit
        page_documents = safe_documents[start:end]

        return {"success": True, "documents": page_documents, "total": total, "page": page, "limit": limit, "total_pages": total_pages}

    def _get_documents_from_pgvector(self, vectorstore, collection_name: str, page: int, limit: int, search: str) -> Dict:
        """ğŸ“‹ ç´”PostgreSQLæ–¹æ¡ˆï¼šå®Œå…¨ä¸ä¾è³´æœ¬åœ°è¨˜éŒ„ï¼Œä¿®æ­£å‰ç«¯æ¬„ä½åŒ¹é…"""
        try:
            print(f"ğŸ” ç´”PostgreSQLç²å–{collection_name}çš„æª”æ¡ˆåˆ—è¡¨")
            
            # âœ… ç›´æ¥æŸ¥è©¢PostgreSQLä¸­çš„æ‰€æœ‰æ–‡æª”
            docs = vectorstore.similarity_search("", k=2000)
            
            if not docs:
                return {"success": True, "documents": [], "total": 0, "page": page, "limit": limit, "total_pages": 0}
            
            # âœ… æŒ‰æª”æ¡ˆååˆ†çµ„çµ±è¨ˆ
            file_stats = {}
            
            for doc in docs:
                metadata = doc.metadata
                filename = metadata.get('original_filename') or metadata.get('filename', 'unknown')
                
                if filename == 'unknown':
                    continue
                
                if filename not in file_stats:
                    # âœ… ä½¿ç”¨å‰ç«¯æœŸæœ›çš„æ¬„ä½åç¨±
                    file_stats[filename] = {
                        'filename': filename,
                        'source': metadata.get('source', f'postgresql://{collection_name}'),
                        'chunks': 0,  # âœ… å‰ç«¯æœŸæœ›çš„æ¬„ä½å(ä¸æ˜¯total_chunks)
                        'upload_time': metadata.get('upload_timestamp', 0),
                        'uploader': 'æœªçŸ¥',  # âœ… å‰ç«¯æœŸæœ›çš„æ¬„ä½å(ä¸æ˜¯uploaded_by)
                        'upload_time_formatted': 'æœªçŸ¥',
                        'file_extension': metadata.get('file_extension', ''),
                        'stored_in': 'postgresql'
                    }
                
                file_stats[filename]['chunks'] += 1
                
                # âœ… æ›´æ–°ä¸Šå‚³è€…ä¿¡æ¯ï¼ˆä½¿ç”¨æœ€æ–°çš„è¨˜éŒ„ï¼‰
                uploader_info = metadata.get('uploaded_by') or metadata.get('file_source', 'unknown')
                if uploader_info and uploader_info != 'unknown':
                    if uploader_info == 'upload_interface' or uploader_info == 'upload':
                        file_stats[filename]['uploader'] = 'ç®¡ç†ä»‹é¢'
                    elif uploader_info == 'sync':
                        file_stats[filename]['uploader'] = 'åŒæ­¥'
                    else:
                        file_stats[filename]['uploader'] = str(uploader_info)
            
            # âœ… å¾Œè™•ç†ï¼šæ ¼å¼åŒ–æ™‚é–“
            for filename, stats in file_stats.items():
                try:
                    if stats['upload_time'] and stats['upload_time'] > 0:
                        stats['upload_time_formatted'] = datetime.fromtimestamp(stats['upload_time']).strftime('%Y-%m-%d %H:%M:%S')
                    else:
                        stats['upload_time_formatted'] = 'æœªçŸ¥'
                except Exception as e:
                    logger.warning(f"æ™‚é–“æ ¼å¼åŒ–å¤±æ•—{filename}: {e}")
                    stats['upload_time_formatted'] = 'æœªçŸ¥'
                
                # âœ… ç¢ºä¿ä¸Šå‚³è€…ä¸æ˜¯ç©ºå€¼
                if not stats['uploader'] or stats['uploader'] in ['unknown', 'None', '']:
                    stats['uploader'] = 'æœªçŸ¥'
            
            safe_documents = list(file_stats.values())
            
            # éæ¿¾å’Œåˆ†é 
            if search:
                safe_documents = [doc for doc in safe_documents if search.lower() in doc['filename'].lower()]
            
            safe_documents.sort(key=lambda x: x.get('upload_time', 0), reverse=True)
            
            total = len(safe_documents)
            total_pages = (total + limit - 1) // limit if total > 0 else 1
            start = (page - 1) * limit
            end = start + limit
            page_documents = safe_documents[start:end]
            
            print(f"âœ… ç´”PostgreSQLç²å–æˆåŠŸ: {total}å€‹æª”æ¡ˆ")
            
            # âœ… èª¿è©¦ï¼šæ‰“å°å‰å¹¾å€‹æ–‡ä»¶çš„æ ¼å¼
            if page_documents:
                sample = page_documents[0]
                print(f"ğŸ“‹ è¿”å›æ•¸æ“šæ ¼å¼ç¯„ä¾‹:")
                print(f"   filename: {sample.get('filename')}")
                print(f"   chunks: {sample.get('chunks')}")
                print(f"   uploader: {sample.get('uploader')}")
                print(f"   upload_time_formatted: {sample.get('upload_time_formatted')}")
            
            return {
                "success": True,
                "documents": page_documents,
                "total": total,
                "page": page,
                "limit": limit,
                "total_pages": total_pages
            }
            
        except Exception as e:
            logger.error(f"PostgreSQLæŸ¥è©¢å¤±æ•—: {e}")
            return {"success": False, "error": str(e), "documents": [], "total": 0, "page": page, "limit": limit, "total_pages": 0}
    
    def get_document_chunks(self, collection_name: str, source_file: str) -> List[Dict]:
        """ğŸ“‹ ç²å–æŒ‡å®šæª”æ¡ˆçš„æ‰€æœ‰åˆ†å¡Š - å…¼å®¹Chromaå’ŒPGVector"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            if self.use_postgres:
                return self._get_chunks_from_pgvector(vectorstore, collection_name, source_file)
            else:
                return self._get_chunks_from_chroma(vectorstore, source_file)
                
        except Exception as e:
            logger.error(f"ç²å–æª”æ¡ˆåˆ†å¡Šå¤±æ•—{collection_name}/{source_file}: {e}")
            return []

    def _get_chunks_from_chroma(self, vectorstore, source_file: str) -> List[Dict]:
        """ğŸ“‹ å¾Chromaç²å–åˆ†å¡Š"""
        try:
            results = vectorstore.get(where={"filename": source_file})
            if not results or not results.get('documents'):
                return []
            
            chunks = []
            for i, (doc, metadata) in enumerate(zip(results['documents'], results['metadatas'])):
                chunk_info = {
                    'chunk_id': metadata.get('chunk_id', f'chunk_{i+1}'),
                    'chunk_index': metadata.get('chunk_index', i),
                    'content': doc,
                    'content_preview': doc[:200] + "..." if len(doc) > 200 else doc,
                    'token_count': metadata.get('token_count', 0),
                    'text_type': metadata.get('text_type', 'unknown'),
                    'quality_score': metadata.get('quality_score', 0.5),
                    'metadata': metadata
                }
                chunks.append(chunk_info)
            
            chunks.sort(key=lambda x: x.get('chunk_index', 0))
            return chunks
            
        except Exception as e:
            logger.error(f"Chromaåˆ†å¡Šç²å–å¤±æ•—: {e}")
            return []

    def _get_chunks_from_pgvector(self, vectorstore, collection_name: str, source_file: str) -> List[Dict]:
        """ğŸ“‹ å¾PGVectorç²å–åˆ†å¡Š"""
        try:
            collection_folder = collection_name.replace('collection_', '')
            possible_paths = [
                source_file,
                f"data/{collection_folder}/{source_file}",
                f"data\\{collection_folder}\\{source_file}"
            ]
            
            all_chunks = []
            
            for search_path in possible_paths:
                try:
                    docs = vectorstore.similarity_search("", k=1000)
                    matching_chunks = []
                    
                    for doc in docs:
                        metadata = doc.metadata
                        doc_filename = metadata.get('filename', metadata.get('original_filename', ''))
                        doc_source = metadata.get('source', '')
                        
                        if (
                            doc_filename == source_file or 
                            doc_source.endswith(source_file) or
                            search_path in doc_source
                        ):
                            
                            chunk_info = {
                                'chunk_id': metadata.get('chunk_id', f'chunk_{len(matching_chunks)+1}'),
                                'chunk_index': metadata.get('chunk_index', len(matching_chunks)),
                                'content': doc.page_content,
                                'content_preview': doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                                'token_count': metadata.get('token_count', 0),
                                'text_type': metadata.get('text_type', 'unknown'),
                                'quality_score': metadata.get('quality_score', 0.5),
                                'metadata': metadata
                            }
                            matching_chunks.append(chunk_info)
                    
                    if matching_chunks:
                        all_chunks = matching_chunks
                        print(f"ğŸ¯ PGVectoræ‰¾åˆ°{len(all_chunks)}å€‹åˆ†å¡Š")
                        break
                        
                except Exception as search_error:
                    logger.warning(f"PGVectoræŸ¥è©¢å¤±æ•—: {search_error}")
                    continue
            
            all_chunks.sort(key=lambda x: x.get('chunk_index', 0))
            return all_chunks
            
        except Exception as e:
            logger.error(f"PGVectoråˆ†å¡Šç²å–å¤±æ•—: {e}")
            return []
    
    def get_chunk_content(self, collection_name: str, chunk_id: str) -> Optional[Dict]:
        """ğŸ“‹ ç²å–æŒ‡å®šåˆ†å¡Šçš„è©³ç´°å…§å®¹"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            # PGVector does not have a 'get' method, so we need to iterate
            if self.use_postgres:
                # This is not efficient, but it's a workaround for now.
                docs = vectorstore.similarity_search("", k=5000)
                for doc in docs:
                    if doc.metadata.get("chunk_id") == chunk_id:
                        # Found the chunk
                        content_stats = {
                            'total_chars': len(doc.page_content),
                            'total_lines': len(doc.page_content.split('\n')),
                            'total_words': len(doc.page_content.split()),
                            'total_sentences': len([s for s in doc.page_content.split('ã€‚') if s.strip()]) + len([s for s in doc.page_content.split('.') if s.strip()])
                        }
                        chunk_detail = {
                            'chunk_id': chunk_id,
                            'chunk_index': doc.metadata.get('chunk_index', 0),
                            'content': doc.page_content,
                            'content_stats': content_stats,
                            'token_count': doc.metadata.get('token_count', 0),
                            'text_type': doc.metadata.get('text_type', 'unknown'),
                            'quality_score': doc.metadata.get('quality_score', 0.5),
                            'language': doc.metadata.get('language', 'unknown'),
                            'source_file': doc.metadata.get('source', 'unknown'),
                            'original_filename': doc.metadata.get('original_filename', doc.metadata.get('filename', 'unknown')),
                            'processing_strategy': doc.metadata.get('processing_strategy', 'unknown'),
                            'split_method': doc.metadata.get('split_method', 'unknown'),
                            'has_overlap': doc.metadata.get('has_overlap', False),
                            'metadata': doc.metadata
                        }
                        return chunk_detail
                return None # Chunk not found
            else: # ChromaDB
                results = vectorstore.get(where={"chunk_id": chunk_id})
                
                if not results or not results.get('documents') or len(results['documents']) == 0:
                    return None
                
                # å–ç¬¬ä¸€å€‹çµæœï¼ˆchunk_idæ‡‰è©²æ˜¯å”¯ä¸€çš„ï¼‰
                doc = results['documents'][0]
                metadata = results['metadatas'][0]
                
                # ğŸ”§ è¨ˆç®—å­—ç¬¦å’Œè¡Œæ•¸çµ±è¨ˆ
                content_stats = {
                    'total_chars': len(doc),
                    'total_lines': len(doc.split('\n')),
                    'total_words': len(doc.split()),
                    'total_sentences': len([s for s in doc.split('ã€‚') if s.strip()]) + len([s for s in doc.split('.') if s.strip()])
                }
                
                chunk_detail = {
                    'chunk_id': chunk_id,
                    'chunk_index': metadata.get('chunk_index', 0),
                    'content': doc,
                    'content_stats': content_stats,
                    'token_count': metadata.get('token_count', 0),
                    'text_type': metadata.get('text_type', 'unknown'),
                    'quality_score': metadata.get('quality_score', 0.5),
                    'language': metadata.get('language', 'unknown'),
                    'source_file': metadata.get('source', 'unknown'),
                    'original_filename': metadata.get('original_filename', metadata.get('filename', 'unknown')),
                    'processing_strategy': metadata.get('processing_strategy', 'unknown'),
                    'split_method': metadata.get('split_method', 'unknown'),
                    'has_overlap': metadata.get('has_overlap', False),
                    'metadata': metadata
                }
                
                return chunk_detail
            
        except Exception as e:
            logger.error(f"ç²å–åˆ†å¡Šå…§å®¹å¤±æ•—{collection_name}/{chunk_id}: {e}")
            return None


    def get_chunk_by_id(self, collection_name: str, chunk_id: str) -> Optional[Dict]:
        """ğŸ“‹ é€šéIDç²å–åˆ†å¡Š (get_chunk_contentçš„åˆ¥å)"""
        return self.get_chunk_content(collection_name, chunk_id)
    
    # ==================== ğŸ—‘ï¸ åˆªé™¤åŠŸèƒ½ (é‡æ§‹å¾Œ) ====================

    def delete_by_file_ids(self, collection_name: str, filename: str) -> Dict:
        """ğŸ—‘ï¸ [æ¥µç°¡ä¿®å¾©ç‰ˆ] åªä¿®å¾©æ ¸å¿ƒå•é¡Œï¼Œä¸æ·»åŠ è¤‡é›œé‚è¼¯"""
        try:
            print(f"ğŸ¯ åˆªé™¤æ–‡æª”: {filename}")
            
            # ğŸ”‘ é—œéµä¿®å¾©ï¼šç¢ºä¿ç²å–æœ‰æ•ˆçš„vectorstore
            vectorstore = self._ensure_valid_vectorstore(collection_name)
            
            # æŸ¥æ‰¾è¦åˆªé™¤çš„æ–‡æª”
            all_docs = vectorstore.similarity_search("", k=5000)
            target_docs = [
                doc for doc in all_docs 
                if (doc.metadata.get('original_filename') == filename or 
                    doc.metadata.get('filename') == filename)
            ]
            
            if not target_docs:
                return {
                    "success": True,
                    "message": f"æ–‡æª” '{filename}' ä¸å­˜åœ¨",
                    "deleted_chunks": 0,
                    "filename": filename
                }
            
            print(f"ğŸ“‹ æ‰¾åˆ° {len(target_docs)} å€‹åˆ†å¡Š")
            
            # ğŸ¯ æ ¸å¿ƒä¿®å¾©ï¼šä½¿ç”¨æœ€å®‰å…¨çš„åˆªé™¤æ–¹æ³•
            success = self._safe_delete_documents(vectorstore, target_docs, filename)
            
            if success:
                return {
                    "success": True,
                    "message": f"æ–‡æª” '{filename}' åˆªé™¤æˆåŠŸ",
                    "deleted_chunks": len(target_docs),
                    "filename": filename
                }
            else:
                return {
                    "success": False,
                    "message": f"æ–‡æª” '{filename}' åˆªé™¤å¤±æ•—",
                    "deleted_chunks": 0,
                    "filename": filename
                }
                
        except Exception as e:
            logger.error(f"åˆªé™¤æ–‡æª”å¤±æ•— {filename}: {e}")
            return {
                "success": False,
                "message": f"åˆªé™¤å¤±æ•—: {str(e)}",
                "deleted_chunks": 0,
                "filename": filename
            }

    def _ensure_valid_vectorstore(self, collection_name: str):
        """ğŸ”‘ ç¢ºä¿vectorstoreæœ‰æ•ˆ - æ¥µç°¡ç‰ˆæœ¬"""
        # å¦‚æœç·©å­˜ä¸­çš„å¯¦ä¾‹æœ‰å•é¡Œï¼Œæ¸…é™¤å®ƒ
        if collection_name in self._vector_stores:
            try:
                cached_store = self._vector_stores[collection_name]
                # å¿«é€Ÿæ¸¬è©¦
                cached_store.similarity_search("", k=1)
                return cached_store
            except Exception as e:
                print(f"âš ï¸ æ¸…é™¤ç„¡æ•ˆç·©å­˜: {e}")
                del self._vector_stores[collection_name]
        
        # ä½¿ç”¨åŸæœ‰çš„å‰µå»ºé‚è¼¯ï¼ˆä¸é‡è¤‡ä»£ç¢¼ï¼‰
        return self.get_or_create_vectorstore(collection_name)

    def _safe_delete_documents(self, vectorstore, target_docs: List, filename: str) -> bool:
        """ğŸ›¡ï¸ å®‰å…¨åˆªé™¤æ–‡æª” - é¿å…collectionç´šæ“ä½œ"""
        try:
            # æ–¹æ³•1ï¼šwhereæ¢ä»¶åˆªé™¤ï¼ˆChromaå‹å¥½ï¼‰
            if hasattr(vectorstore, 'delete') and not self.use_postgres:
                try:
                    vectorstore.delete(where={"filename": filename})
                    print("âœ… Whereæ¢ä»¶åˆªé™¤æˆåŠŸ")
                    return True
                except Exception as e:
                    print(f"âš ï¸ Whereæ¢ä»¶åˆªé™¤å¤±æ•—: {e}")
            
            # æ–¹æ³•2ï¼šå˜—è©¦ä½¿ç”¨çœŸå¯¦çš„æ–‡æª”IDï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(vectorstore, 'delete'):
                try:
                    # ğŸ”‘ é—œéµï¼šåªä½¿ç”¨vectorstoreæä¾›çš„çœŸå¯¦ID
                    if hasattr(vectorstore, 'get'):
                        # å°æ–¼Chroma
                        existing_docs = vectorstore.get(where={"filename": filename})
                        if existing_docs and existing_docs.get('ids'):
                            vectorstore.delete(ids=existing_docs['ids'])
                            print("âœ… çœŸå¯¦IDåˆªé™¤æˆåŠŸ")
                            return True
                except Exception as e:
                    print(f"âš ï¸ çœŸå¯¦IDåˆªé™¤å¤±æ•—: {e}")
            
            # å¦‚æœéƒ½å¤±æ•—äº†ï¼Œä¸é€²è¡Œå±éšªæ“ä½œ
            print("âŒ å®‰å…¨åˆªé™¤æ–¹æ³•éƒ½å¤±æ•—ï¼Œæ‹’çµ•é€²è¡Œå±éšªæ“ä½œ")
            return False
            
        except Exception as e:
            print(f"âŒ å®‰å…¨åˆªé™¤å¤±æ•—: {e}")
            return False
    
    def _ensure_valid_vectorstore(self, collection_name: str):
        """ğŸ”‘ ç¢ºä¿vectorstoreæœ‰æ•ˆ - æ¥µç°¡ç‰ˆæœ¬"""
        # å¦‚æœç·©å­˜ä¸­çš„å¯¦ä¾‹æœ‰å•é¡Œï¼Œæ¸…é™¤å®ƒ
        if collection_name in self._vector_stores:
            try:
                cached_store = self._vector_stores[collection_name]
                # å¿«é€Ÿæ¸¬è©¦
                cached_store.similarity_search("", k=1)
                return cached_store
            except Exception as e:
                print(f"âš ï¸ æ¸…é™¤ç„¡æ•ˆç·©å­˜: {e}")
                del self._vector_stores[collection_name]
        
        # ä½¿ç”¨åŸæœ‰çš„å‰µå»ºé‚è¼¯ï¼ˆä¸é‡è¤‡ä»£ç¢¼ï¼‰
        return self.get_or_create_vectorstore(collection_name)

    def _safe_delete_documents(self, vectorstore, target_docs: List, filename: str) -> bool:
        """ğŸ›¡ï¸ å®‰å…¨åˆªé™¤æ–‡æª” - é¿å…collectionç´šæ“ä½œ"""
        try:
            # æ–¹æ³•1ï¼šwhereæ¢ä»¶åˆªé™¤ï¼ˆChromaå‹å¥½ï¼‰
            if hasattr(vectorstore, 'delete') and not self.use_postgres:
                try:
                    vectorstore.delete(where={"filename": filename})
                    print("âœ… Whereæ¢ä»¶åˆªé™¤æˆåŠŸ")
                    return True
                except Exception as e:
                    print(f"âš ï¸ Whereæ¢ä»¶åˆªé™¤å¤±æ•—: {e}")
            
            # æ–¹æ³•2ï¼šå˜—è©¦ä½¿ç”¨çœŸå¯¦çš„æ–‡æª”IDï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(vectorstore, 'delete'):
                try:
                    # ğŸ”‘ é—œéµï¼šåªä½¿ç”¨vectorstoreæä¾›çš„çœŸå¯¦ID
                    if hasattr(vectorstore, 'get'):
                        # å°æ–¼Chroma
                        existing_docs = vectorstore.get(where={"filename": filename})
                        if existing_docs and existing_docs.get('ids'):
                            vectorstore.delete(ids=existing_docs['ids'])
                            print("âœ… çœŸå¯¦IDåˆªé™¤æˆåŠŸ")
                            return True
                except Exception as e:
                    print(f"âš ï¸ çœŸå¯¦IDåˆªé™¤å¤±æ•—: {e}")
            
            # å¦‚æœéƒ½å¤±æ•—äº†ï¼Œä¸é€²è¡Œå±éšªæ“ä½œ
            print("âŒ å®‰å…¨åˆªé™¤æ–¹æ³•éƒ½å¤±æ•—ï¼Œæ‹’çµ•é€²è¡Œå±éšªæ“ä½œ")
            return False
            
        except Exception as e:
            print(f"âŒ å®‰å…¨åˆªé™¤å¤±æ•—: {e}")
            return False



    
    # ==================== ğŸ“Š çµ±è¨ˆæŸ¥è©¢åŠŸèƒ½ ====================
    
    def get_stats(self) -> Dict:
        """ğŸ“Š ç²å–ç³»çµ±çµ±è¨ˆ - ç´”PostgreSQLç‰ˆæœ¬"""
        try:
            stats = {}
            
            # å¾æ‰€æœ‰å·²çŸ¥é›†åˆç²å–çµ±è¨ˆ
            collections = self.get_available_collections()
            
            for collection_info in collections:
                collection_name = collection_info['collection_name']
                display_name = collection_info['display_name']
                
                try:
                    vectorstore = self.get_or_create_vectorstore(collection_name)
                    
                    # ç›´æ¥æŸ¥è©¢PostgreSQLè¨ˆç®—æ–‡æª”æ•¸
                    docs = vectorstore.similarity_search("", k=5000)
                    stats[display_name] = len(docs)
                    
                except Exception as e:
                    logger.warning(f"ç²å–é›†åˆçµ±è¨ˆå¤±æ•—{collection_name}: {e}")
                    stats[display_name] = 0
            
            return stats
        except Exception as e:
            logger.error(f"ç²å–çµ±è¨ˆå¤±æ•—: {e}")
            return {}

    def get_available_collections(self) -> List[Dict]:
        """ğŸ“Š ç²å–æ‰€æœ‰å¯ç”¨çš„é›†åˆåˆ—è¡¨ - ç´”PostgreSQLç‰ˆæœ¬"""
        try:
            collections = []
            
            # å¾ç’°å¢ƒæˆ–é…ç½®ä¸­ç²å–å·²çŸ¥é›†åˆ
            known_collections = ["collection_test_01", "collection_test", "collection_default"]
            
            for collection_name in known_collections:
                display_name = collection_name.replace('collection_', '')
                
                try:
                    vectorstore = self.get_or_create_vectorstore(collection_name)
                    docs = vectorstore.similarity_search("", k=100)
                    
                    if docs:  # åªæœ‰ç•¶é›†åˆä¸­æœ‰æ–‡æª”æ™‚æ‰åŠ å…¥
                        doc_count = len(docs)
                        
                        # è¨ˆç®—æ–‡ä»¶æ•¸ï¼ˆæŒ‰æª”åå»é‡ï¼‰
                        unique_files = set()
                        for doc in docs:
                            filename = doc.metadata.get('original_filename') or doc.metadata.get('filename', 'unknown')
                            if filename != 'unknown':
                                unique_files.add(filename)
                        
                        collections.append({
                            'collection_name': collection_name,
                            'display_name': display_name,
                            'document_count': doc_count,
                            'file_count': len(unique_files),
                            'status': 'active'
                        })
                        
                except Exception as e:
                    logger.warning(f"æª¢æŸ¥é›†åˆå¤±æ•—{collection_name}: {e}")
            
            collections.sort(key=lambda x: x['display_name'])
            return collections
            
        except Exception as e:
            logger.error(f"ç²å–é›†åˆåˆ—è¡¨å¤±æ•—: {e}")
            return []
        

    def test_delete_capability(self, collection_name: str = "test_collection") -> Dict:
        """ğŸ§ª æ¸¬è©¦åˆªé™¤åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            capabilities = {
                "vectorstore_type": type(vectorstore).__name__,
                "has_delete_method": hasattr(vectorstore, 'delete'),
                "has_delete_collection": hasattr(vectorstore, 'delete_collection'),
                "has_clear_method": hasattr(vectorstore, 'clear'),
                "recommended_method": "unknown"
            }
            
            if hasattr(vectorstore, 'delete'):
                capabilities["recommended_method"] = "standard_api"
            elif hasattr(vectorstore, 'delete_collection'):
                capabilities["recommended_method"] = "collection_rebuild"
            else:
                capabilities["recommended_method"] = "manual_rebuild"
            
            # æ¸¬è©¦åŸºæœ¬æ“ä½œ
            try:
                # å˜—è©¦æœç´¢æ“ä½œ
                test_results = vectorstore.similarity_search("test", k=1)
                capabilities["basic_search_works"] = True
            except Exception as e:
                capabilities["basic_search_works"] = False
                capabilities["search_error"] = str(e)
            
            return {
                "success": True,
                "capabilities": capabilities,
                "message": "åˆªé™¤åŠŸèƒ½è¨ºæ–·å®Œæˆ"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": "åˆªé™¤åŠŸèƒ½è¨ºæ–·å¤±æ•—"
            }
    
    # ==================== ğŸ”§ ç³»çµ±è¨ºæ–·åŠŸèƒ½ ====================
    
    def diagnose_system(self) -> Dict:
        """ğŸ”§ ç³»çµ±è¨ºæ–·"""
        print("ğŸ” === ç³»çµ±è¨ºæ–· ===")
        
        diagnosis = {
            "environment": {},
            "embedding_model": {},
            "text_processing": {},
            "performance": {},
            "recommendations": []
        }
        
        # ç’°å¢ƒæª¢æŸ¥
        api_key = os.getenv("OPENAI_API_KEY")
        diagnosis["environment"]["openai_api_key"] = "âœ… å·²è¨­ç½®" if api_key else "âŒ æœªè¨­ç½®"
        diagnosis["environment"]["model_type"] = self.model_type
        
        # åµŒå…¥æ¨¡å‹æª¢æŸ¥
        try:
            test_result = self.embeddings.embed_query("æ¸¬è©¦")
            diagnosis["embedding_model"]["status"] = "âœ… æ­£å¸¸"
            diagnosis["embedding_model"]["dimension"] = len(test_result)
        except Exception as e:
            diagnosis["embedding_model"]["status"] = f"âŒ å¤±æ•—: {e}"
        
        # æ–‡æœ¬è™•ç†æª¢æŸ¥
        diagnosis["text_processing"]["normalizer"] = "âœ… æ­£å¸¸" if hasattr(self, 'normalizer') else "âŒ ç•°å¸¸"
        diagnosis["text_processing"]["analyzer"] = "âœ… æ­£å¸¸" if hasattr(self, 'analyzer') else "âŒ ç•°å¸¸"
        diagnosis["text_processing"]["splitter"] = "âœ… æ­£å¸¸" if hasattr(self, 'text_splitter') else "âŒ ç•°å¸¸"
        
        # æ€§èƒ½çµ±è¨ˆ
        if hasattr(self, 'batch_processor'):
            perf_stats = self.batch_processor.get_performance_stats()
            diagnosis["performance"] = perf_stats
        else:
            diagnosis["performance"] = {"status": "æœªåˆå§‹åŒ–"}
        
        # å»ºè­°
        if not api_key:
            diagnosis["recommendations"].append("è¨­ç½®OPENAI_API_KEYç’°å¢ƒè®Šæ•¸")
        
        perf_success_rate = diagnosis["performance"].get("success_rate", 1)
        if perf_success_rate < 0.8:
            diagnosis["recommendations"].append("æˆåŠŸç‡åä½ï¼Œå»ºè­°æª¢æŸ¥ç¶²è·¯é€£æ¥å’ŒAPIé…é¡")
        
        # è¼¸å‡ºè¨ºæ–·çµæœ
        for category, info in diagnosis.items():
            if category != "recommendations":
                print(f"\nğŸ”§ {category.upper()}:")
                for key, value in info.items():
                    print(f"   {key}: {value}")
        
        if diagnosis["recommendations"]:
            print(f"\nğŸ’¡ å»ºè­°:")
            for rec in diagnosis["recommendations"]:
                print(f"   â€¢ {rec}")
        
        return diagnosis

    def debug_collection_content(self, collection_name: str, filename: str = None):
        """ğŸ”§ èª¿è©¦é›†åˆå…§å®¹"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            print(f"\nğŸ” èª¿è©¦é›†åˆ: {collection_name}")
            
            if filename:
                print(f"ğŸ¯ ç‰¹å®šæª”æ¡ˆ: {filename}")
                docs = vectorstore.similarity_search("", k=1000)
                matching_docs = [
                    doc for doc in docs 
                    if doc.metadata.get('filename') == filename or 
                       doc.metadata.get('original_filename') == filename
                ]
                docs_to_show = matching_docs[:5]
                print(f"ğŸ“„ æ‰¾åˆ° {len(matching_docs)} å€‹ç›¸é—œæ–‡æª”")
            else:
                docs = vectorstore.similarity_search("", k=5)
                docs_to_show = docs
                print(f"ğŸ“„ é›†åˆä¸­å…±æœ‰ {len(docs)} å€‹æ–‡æª”")
            
            for i, doc in enumerate(docs_to_show):
                print(f"\nğŸ“„ æ–‡æª” {i+1}:")
                print(f"   å…§å®¹é è¦½: {doc.page_content[:100]}...")
                print(f"   å…ƒæ•¸æ“š:")
                for key, value in doc.metadata.items():
                    if len(str(value)) > 100:
                        print(f"     {key}: {str(value)[:100]}...")
                    else:
                        print(f"     {key}: {value}")
                        
        except Exception as e:
            print(f"âŒ èª¿è©¦å¤±æ•—: {e}")

    def test_knowledge_management(self):
        """ğŸ§ª æ¸¬è©¦çŸ¥è­˜ç®¡ç†ç³»çµ±åŠŸèƒ½"""
        print("\n" + "="*60)
        print("ğŸ§ª çŸ¥è­˜ç®¡ç†ç³»çµ±åŠŸèƒ½æ¸¬è©¦")
        print("="*60)
        
        try:
            # æ¸¬è©¦åŸºæœ¬åŠŸèƒ½
            print("ğŸ“Š ç²å–ç³»çµ±çµ±è¨ˆ...")
            stats = self.get_stats()
            print(f"   æ‰¾åˆ° {len(stats)} å€‹é›†åˆ")
            
            print("ğŸ“‹ ç²å–é›†åˆåˆ—è¡¨...")
            collections = self.get_available_collections()
            print(f"   æ‰¾åˆ° {len(collections)} å€‹æ´»èºé›†åˆ")
            
            # æ¸¬è©¦æœç´¢åŠŸèƒ½
            if collections:
                first_collection = collections[0]['collection_name']
                print(f"ğŸ” æ¸¬è©¦æœç´¢åŠŸèƒ½ (é›†åˆ: {first_collection})...")
                search_results = self.search("æ¸¬è©¦", first_collection, k=3)
                print(f"   æ‰¾åˆ° {len(search_results)} å€‹æœç´¢çµæœ")
            
            print("âœ… åŸºæœ¬åŠŸèƒ½æ¸¬è©¦å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ¸¬è©¦éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
            logger.error(f"æ¸¬è©¦å¤±æ•—: {e}")


# âœ… ç¢ºä¿å‘å¾Œå…¼å®¹æ€§çš„åŒ¯å‡º
__all__ = ['OptimizedVectorSystem']

"""
ğŸ“‹ 4Bæª”æ¡ˆ (management_interface.py) åŠŸèƒ½ç¸½çµ:

ğŸ”— ä¾è³´é—œä¿‚: ç¹¼æ‰¿è‡ª VectorOperationsCore (4Aæª”æ¡ˆ)
ğŸ“Š åŒ…å«æ–¹æ³•: 45å€‹å®Œæ•´çš„ç®¡ç†APIæ–¹æ³•
ğŸ”§ æ ¸å¿ƒåŠŸèƒ½: 
   - ğŸ” æœç´¢åŠŸèƒ½ (1å€‹æ–¹æ³•)
   - ğŸ“¤ æ–‡ä»¶ä¸Šå‚³ (1å€‹æ–¹æ³•)  
   - ğŸ“‹ æ–‡æª”æŸ¥è©¢ (7å€‹æ–¹æ³•)
   - ğŸ—‘ï¸ åˆªé™¤åŠŸèƒ½ (8å€‹ä¸»è¦æ–¹æ³•)
   - ğŸ“Š çµ±è¨ˆæŸ¥è©¢ (2å€‹æ–¹æ³•)
   - ğŸ”§ ç³»çµ±è¨ºæ–· (2å€‹æ–¹æ³•)

âœ… å‘å¾Œå…¼å®¹: OptimizedVectorSystem å®Œå…¨å…¼å®¹ç¾æœ‰ä»£ç¢¼
ğŸ”’ å°å…¥å®‰å…¨: æ™ºèƒ½å›é€€æ©Ÿåˆ¶ï¼Œç¢ºä¿å¾4Aæª”æ¡ˆæ­£ç¢ºç²å–é…ç½®
ğŸ“¦ æ¨¡å¡ŠåŒ–è¨­è¨ˆ: æ¸…æ™°çš„è·è²¬åˆ†é›¢ï¼Œä¾¿æ–¼ç¶­è­·å’Œæ“´å±•
"""