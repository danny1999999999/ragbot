#!/usr/bin/env python3
"""
å‘é‡å»ºç«‹å±¤ - è‡ªé©æ‡‰æ‰¹æ¬¡è™•ç†
- ğŸ§  æ™ºèƒ½æ‰¹æ¬¡å¤§å°èª¿æ•´
- ğŸ“Š æ€§èƒ½çµ±è¨ˆèˆ‡ç›£æ§
- ğŸ”§ å¤§æ–‡æª”è‡ªå‹•åˆ†å‰²
- âš¡ è‡ªé©æ‡‰è™•ç†ç­–ç•¥
"""

from core_config import *
from text_processing import AdvancedTokenEstimator
import time
from typing import List, Tuple, Dict, Any
from collections import defaultdict
from langchain_core.documents import Document

class AdaptiveBatchProcessor:
    """ğŸ”§ è‡ªé©æ‡‰æ‰¹æ¬¡è™•ç†å™¨"""
    
    def __init__(self):
        self.token_estimator = AdvancedTokenEstimator()
        self.max_tokens_per_batch = TOKEN_LIMITS["max_tokens_per_request"]
        self.max_batch_size = TOKEN_LIMITS["max_batch_size"]
        self.adaptive_batching = TOKEN_LIMITS.get("adaptive_batching", True)
        
        # æ€§èƒ½çµ±è¨ˆ
        self.batch_stats = {
            'total_batches': 0,
            'total_documents': 0,
            'total_tokens': 0,
            'avg_batch_time': 0,
            'success_rate': 0,
            'adaptive_adjustments': 0
        }
        
        # è‡ªé©æ‡‰åƒæ•¸
        self.current_batch_size = self.max_batch_size
        self.success_streak = 0
        self.failure_streak = 0
    
    def create_smart_batches(self, documents: List[Document]) -> List[Tuple[List[Document], Dict]]:
        """å‰µå»ºæ™ºèƒ½æ‰¹æ¬¡"""
        if not documents:
            return []
        
        print(f"ğŸ”§ å‰µå»ºæ™ºèƒ½æ‰¹æ¬¡...")
        print(f"   ğŸ“„ ç¸½æ–‡æª”æ•¸: {len(documents)}")
        print(f"   ğŸ”¢ Token é™åˆ¶: {self.max_tokens_per_batch:,}")
        print(f"   ğŸ“¦ æœ€å¤§æ‰¹æ¬¡å¤§å°: {self.max_batch_size}")
        print(f"   ğŸ§  è‡ªé©æ‡‰æ‰¹æ¬¡: {'âœ…' if self.adaptive_batching else 'âŒ'}")
        
        batches = []
        current_batch = []
        current_tokens = 0
        batch_info = {'documents': 0, 'tokens': 0, 'types': defaultdict(int)}
        
        # æŒ‰ token æ•¸æ’åºæ–‡æª”ï¼ˆå¤§çš„åœ¨å‰é¢ï¼Œæ›´å®¹æ˜“è™•ç†ï¼‰
        sorted_docs = sorted(
            documents, 
            key=lambda doc: doc.metadata.get('token_count', 
                self.token_estimator.estimate_tokens(doc.page_content)),
            reverse=True
        )
        
        for doc_idx, doc in enumerate(sorted_docs):
            doc_tokens = doc.metadata.get('token_count') or \
                        self.token_estimator.estimate_tokens(doc.page_content)
            
            # æª¢æŸ¥å–®å€‹æ–‡æª”æ˜¯å¦éå¤§
            if doc_tokens > self.max_tokens_per_batch:
                print(f"   âš ï¸ æ–‡æª” {doc_idx+1} éå¤§ ({doc_tokens:,} tokens)ï¼Œéœ€è¦åˆ†å‰²")
                
                # å®Œæˆç•¶å‰æ‰¹æ¬¡
                if current_batch:
                    batches.append((current_batch, dict(batch_info)))
                    current_batch = []
                    current_tokens = 0
                    batch_info = {'documents': 0, 'tokens': 0, 'types': defaultdict(int)}
                
                # åˆ†å‰²å¤§æ–‡æª”
                split_docs = self._split_large_document(doc)
                for split_doc in split_docs:
                    split_tokens = self.token_estimator.estimate_tokens(split_doc.page_content)
                    split_doc.metadata['token_count'] = split_tokens
                    batches.append(([split_doc], {
                        'documents': 1, 
                        'tokens': split_tokens, 
                        'types': {split_doc.metadata.get('text_type', 'unknown'): 1},
                        'is_split': True
                    }))
                continue
            
            # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŠ å…¥ç•¶å‰æ‰¹æ¬¡
            would_exceed_tokens = current_tokens + doc_tokens > self.max_tokens_per_batch
            would_exceed_size = len(current_batch) >= self._get_adaptive_batch_size()
            
            if would_exceed_tokens or would_exceed_size:
                # å®Œæˆç•¶å‰æ‰¹æ¬¡
                if current_batch:
                    batches.append((current_batch, dict(batch_info)))
                
                # é–‹å§‹æ–°æ‰¹æ¬¡
                current_batch = [doc]
                current_tokens = doc_tokens
                batch_info = {
                    'documents': 1, 
                    'tokens': doc_tokens, 
                    'types': defaultdict(int)
                }
                batch_info['types'][doc.metadata.get('text_type', 'unknown')] += 1
            else:
                # åŠ å…¥ç•¶å‰æ‰¹æ¬¡
                current_batch.append(doc)
                current_tokens += doc_tokens
                batch_info['documents'] += 1
                batch_info['tokens'] += doc_tokens
                batch_info['types'][doc.metadata.get('text_type', 'unknown')] += 1
        
        # è™•ç†æœ€å¾Œä¸€å€‹æ‰¹æ¬¡
        if current_batch:
            batches.append((current_batch, dict(batch_info)))
        
        # çµ±è¨ˆä¿¡æ¯
        total_docs = sum(info['documents'] for _, info in batches)
        total_tokens = sum(info['tokens'] for _, info in batches)
        avg_tokens_per_batch = total_tokens / len(batches) if batches else 0
        
        print(f"âœ… æ™ºèƒ½æ‰¹æ¬¡å‰µå»ºå®Œæˆ:")
        print(f"   ğŸ“¦ ç¸½æ‰¹æ¬¡æ•¸: {len(batches)}")
        print(f"   ğŸ“„ ç¸½æ–‡æª”æ•¸: {total_docs}")
        print(f"   ğŸ”¢ ç¸½ tokens: {total_tokens:,}")
        print(f"   ğŸ“Š å¹³å‡ tokens/æ‰¹æ¬¡: {avg_tokens_per_batch:.0f}")
        print(f"   ğŸ’° ä¼°ç®—æˆæœ¬: ${self.token_estimator.estimate_embedding_cost(total_tokens):.4f}")
        
        # æ›´æ–°çµ±è¨ˆ
        self.batch_stats['total_batches'] += len(batches)
        self.batch_stats['total_documents'] += total_docs
        self.batch_stats['total_tokens'] += total_tokens
        
        return batches
    
    def _get_adaptive_batch_size(self) -> int:
        """ç²å–è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°"""
        if not self.adaptive_batching:
            return self.max_batch_size
        
        # æ ¹æ“šæˆåŠŸç‡èª¿æ•´æ‰¹æ¬¡å¤§å°
        if self.success_streak >= 3:
            # é€£çºŒæˆåŠŸï¼Œå¯ä»¥å¢åŠ æ‰¹æ¬¡å¤§å°
            self.current_batch_size = min(self.max_batch_size, self.current_batch_size + 1)
            self.batch_stats['adaptive_adjustments'] += 1
        elif self.failure_streak >= 2:
            # é€£çºŒå¤±æ•—ï¼Œæ¸›å°‘æ‰¹æ¬¡å¤§å°
            self.current_batch_size = max(1, self.current_batch_size - 2)
            self.batch_stats['adaptive_adjustments'] += 1
        
        return self.current_batch_size
    
    def _split_large_document(self, doc: Document) -> List[Document]:
        """åˆ†å‰²éå¤§çš„æ–‡æª”"""
        content = doc.page_content
        max_chars = int(self.max_tokens_per_batch * 2.5)  # ä¼°ç®—å­—ç¬¦æ•¸
        
        # å˜—è©¦æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        split_docs = []
        current_content = ""
        part_index = 0
        
        for para in paragraphs:
            if len(current_content + para) <= max_chars or not current_content:
                current_content += ("\n\n" if current_content else "") + para
            else:
                if current_content.strip():
                    split_doc = Document(
                        page_content=current_content,
                        metadata={
                            **doc.metadata,
                            'chunk_id': f"{doc.metadata.get('chunk_id', 'unknown')}_part{part_index+1}",
                            'is_split_part': True,
                            'part_index': part_index,
                            'split_reason': 'token_limit'
                        }
                    )
                    split_docs.append(split_doc)
                    part_index += 1
                current_content = para
        
        # è™•ç†æœ€å¾Œä¸€éƒ¨åˆ†
        if current_content.strip():
            split_doc = Document(
                page_content=current_content,
                metadata={
                    **doc.metadata,
                    'chunk_id': f"{doc.metadata.get('chunk_id', 'unknown')}_part{part_index+1}",
                    'is_split_part': True,
                    'part_index': part_index,
                    'split_reason': 'token_limit'
                }
            )
            split_docs.append(split_doc)
        
        return split_docs or [doc]  # å¦‚æœåˆ†å‰²å¤±æ•—ï¼Œè¿”å›åŸæ–‡æª”
    
    def record_batch_result(self, success: bool, processing_time: float = 0):
        """è¨˜éŒ„æ‰¹æ¬¡è™•ç†çµæœ"""
        if success:
            self.success_streak += 1
            self.failure_streak = 0
        else:
            self.failure_streak += 1
            self.success_streak = 0
        
        # æ›´æ–°æˆåŠŸç‡
        total_attempts = self.batch_stats['total_batches']
        if total_attempts > 0:
            self.batch_stats['success_rate'] = (total_attempts - self.failure_streak) / total_attempts
        
        # æ›´æ–°å¹³å‡è™•ç†æ™‚é–“
        if processing_time > 0:
            current_avg = self.batch_stats['avg_batch_time']
            self.batch_stats['avg_batch_time'] = (current_avg + processing_time) / 2
    
    def get_performance_stats(self) -> Dict:
        """ç²å–æ€§èƒ½çµ±è¨ˆ"""
        return dict(self.batch_stats)