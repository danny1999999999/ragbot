#!/usr/bin/env python3
import os
import time
import hashlib
from dataclasses import dataclass
from typing import Dict, List, Optional, Any, Set, Tuple
from pathlib import Path

# ğŸ†• 1. æ·»åŠ  Railway ç’°å¢ƒæª¢æ¸¬å‡½æ•¸ï¼ˆæ”¾åœ¨æ–‡ä»¶é ‚éƒ¨ï¼Œå°å…¥èªå¥ä¹‹å¾Œï¼‰
def detect_railway_environment():
    """æª¢æ¸¬æ˜¯å¦åœ¨ Railway ç’°å¢ƒä¸­é‹è¡Œ"""
    railway_indicators = [
        os.getenv("RAILWAY_PROJECT_ID"),
        os.getenv("RAILWAY_SERVICE_ID"), 
        os.getenv("DATABASE_URL"),
        "railway.internal" in os.getenv("POSTGRES_HOST", "")
    ]
    
    is_railway = any(railway_indicators)
    if is_railway:
        print("ğŸš‚ æª¢æ¸¬åˆ° Railway éƒ¨ç½²ç’°å¢ƒ")
        # é¡¯ç¤º Railway ç‰¹å®šä¿¡æ¯
        project_id = os.getenv("RAILWAY_PROJECT_ID", "unknown")
        service_id = os.getenv("RAILWAY_SERVICE_ID", "unknown")  
        print(f"   é …ç›®ID: {project_id[:8]}***")
        print(f"   æœå‹™ID: {service_id[:8]}***")
    
    return is_railway

# ğŸ”§ å„ªåŒ–ç‰ˆç³»çµ±é…ç½®
SYSTEM_CONFIG = {
    "persist_dir": "./chroma_langchain_db",
    "data_dir": "./data", 
    "device": "cpu",  
    "log_level": "INFO",
    "max_workers": 4,  # ä¸¦ç™¼è™•ç†æ•¸
    "cache_embeddings": True,  # å¿«å–åµŒå…¥å‘é‡
    "backup_enabled": True  # è‡ªå‹•å‚™ä»½
}

# ğŸ§  æ™ºèƒ½æ–‡æœ¬åˆ†é¡é…ç½®
SMART_TEXT_CONFIG = {
    # æ–‡æœ¬é•·åº¦é–¾å€¼ï¼ˆå­—ç¬¦æ•¸ï¼‰
    "micro_text_threshold": 100,      # å¾®æ–‡æœ¬ï¼šæ¨™é¡Œã€æ‘˜è¦ç­‰
    "short_text_threshold": 500,      # çŸ­æ–‡æœ¬ï¼šçŸ­æ–‡ç« ã€æ®µè½
    "medium_text_threshold": 2000,    # ä¸­ç­‰æ–‡æœ¬ï¼šä¸­ç¯‡æ–‡ç« 
    "long_text_threshold": 8000,      # é•·æ–‡æœ¬ï¼šé•·ç¯‡æ–‡ç« 
    "ultra_long_threshold": 20000,    # è¶…é•·æ–‡æœ¬ï¼šæ›¸ç±ç« ç¯€
    
    # å„é¡æ–‡æœ¬çš„è™•ç†ç­–ç•¥
    "micro_text": {
        "min_length": 20,
        "process_whole": True,
        "merge_with_next": True,  # å˜—è©¦èˆ‡ä¸‹ä¸€å€‹ç‰‡æ®µåˆä½µ
        "chunk_size": 0,
        "description": "å¾®æ–‡æœ¬æ•´é«”è™•ç†"
    },
    
    "short_text": {
        "min_length": 50,
        "process_whole": True,
        "preserve_structure": True,
        "chunk_size": 0,
        "allow_merge": True,  # å…è¨±åˆä½µç›¸é„°çŸ­æ–‡æœ¬
        "description": "çŸ­æ–‡æœ¬æ•´é«”ä¿å­˜"
    },
    
    "medium_text": {
        "chunk_size": 800,
        "chunk_overlap": 120,
        "preserve_paragraphs": True,
        "smart_boundaries": True,  # æ™ºèƒ½é‚Šç•Œæª¢æ¸¬
        "min_chunk_ratio": 0.3,   # æœ€å°ç‰‡æ®µæ¯”ä¾‹
        "description": "ä¸­ç­‰æ–‡æœ¬æ®µè½æ„ŸçŸ¥åˆ†å‰²"
    },
    
    "long_text": {
        "chunk_size": 500,
        "chunk_overlap": 80,
        "hierarchical_split": True,
        "section_aware": True,    # ç« ç¯€æ„ŸçŸ¥
        "quality_check": True,    # è³ªé‡æª¢æŸ¥
        "description": "é•·æ–‡æœ¬ç²¾ç´°åŒ–åˆ†å‰²"
    },
    
    "ultra_long": {
        "chunk_size": 600,
        "chunk_overlap": 100,
        "multi_level_split": True,  # å¤šå±¤ç´šåˆ†å‰²
        "chapter_detection": True,  # ç« ç¯€æª¢æ¸¬
        "summary_chunks": True,     # ç”Ÿæˆæ‘˜è¦ç‰‡æ®µ
        "description": "è¶…é•·æ–‡æœ¬éšå±¤å¼è™•ç†"
    },
    
    "mega_text": {
        "chunk_size": 500,
        "chunk_overlap": 80,
        "multi_level_split": True,  # å¤šå±¤ç´šåˆ†å‰²
        "chapter_detection": True,  # ç« ç¯€æª¢æ¸¬
        "aggressive_split": True,   # ç©æ¥µåˆ†å‰²
        "quality_filter": True,     # è³ªé‡éæ¿¾
        "description": "è¶…å¤§æ–‡æœ¬ç©æ¥µåˆ†å‰²è™•ç†"
    }
}

# ğŸ”§ å„ªåŒ–ç‰ˆ Token é™åˆ¶é…ç½®
TOKEN_LIMITS = {
    "max_tokens_per_request": 150000,  # æ›´ä¿å®ˆçš„é™åˆ¶
    "max_batch_size": 8,               # æ¸›å°æ‰¹æ¬¡å¤§å°
    "min_batch_size": 1,
    "batch_delay": 5.0,                # å¢åŠ æ‰¹æ¬¡é–“å»¶é²
    "retry_delay": 10,
    "max_retries": 3,
    "token_safety_margin": 0.2,        # 20% å®‰å…¨é‚Šéš›
    "adaptive_batching": True           # è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°
}

# ğŸ”§ æ€§èƒ½å„ªåŒ–é…ç½®
PERFORMANCE_CONFIG = {
    "embedding_batch_size": 12,
    "parallel_processing": True,
    "memory_limit_mb": 1024,      # è¨˜æ†¶é«”é™åˆ¶
    "chunk_cache_size": 1000,     # åˆ†å¡Šå¿«å–å¤§å°
    "preload_models": True,       # é è¼‰å…¥æ¨¡å‹
    "gc_frequency": 50            # åƒåœ¾å›æ”¶é »ç‡
}

# æ”¯æŒçš„æ–‡ä»¶æ ¼å¼æ“´å±•
SUPPORTED_EXTENSIONS = {
    '.txt', '.md', '.pdf', '.csv', '.json', '.py', '.js', 
    '.docx', '.doc', '.epub', '.rst', '.org'
}

from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from langchain_community.vectorstores import Chroma

try:
    from langchain_community.vectorstores import Chroma
    CHROMA_AVAILABLE = True
except ImportError:
    CHROMA_AVAILABLE = False
    # å‰µå»ºä¸€å€‹è™›æ“¬çš„ Chroma é¡å‹ç”¨æ–¼é¡å‹è¨»è§£
    class Chroma:
        pass

# ğŸ”§ æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
def check_openai_api_key():
    """æª¢æŸ¥ OpenAI API Key"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸ æœªè¨­ç½® OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
        print("   è«‹åœ¨ .env æ–‡ä»¶ä¸­è¨­ç½®: OPENAI_API_KEY=sk-your-api-key")
        return False
    
    if api_key.startswith("sk-") and len(api_key) > 20:
        print("âœ… OpenAI API Key æ ¼å¼æ­£ç¢º")
        print(f"   API Key: {api_key[:8]}***{api_key[-4:]}")
        return True
    else:
        print("âš ï¸ OpenAI API Key æ ¼å¼å¯èƒ½ä¸æ­£ç¢º")
        return False

# LangChain æ ¸å¿ƒçµ„ä»¶å°å…¥
try:
    from langchain_postgres import PGVector
    PGVECTOR_AVAILABLE = True
    print("âœ… PGVector å¯ç”¨")
except ImportError:
    try:
        from langchain_community.vectorstores import PGVector
        PGVECTOR_AVAILABLE = True
        print("âœ… PGVector (community) å¯ç”¨")
    except ImportError:
        PGVECTOR_AVAILABLE = False
        print("âŒ PGVector ä¸å¯ç”¨")
        # å¯ä»¥å›é€€åˆ° Chroma
        from langchain_community.vectorstores import Chroma

# OpenAI embeddings å°å…¥
try:
    from langchain_openai import OpenAIEmbeddings
    OPENAI_EMBEDDINGS_AVAILABLE = True
    print("âœ… OpenAI Embeddings å¯ç”¨")
    
    if check_openai_api_key():
        print("âœ… OpenAI ç’°å¢ƒæª¢æŸ¥é€šé")
    else:
        print("âš ï¸ OpenAI ç’°å¢ƒé…ç½®å¯èƒ½æœ‰å•é¡Œ")
        
except ImportError as e:
    OPENAI_EMBEDDINGS_AVAILABLE = False
    print(f"âš ï¸ OpenAI Embeddings ä¸å¯ç”¨: {e}")

# ğŸ”§ PostgreSQL ä¾è³´æª¢æŸ¥
try:
    import psycopg2
    PSYCOPG2_AVAILABLE = True
except ImportError:
    PSYCOPG2_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: psycopg2 æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install psycopg2-binary")

# ä¸­æ–‡åˆ†è©å’Œè½‰æ›
try:
    import jieba
    jieba.initialize()
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: jieba æœªå®‰è£ï¼Œä¸­æ–‡åˆ†è©åŠŸèƒ½å°‡è¢«ç¦ç”¨")

try:
    import opencc
    OPENCC_AVAILABLE = True
except ImportError:
    OPENCC_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: opencc æœªå®‰è£ï¼Œç¹ç°¡è½‰æ›åŠŸèƒ½å°‡è¢«ç¦ç”¨")

# æ–‡æª”è™•ç†æ”¯æŒ
try:
    import docx2txt
    DOCX2TXT_AVAILABLE = True
    DOCX_METHOD = "docx2txt"
    print("âœ… DOCX æ”¯æŒå·²å•Ÿç”¨ (docx2txt)")
except ImportError:
    try:
        from docx import Document as DocxDocument
        DOCX2TXT_AVAILABLE = True
        DOCX_METHOD = "python-docx"
        print("âœ… DOCX æ”¯æŒå·²å•Ÿç”¨ (python-docx)")
    except ImportError:
        DOCX2TXT_AVAILABLE = False
        DOCX_METHOD = None
        print("âš ï¸ è­¦å‘Š: docx2txt æˆ– python-docx æœªå®‰è£")

try:
    import ebooklib
    from ebooklib import epub
    from bs4 import BeautifulSoup
    EPUB_AVAILABLE = True
    print("âœ… EPUB æ”¯æŒå·²å•Ÿç”¨")
except ImportError:
    EPUB_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: ebooklib æˆ– beautifulsoup4 æœªå®‰è£")

@dataclass
class FileInfo:
    """æ–‡ä»¶è³‡è¨Š"""
    path: str
    size: int
    mtime: float
    hash: str
    encoding: str = "utf-8"
    file_type: str = ""

@dataclass
class TextAnalysis:
    """æ–‡æœ¬åˆ†æçµæœ"""
    length: int
    text_type: str
    language: str
    encoding: str
    structure_info: Dict
    quality_score: float
    processing_strategy: str

@dataclass
class ChunkInfo:
    """åˆ†å¡Šè³‡è¨Š"""
    chunk_id: str
    content: str
    metadata: Dict
    token_count: int
    quality_score: float
    relationships: List[str]  # èˆ‡å…¶ä»–åˆ†å¡Šçš„é—œä¿‚

@dataclass
class SearchResult:
    content: str
    score: float
    metadata: Dict
    collection: str
    chunk_info: Optional[ChunkInfo] = None