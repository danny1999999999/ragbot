#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
conversation_logger_simple.py - ğŸ—ƒï¸ PostgreSQLç‰ˆå°è©±è¨˜éŒ„å™¨ï¼ˆä¿®æ­£ç‰ˆï¼‰
ä½¿ç”¨database_adapter.pyæŠ½è±¡å±¤ï¼Œæ”¯æ´SQLiteå’ŒPostgreSQL
"""

import json
import os
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import logging

# å°å…¥æ•¸æ“šåº«æŠ½è±¡å±¤
from database_adapter import DatabaseFactory, SQLDialect

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PostgreSQLConversationLogger:
    """PostgreSQLç‰ˆå°è©±è¨˜éŒ„å™¨ - ä¿®æ­£ç‰ˆï¼Œä½¿ç”¨æ•¸æ“šåº«æŠ½è±¡å±¤"""
    
    def __init__(self, db_config: Dict = None):
        """
        åˆå§‹åŒ–å°è©±è¨˜éŒ„å™¨
        
        Args:
            db_config: æ•¸æ“šåº«é…ç½®å­—å…¸ï¼Œå¦‚æœç‚ºNoneå‰‡å¾ç’°å¢ƒè®Šé‡è®€å–
        """
        if db_config is None:
            # å¾ç’°å¢ƒè®Šé‡è‡ªå‹•å‰µå»ºé©é…å™¨
            self.db_adapter = DatabaseFactory.create_from_env("conversations")
        else:
            # ä½¿ç”¨æä¾›çš„é…ç½®
            db_type = db_config.get("type", "postgresql")
            self.db_adapter = DatabaseFactory.create_adapter(db_type, db_config)
        
        # ğŸ”§ ä¿®æ­£ï¼šæ›´å¯é çš„æ•¸æ“šåº«é¡å‹æª¢æ¸¬
        self.db_type = self._detect_database_type()
        
        self.init_database()
    
    def _detect_database_type(self) -> str:
        """æª¢æ¸¬æ•¸æ“šåº«é¡å‹"""
        # æ–¹æ³•1: å¾é…ç½®ä¸­æª¢æ¸¬
        if hasattr(self.db_adapter, 'config'):
            config_type = self.db_adapter.config.get("type")
            if config_type:
                return config_type.lower()
        
        # æ–¹æ³•2: å¾é©é…å™¨é¡å‹æª¢æ¸¬
        adapter_class = self.db_adapter.__class__.__name__
        if "PostgreSQL" in adapter_class:
            return "postgresql"
        elif "SQLite" in adapter_class:
            return "sqlite"
        
        # æ–¹æ³•3: å¾å±¬æ€§æª¢æ¸¬
        if hasattr(self.db_adapter, 'host'):
            return "postgresql"
        elif hasattr(self.db_adapter, 'db_file'):
            return "sqlite"
        
        # é»˜èªå€¼
        logger.warning("ç„¡æ³•æª¢æ¸¬æ•¸æ“šåº«é¡å‹ï¼Œä½¿ç”¨é»˜èªå€¼: postgresql")
        return "postgresql"
    
    def _get_placeholder(self, count: int = 1) -> str:
        """ç²å–æ­£ç¢ºçš„åƒæ•¸ä½”ä½ç¬¦"""
        if self.db_type == "sqlite":
            return "?" if count == 1 else ", ".join(["?"] * count)
        else:  # postgresql
            if count == 1:
                return "%s"
            else:
                return ", ".join(["%s"] * count)
    
    def _adapt_sql(self, sql: str) -> str:
        """é©é…SQLèªå¥åˆ°ç‰¹å®šæ•¸æ“šåº«"""
        if self.db_type == "sqlite":
            # PostgreSQLèªæ³•è½‰SQLite
            sql = sql.replace("%s", "?")
            sql = sql.replace("CURRENT_DATE", "DATE('now')")
            sql = sql.replace("CURRENT_TIMESTAMP", "DATETIME('now')")
            sql = sql.replace("NOW()", "DATETIME('now')")
        return sql
    
    def init_database(self):
        """åˆå§‹åŒ–å°è©±æ•¸æ“šåº« - ä¿®æ­£ç‰ˆï¼Œè‡ªå‹•æª¢æ¸¬å’Œæ–°å¢ç¼ºå¤±çš„æ¬„ä½"""
        try:
            self.db_adapter.connect()
            
            # 1. å‰µå»ºä¸»å°è©±è¨˜éŒ„è¡¨ (å¦‚æœä¸å­˜åœ¨)
            auto_increment = SQLDialect.get_auto_increment_column(self.db_type)
            timestamp_col = SQLDialect.get_timestamp_column(self.db_type)
            
            create_table_sql = f'''
                CREATE TABLE IF NOT EXISTS conversations (
                    id {auto_increment},
                    user_id TEXT NOT NULL,
                    user_query TEXT NOT NULL,
                    ai_response TEXT NOT NULL,
                    timestamp {timestamp_col}
                )
            '''
            
            self.db_adapter.execute_update(create_table_sql)

            # 2. å®šç¾©æ‰€æœ‰æ‡‰æœ‰çš„æ¬„ä½åŠå…¶é¡å‹
            required_columns = {
                "collection_used": "TEXT",
                "retrieved_docs": "TEXT",
                "doc_similarities": "TEXT",
                "chunk_references": "TEXT",
                "processing_time_ms": "INTEGER DEFAULT 0",
                "is_image_generation": SQLDialect.get_boolean_column(self.db_type, False),
                "image_url": "TEXT",
                "error_occurred": SQLDialect.get_boolean_column(self.db_type, False),
                "error_message": "TEXT",
                "authenticated_user_id": "INTEGER",
                "user_role": "TEXT DEFAULT 'anonymous'",
                "created_at": timestamp_col
            }

            # 3. ç²å–ç•¶å‰è¡¨æ ¼çš„æ‰€æœ‰æ¬„ä½
            existing_columns = self.db_adapter.get_table_columns("conversations")

            # 4. æ¯”è¼ƒä¸¦æ–°å¢æ‰€æœ‰ç¼ºå¤±çš„æ¬„ä½
            for col_name, col_type in required_columns.items():
                if col_name not in existing_columns:
                    print(f"âš ï¸ æª¢æ¸¬åˆ°ç¼ºå¤±çš„è³‡æ–™åº«æ¬„ä½ï¼Œæ­£åœ¨è‡ªå‹•æ–°å¢: {col_name}")
                    alter_sql = f'ALTER TABLE conversations ADD COLUMN {col_name} {col_type}'
                    self.db_adapter.execute_update(alter_sql)

            # 5. å‰µå»ºçµ±è¨ˆè¡¨ (å¦‚æœä¸å­˜åœ¨)
            stats_table_sql = f'''
                CREATE TABLE IF NOT EXISTS conversation_stats (
                    id {auto_increment},
                    stat_date DATE NOT NULL,
                    total_conversations INTEGER DEFAULT 0,
                    successful_conversations INTEGER DEFAULT 0,
                    failed_conversations INTEGER DEFAULT 0,
                    image_generations INTEGER DEFAULT 0,
                    average_processing_time_ms REAL DEFAULT 0,
                    most_used_collection TEXT,
                    created_at {timestamp_col}
                )
            '''
            
            self.db_adapter.execute_update(stats_table_sql)
            
            # 6. å‰µå»ºå”¯ä¸€ç´„æŸ (PostgreSQLèªæ³•)
            if self.db_type == "postgresql":
                try:
                    self.db_adapter.execute_update(
                        'ALTER TABLE conversation_stats ADD CONSTRAINT unique_stat_date UNIQUE (stat_date)'
                    )
                except Exception:
                    # ç´„æŸå¯èƒ½å·²å­˜åœ¨ï¼Œå¿½ç•¥éŒ¯èª¤
                    pass
            
            # 7. å‰µå»ºç´¢å¼•ä»¥æé«˜æŸ¥è©¢æ€§èƒ½
            indexes = [
                "CREATE INDEX IF NOT EXISTS idx_user_id ON conversations(user_id)",
                "CREATE INDEX IF NOT EXISTS idx_timestamp ON conversations(timestamp)",
                "CREATE INDEX IF NOT EXISTS idx_collection ON conversations(collection_used)",
                "CREATE INDEX IF NOT EXISTS idx_error ON conversations(error_occurred)",
                "CREATE INDEX IF NOT EXISTS idx_image_gen ON conversations(is_image_generation)"
            ]
            
            for index_sql in indexes:
                try:
                    self.db_adapter.execute_update(index_sql)
                except Exception as e:
                    # ç´¢å¼•å¯èƒ½å·²å­˜åœ¨ï¼Œè¨˜éŒ„ä½†ä¸ä¸­æ–·
                    logger.debug(f"ç´¢å¼•å‰µå»ºè­¦å‘Š: {e}")
            
            print(f"âœ… {self.db_type.upper()}å°è©±æ•¸æ“šåº«åˆå§‹åŒ–æˆåŠŸ (è‡ªå‹•æ¬„ä½ä¿®å¾©å·²å•Ÿç”¨)")
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šåº«åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def log_conversation(self, 
                    user_id: str,
                    user_query: str,
                    ai_response: str,
                    collection_used: str = None,
                    retrieved_docs: List[str] = None,
                    doc_similarities: List[float] = None,
                    processing_time_ms: int = 0,
                    is_image_generation: bool = False,
                    image_url: str = None,
                    error_occurred: bool = False,
                    error_message: str = None,
                    authenticated_user_id: int = None,
                    user_role: str = "anonymous",
                    chunk_references: Optional[List[Dict]] = None,
                    chunk_indices: Optional[List[int]] = None) -> str:
        """è¨˜éŒ„å°è©± - ä¿®æ­£ç‰ˆï¼Œæ­£ç¢ºè™•ç† chunk ç´¢å¼•"""
        try:
            # ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¢ºçš„äº‹å‹™è™•ç†
            with self.db_adapter.transaction():
                # è™•ç†æª¢ç´¢æ–‡æª”å’Œç›¸ä¼¼åº¦
                retrieved_docs_json = json.dumps(retrieved_docs or [], ensure_ascii=False)
                doc_similarities_json = json.dumps(doc_similarities or [], ensure_ascii=False)
                
                # ğŸ”§ ä¿®å¾©ï¼šæ­£ç¢ºè™•ç† chunk ç´¢å¼•
                if chunk_references is None:
                    chunk_references = []
                    if retrieved_docs and doc_similarities:
                        for i, (doc, similarity) in enumerate(zip(retrieved_docs, doc_similarities)):
                            # ğŸ†• ä½¿ç”¨å‚³å…¥çš„çœŸå¯¦ chunk ç´¢å¼•ï¼Œè€Œä¸æ˜¯å¾ªç’°è®Šæ•¸
                            actual_index = chunk_indices[i] if chunk_indices and i < len(chunk_indices) else None
                            
                            chunk_ref = {
                                "id": f"chunk_{i+1}",
                                "content_preview": doc[:100] + "..." if len(doc) > 100 else doc,
                                "similarity": round(similarity, 4),
                            }
                            
                            # ğŸ”§ åªæœ‰åœ¨æœ‰çœŸå¯¦ç´¢å¼•æ™‚æ‰æ·»åŠ  index å­—æ®µ
                            if actual_index is not None:
                                chunk_ref["index"] = actual_index
                                logger.debug(f"ğŸ” å°è©± chunk {i+1} ä½¿ç”¨çœŸå¯¦ç´¢å¼•: {actual_index}")
                            else:
                                logger.warning(f"âš ï¸ å°è©± chunk {i+1} ç¼ºå°‘çœŸå¯¦ç´¢å¼•ï¼Œå°‡ä¸è¨­ç½® index å­—æ®µ")
                            
                            chunk_references.append(chunk_ref)
                
                chunk_references_json = json.dumps(chunk_references, ensure_ascii=False)
                
                # ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨çµ±ä¸€çš„åƒæ•¸ä½”ä½ç¬¦
                placeholders = self._get_placeholder(14)  # 14å€‹åƒæ•¸
                insert_sql = f'''
                    INSERT INTO conversations (
                        user_id, user_query, ai_response, collection_used,
                        retrieved_docs, doc_similarities, chunk_references,
                        processing_time_ms, is_image_generation, image_url,
                        error_occurred, error_message, authenticated_user_id, user_role
                    ) VALUES ({placeholders})
                '''
                
                conversation_id = self.db_adapter.execute_insert(insert_sql, (
                    user_id, user_query, ai_response, collection_used,
                    retrieved_docs_json, doc_similarities_json, chunk_references_json,
                    processing_time_ms, is_image_generation, image_url,
                    error_occurred, error_message, authenticated_user_id, user_role
                ))
                
                # æ›´æ–°çµ±è¨ˆä¿¡æ¯
                self._update_daily_stats()
                
                # ğŸ†• æ”¹é€²æ—¥èªŒä¿¡æ¯
                valid_chunk_count = len([ref for ref in chunk_references if 'index' in ref])
                logger.info(f"âœ… è¨˜éŒ„å°è©±æˆåŠŸ: ID {conversation_id}, chunks: {len(chunk_references)}, æœ‰æ•ˆç´¢å¼•: {valid_chunk_count}")
                return f"conv_{conversation_id}"
                
        except Exception as e:
            logger.error(f"âŒ è¨˜éŒ„å°è©±å¤±æ•—: {e}")
            return None
    
    def get_conversations(self, 
                     limit: int = 50, 
                     offset: int = 0,
                     search: str = None,
                     collection: str = None,
                     user_id: str = None,
                     start_date: str = None,
                     end_date: str = None) -> Tuple[List[Dict], int]:
        """ç²å–å°è©±è¨˜éŒ„ - ä¿®æ­£ç‰ˆï¼Œæ­£ç¢ºæå– chunk_ids"""
        try:
            # æ§‹å»ºæŸ¥è©¢æ¢ä»¶
            where_conditions = []
            params = []
            
            if search:
                search_placeholder = self._get_placeholder(2)
                where_conditions.append(f"(user_query LIKE {search_placeholder.split(',')[0]} OR ai_response LIKE {search_placeholder.split(',')[1]})")
                search_param = f"%{search}%"
                params.extend([search_param, search_param])
            
            if collection:
                where_conditions.append(f"collection_used = {self._get_placeholder()}")
                params.append(collection)
            
            if user_id:
                where_conditions.append(f"user_id = {self._get_placeholder()}")
                params.append(user_id)
            
            if start_date:
                where_conditions.append(f"DATE(timestamp) >= {self._get_placeholder()}")
                params.append(start_date)
            
            if end_date:
                where_conditions.append(f"DATE(timestamp) <= {self._get_placeholder()}")
                params.append(end_date)
            
            where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"
            
            # ç²å–ç¸½æ•¸
            count_query = f"SELECT COUNT(*) as count FROM conversations WHERE {where_clause}"
            count_result = self.db_adapter.execute_query(count_query, params)
            total = count_result[0]['count'] if count_result else 0
            
            # ç²å–è¨˜éŒ„
            limit_offset_placeholder = self._get_placeholder(2)
            query = f"""
                SELECT * FROM conversations 
                WHERE {where_clause}
                ORDER BY timestamp DESC 
                LIMIT {limit_offset_placeholder.split(',')[0]} OFFSET {limit_offset_placeholder.split(',')[1]}
            """
            params.extend([limit, offset])
            
            rows = self.db_adapter.execute_query(query, params)
            
            # è½‰æ›ç‚ºå­—å…¸ä¸¦è§£æ JSON å­—æ®µ
            conversations = []
            for row in rows:
                conv = dict(row)
                
                # ğŸ”§ ä¿®å¾©ï¼šæ­£ç¢ºè§£æ JSON å­—æ®µä¸¦æå– chunk_ids
                try:
                    conv['retrieved_docs'] = json.loads(conv.get('retrieved_docs') or '[]')
                    conv['doc_similarities'] = json.loads(conv.get('doc_similarities') or '[]')
                    chunk_refs = json.loads(conv.get('chunk_references') or '[]')
                    conv['chunk_references'] = chunk_refs
                    
                    # ğŸ”§ ä¿®å¾©ï¼šåªæå–æœ‰æ•ˆçš„ chunk ç´¢å¼•
                    chunk_ids = []
                    if isinstance(chunk_refs, list):
                        for ref in chunk_refs:
                            if isinstance(ref, dict) and 'index' in ref:
                                # åªæœ‰ç•¶ index å­—æ®µå­˜åœ¨ä¸”æœ‰æ•ˆæ™‚æ‰æ·»åŠ 
                                index_val = ref['index']
                                if isinstance(index_val, (int, float)) and index_val >= 0:
                                    chunk_ids.append(int(index_val))
                                else:
                                    logger.warning(f"âš ï¸ å°è©± {conv['id']} åŒ…å«ç„¡æ•ˆçš„ chunk ç´¢å¼•: {index_val}")
                            elif isinstance(ref, (int, float)) and ref >= 0:
                                # å‘å¾Œå…¼å®¹ï¼šç›´æ¥æ˜¯æ•¸å­—çš„æƒ…æ³
                                chunk_ids.append(int(ref))
                    
                    conv['chunk_ids'] = chunk_ids
                    
                    if chunk_ids:
                        logger.debug(f"ğŸ” å°è©± {conv['id']} è§£æå‡ºæœ‰æ•ˆ chunk_ids: {chunk_ids}")
                    else:
                        logger.debug(f"ğŸ” å°è©± {conv['id']} æ²’æœ‰æœ‰æ•ˆçš„ chunk ç´¢å¼•")
                    
                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸ è§£æ JSON å­—æ®µå¤±æ•— (ID: {conv['id']}): {e}")
                    conv['retrieved_docs'] = []
                    conv['doc_similarities'] = []
                    conv['chunk_references'] = []
                    conv['chunk_ids'] = []
                
                conversations.append(conv)
            
            logger.info(f"âœ… ç²å–å°è©±è¨˜éŒ„: {len(conversations)}/{total}")
            return conversations, total
            
        except Exception as e:
            logger.error(f"âŒ ç²å–å°è©±è¨˜éŒ„å¤±æ•—: {e}")
            return [], 0
    
    def delete_conversation(self, conversation_id: int) -> bool:
        """åˆªé™¤å–®ç­†å°è©±"""
        try:
            delete_sql = f"DELETE FROM conversations WHERE id = {self._get_placeholder()}"
            deleted_count = self.db_adapter.execute_update(delete_sql, (conversation_id,))
            
            if deleted_count > 0:
                logger.info(f"âœ… åˆªé™¤å°è©±æˆåŠŸ: ID {conversation_id}")
                self._update_daily_stats()
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"âŒ åˆªé™¤å°è©±å¤±æ•—: {e}")
            return False
    
    def delete_conversations_batch(self, conversation_ids: List[int]) -> int:
        """æ‰¹æ¬¡åˆªé™¤å°è©±"""
        try:
            if not conversation_ids:
                return 0
            
            placeholders = self._get_placeholder(len(conversation_ids))
            delete_sql = f"DELETE FROM conversations WHERE id IN ({placeholders})"
            
            deleted_count = self.db_adapter.execute_update(delete_sql, conversation_ids)
            
            if deleted_count > 0:
                logger.info(f"âœ… æ‰¹æ¬¡åˆªé™¤å°è©±æˆåŠŸ: {deleted_count} ç­†")
                self._update_daily_stats()
            
            return deleted_count
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡åˆªé™¤å°è©±å¤±æ•—: {e}")
            return 0
    
    def get_collections_stats(self) -> List[Tuple[str, int]]:
        """ç²å–å„é›†åˆçš„å°è©±çµ±è¨ˆ"""
        try:
            query = '''
                SELECT collection_used, COUNT(*) as count 
                FROM conversations 
                WHERE collection_used IS NOT NULL 
                GROUP BY collection_used 
                ORDER BY count DESC
            '''
            
            results = self.db_adapter.execute_query(query)
            return [(row['collection_used'], row['count']) for row in results]
            
        except Exception as e:
            logger.error(f"âŒ ç²å–é›†åˆçµ±è¨ˆå¤±æ•—: {e}")
            return []
    
    def get_statistics(self) -> Dict:
        """ç²å–ç³»çµ±çµ±è¨ˆä¿¡æ¯"""
        try:
            stats = {}
            
            # ç¸½å°è©±æ•¸
            result = self.db_adapter.execute_query("SELECT COUNT(*) as count FROM conversations")
            stats['total_conversations'] = result[0]['count'] if result else 0
            
            # ä»Šæ—¥å°è©±æ•¸ - ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨é©é…çš„SQL
            if self.db_type == "sqlite":
                today_sql = "SELECT COUNT(*) as count FROM conversations WHERE DATE(timestamp) = DATE('now')"
            else:
                today_sql = "SELECT COUNT(*) as count FROM conversations WHERE DATE(timestamp) = CURRENT_DATE"
            
            result = self.db_adapter.execute_query(today_sql)
            stats['today_conversations'] = result[0]['count'] if result else 0
            
            # æˆåŠŸç‡
            success_sql = f"SELECT COUNT(*) as count FROM conversations WHERE error_occurred = {self._get_placeholder()}"
            result = self.db_adapter.execute_query(success_sql, (False,))
            successful_conversations = result[0]['count'] if result else 0
            stats['success_rate'] = int((successful_conversations / stats['total_conversations'] * 100)) if stats['total_conversations'] > 0 else 100
            
            # éŒ¯èª¤æ•¸é‡
            error_sql = f"SELECT COUNT(*) as count FROM conversations WHERE error_occurred = {self._get_placeholder()}"
            result = self.db_adapter.execute_query(error_sql, (True,))
            stats['error_count'] = result[0]['count'] if result else 0
            
            # å¹³å‡è™•ç†æ™‚é–“
            avg_sql = "SELECT AVG(processing_time_ms) as avg_time FROM conversations WHERE processing_time_ms > 0"
            result = self.db_adapter.execute_query(avg_sql)
            stats['average_processing_time_ms'] = int(result[0]['avg_time'] or 0) if result else 0
            
            # æœ€å—æ­¡è¿çš„é›†åˆ
            popular_sql = '''
                SELECT collection_used, COUNT(*) as count 
                FROM conversations 
                WHERE collection_used IS NOT NULL 
                GROUP BY collection_used 
                ORDER BY count DESC 
                LIMIT 1
            '''
            result = self.db_adapter.execute_query(popular_sql)
            stats['most_popular_collection'] = result[0]['collection_used'] if result else "ç„¡"
            
            # åœ–ç‰‡ç”Ÿæˆæ•¸é‡
            image_sql = f"SELECT COUNT(*) as count FROM conversations WHERE is_image_generation = {self._get_placeholder()}"
            result = self.db_adapter.execute_query(image_sql, (True,))
            stats['image_generations'] = result[0]['count'] if result else 0
            
            # æ´»èºæœƒè©±æ•¸ - ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨é©é…çš„SQL
            if self.db_type == "sqlite":
                active_sql = "SELECT COUNT(DISTINCT user_id) as count FROM conversations WHERE DATE(timestamp) = DATE('now')"
            else:
                active_sql = "SELECT COUNT(DISTINCT user_id) as count FROM conversations WHERE DATE(timestamp) = CURRENT_DATE"
            
            result = self.db_adapter.execute_query(active_sql)
            stats['active_sessions'] = result[0]['count'] if result else 0
            
            # æ¨¡æ“¬æ•¸æ“š
            stats['uptime_hours'] = 72
            stats['memory_usage_mb'] = 512
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ ç²å–çµ±è¨ˆä¿¡æ¯å¤±æ•—: {e}")
            return {
                'total_conversations': 0,
                'today_conversations': 0,
                'success_rate': 100,
                'error_count': 0,
                'average_processing_time_ms': 0,
                'most_popular_collection': 'ç„¡',
                'image_generations': 0,
                'active_sessions': 0,
                'uptime_hours': 0,
                'memory_usage_mb': 0
            }
    
    def _update_daily_stats(self):
        """æ›´æ–°æ¯æ—¥çµ±è¨ˆ"""
        try:
            today = datetime.now().date()
            
            # ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨é©é…çš„SQLå’Œåƒæ•¸ä½”ä½ç¬¦
            today_placeholder = self._get_placeholder()
            
            if self.db_type == "sqlite":
                today_sql_filter = f"DATE(timestamp) = {today_placeholder}"
            else:
                today_sql_filter = f"DATE(timestamp) = {today_placeholder}"
            
            # ç¸½æ•¸
            total_sql = f"SELECT COUNT(*) as count FROM conversations WHERE {today_sql_filter}"
            result = self.db_adapter.execute_query(total_sql, (today,))
            total_today = result[0]['count'] if result else 0
            
            # æˆåŠŸæ•¸
            success_placeholder = self._get_placeholder(2)
            success_sql = f"SELECT COUNT(*) as count FROM conversations WHERE {today_sql_filter} AND error_occurred = {success_placeholder.split(',')[1]}"
            result = self.db_adapter.execute_query(success_sql, (today, False))
            successful_today = result[0]['count'] if result else 0
            
            # å¤±æ•—æ•¸
            failed_placeholder = self._get_placeholder(2)
            failed_sql = f"SELECT COUNT(*) as count FROM conversations WHERE {today_sql_filter} AND error_occurred = {failed_placeholder.split(',')[1]}"
            result = self.db_adapter.execute_query(failed_sql, (today, True))
            failed_today = result[0]['count'] if result else 0
            
            # åœ–ç‰‡ç”Ÿæˆæ•¸
            images_placeholder = self._get_placeholder(2)
            images_sql = f"SELECT COUNT(*) as count FROM conversations WHERE {today_sql_filter} AND is_image_generation = {images_placeholder.split(',')[1]}"
            result = self.db_adapter.execute_query(images_sql, (today, True))
            images_today = result[0]['count'] if result else 0
            
            # å¹³å‡æ™‚é–“
            avg_sql = f"SELECT AVG(processing_time_ms) as avg_time FROM conversations WHERE {today_sql_filter} AND processing_time_ms > 0"
            result = self.db_adapter.execute_query(avg_sql, (today,))
            avg_time_today = result[0]['avg_time'] or 0 if result else 0
            
            # ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨é©é…çš„UPSERTèªæ³•
            if self.db_type == "postgresql":
                upsert_placeholder = self._get_placeholder(6)
                upsert_sql = f'''
                    INSERT INTO conversation_stats 
                    (stat_date, total_conversations, successful_conversations, failed_conversations, 
                     image_generations, average_processing_time_ms)
                    VALUES ({upsert_placeholder})
                    ON CONFLICT (stat_date) DO UPDATE SET
                    total_conversations = EXCLUDED.total_conversations,
                    successful_conversations = EXCLUDED.successful_conversations,
                    failed_conversations = EXCLUDED.failed_conversations,
                    image_generations = EXCLUDED.image_generations,
                    average_processing_time_ms = EXCLUDED.average_processing_time_ms
                '''
            else:  # SQLite
                upsert_placeholder = self._get_placeholder(6)
                upsert_sql = f'''
                    INSERT OR REPLACE INTO conversation_stats 
                    (stat_date, total_conversations, successful_conversations, failed_conversations, 
                     image_generations, average_processing_time_ms)
                    VALUES ({upsert_placeholder})
                '''
            
            self.db_adapter.execute_update(upsert_sql, (
                today, total_today, successful_today, failed_today, images_today, avg_time_today
            ))
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ¯æ—¥çµ±è¨ˆå¤±æ•—: {e}")
    
    def cleanup_old_records(self, days_to_keep: int = 30):
        """æ¸…ç†èˆŠè¨˜éŒ„"""
        try:
            # ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨é©é…çš„æ—¥æœŸå‡½æ•¸
            days_placeholder = self._get_placeholder()
            
            if self.db_type == "postgresql":
                cleanup_sql = f'''
                    DELETE FROM conversations 
                    WHERE timestamp < NOW() - INTERVAL '{days_to_keep} days'
                '''
                deleted_count = self.db_adapter.execute_update(cleanup_sql)
            else:  # SQLite
                cleanup_sql = f'''
                    DELETE FROM conversations 
                    WHERE timestamp < datetime('now', '-{days_to_keep} days')
                '''
                deleted_count = self.db_adapter.execute_update(cleanup_sql)
            
            logger.info(f"âœ… æ¸…ç†èˆŠè¨˜éŒ„å®Œæˆ: åˆªé™¤ {deleted_count} ç­†")
            return deleted_count
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†èˆŠè¨˜éŒ„å¤±æ•—: {e}")
            return 0
    
    def export_conversations(self, output_file: str, format: str = "json"):
        """å°å‡ºå°è©±è¨˜éŒ„"""
        try:
            conversations, _ = self.get_conversations(limit=10000)  # å°å‡ºæ‰€æœ‰è¨˜éŒ„
            
            if format.lower() == "json":
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(conversations, f, ensure_ascii=False, indent=2, default=str)
            elif format.lower() == "csv":
                import csv
                with open(output_file, 'w', newline='', encoding='utf-8') as f:
                    if conversations:
                        writer = csv.DictWriter(f, fieldnames=conversations[0].keys())
                        writer.writeheader()
                        for conv in conversations:
                            # å°‡è¤‡é›œå­—æ®µè½‰ç‚ºå­—ç¬¦ä¸²
                            conv_copy = conv.copy()
                            for key in ['retrieved_docs', 'doc_similarities', 'chunk_references']:
                                if key in conv_copy:
                                    conv_copy[key] = json.dumps(conv_copy[key], ensure_ascii=False)
                            writer.writerow(conv_copy)
            
            logger.info(f"âœ… å°å‡ºå°è©±è¨˜éŒ„å®Œæˆ: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å°å‡ºå°è©±è¨˜éŒ„å¤±æ•—: {e}")
            return False
    
    def close(self):
        """é—œé–‰æ•¸æ“šåº«é€£æ¥"""
        if self.db_adapter:
            self.db_adapter.disconnect()


# å‰µå»ºå…¨åŸŸå¯¦ä¾‹ï¼ˆå‘å¾Œå…¼å®¹æ€§ï¼‰
def create_logger_instance(db_config: Dict = None):
    """å‰µå»ºè¨˜éŒ„å™¨å¯¦ä¾‹çš„å·¥å» å‡½æ•¸"""
    return PostgreSQLConversationLogger(db_config)


# æ¸¬è©¦å‡½æ•¸
if __name__ == "__main__":
    # æ¸¬è©¦PostgreSQLé…ç½®
    pg_config = {
        "type": "postgresql",
        "host": "localhost",
        "port": 5432,
        "database": "chatbot_conversations",
        "user": "postgres",
        "password": "your_password",
        "schema": "public"
    }
    
    # æ¸¬è©¦SQLiteé…ç½®ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
    sqlite_config = {
        "type": "sqlite",
        "db_file": "test_pg_conversations.db"
    }
    
    print("ğŸ§ª æ¸¬è©¦PostgreSQLç‰ˆå°è©±è¨˜éŒ„å™¨ï¼ˆä¿®æ­£ç‰ˆï¼‰")
    print("=" * 50)
    
    # ä½¿ç”¨SQLiteé€²è¡Œæ¸¬è©¦ï¼ˆå¦‚æœPostgreSQLä¸å¯ç”¨ï¼‰
    try:
        logger_instance = PostgreSQLConversationLogger(sqlite_config)
        print(f"âœ… ä½¿ç”¨{logger_instance.db_type.upper()}é©é…å™¨æ¸¬è©¦")
    except Exception as e:
        print(f"âŒ å‰µå»ºè¨˜éŒ„å™¨å¤±æ•—: {e}")
        exit(1)
    
    # æ¸¬è©¦è¨˜éŒ„å°è©± - å¸¶æœ‰æ­£ç¢ºçš„ chunk ç´¢å¼•
    conv_id = logger_instance.log_conversation(
        user_id="test_user_pg_001",
        user_query="ä»€éº¼æ˜¯PostgreSQLï¼Ÿ",
        ai_response="PostgreSQLæ˜¯ä¸€å€‹å¼·å¤§çš„é–‹æºé—œè¯å¼è³‡æ–™åº«...",
        collection_used="collection_database",
        retrieved_docs=["PostgreSQLæ–‡æª”1å…§å®¹", "PostgreSQLæ–‡æª”2å…§å®¹"],
        doc_similarities=[0.87, 0.74],
        processing_time_ms=1800,
        user_role="user",
        # ğŸ†• æ¸¬è©¦ï¼šå‚³å…¥çœŸå¯¦çš„ chunk ç´¢å¼•
        chunk_indices=[25, 58]  # çœŸå¯¦çš„å‘é‡è³‡æ–™åº«ç´¢å¼•
    )
    
    print(f"è¨˜éŒ„å°è©±æˆåŠŸ: {conv_id}")
    
    # æ¸¬è©¦ç²å–å°è©±
    conversations, total = logger_instance.get_conversations(limit=10)
    print(f"ç²å–åˆ° {len(conversations)} ç­†å°è©±è¨˜éŒ„ï¼Œç¸½å…± {total} ç­†")
    
    # æª¢æŸ¥ chunk_ids
    if conversations:
        first_conv = conversations[0]
        print(f"ç¬¬ä¸€å€‹å°è©±çš„ chunk_ids: {first_conv.get('chunk_ids', [])}")
    
    # æ¸¬è©¦çµ±è¨ˆ
    stats = logger_instance.get_statistics()
    print(f"çµ±è¨ˆä¿¡æ¯: {stats}")
    
    # æ¸¬è©¦å°å‡º
    logger_instance.export_conversations("test_pg_export.json", "json")
    print("å°å‡ºæ¸¬è©¦å®Œæˆ")
    
    # é—œé–‰é€£æ¥
    logger_instance.close()
    
    # æ¸…ç†æ¸¬è©¦æ–‡ä»¶
    import os
    test_files = ["test_pg_conversations.db", "test_pg_export.json"]
    for file in test_files:
        if os.path.exists(file):
            os.remove(file)
    
    print("âœ… PostgreSQLç‰ˆå°è©±è¨˜éŒ„å™¨æ¸¬è©¦å®Œæˆï¼ˆä¿®æ­£ç‰ˆï¼‰- chunk ç´¢å¼•å•é¡Œå·²ä¿®å¾©")