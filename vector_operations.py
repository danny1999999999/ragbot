#!/usr/bin/env python3
"""
å‘é‡æ“ä½œæ ¸å¿ƒå±¤ - VectorOperationsCore
è·è²¬ï¼šåº•å±¤æ•¸æ“šæ“ä½œã€å‘é‡è™•ç†ã€æ–‡æª”è¼‰å…¥ã€é›†åˆç®¡ç†
åŒ…å«26å€‹åº•å±¤æ ¸å¿ƒæ–¹æ³•ï¼Œå¾åŸ OptimizedVectorSystem ç²¾ç¢ºç§»å‹•è€Œä¾†
"""

import time
import json
import hashlib
import re
import logging
import os
import gc
import threading
import tempfile
import shutil  # âœ… æ·»åŠ ç¼ºå°‘çš„import
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set, Any, Union
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import asdict  # âœ… æ·»åŠ ç¼ºå°‘çš„import

# å°å…¥ä¾è³´
from core_config import *
from text_processing import *
from vector_builder import AdaptiveBatchProcessor

# LangChainæ ¸å¿ƒ
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    TextLoader, PyPDFLoader, Docx2txtLoader, 
    CSVLoader, JSONLoader
)

# å‘é‡å­˜å„² (æ¢ä»¶å°å…¥)
if PGVECTOR_AVAILABLE:
    try:
        from langchain_postgres import PGVector
    except ImportError:
        from langchain_community.vectorstores import PGVector
else:
    from langchain_community.vectorstores import Chroma

# OpenAI (æ¢ä»¶å°å…¥)
if OPENAI_EMBEDDINGS_AVAILABLE:
    from langchain_openai import OpenAIEmbeddings

# PostgreSQL (æ¢ä»¶å°å…¥)  
if PSYCOPG2_AVAILABLE:
    import psycopg2

logger = logging.getLogger(__name__)

class VectorOperationsCore:
    """å‘é‡æ“ä½œæ ¸å¿ƒé¡åˆ¥ - è² è²¬åº•å±¤æ•¸æ“šæ“ä½œå’Œå‘é‡è™•ç†"""
    
    def __init__(self, data_dir: str = None, model_type: str = None):
        """âœ… ç´”PostgreSQLåˆå§‹åŒ– - ç§»é™¤file_recordsä¾è³´"""
        
        # ğŸ”§ 1. åŸºæœ¬è®Šæ•¸è¨­ç½®
        self.data_dir = Path(data_dir or SYSTEM_CONFIG["data_dir"])
        self.model_type = model_type or "openai"
        self.persist_dir = Path(SYSTEM_CONFIG["persist_dir"])  # Chromaå‚™ç”¨

        # âŒ ä¸å†å»ºç«‹æœ¬åœ°ç›®éŒ„ (ç´”PostgreSQLæ–¹æ¡ˆ)
        print("ğŸš€ ç´”PostgreSQLæ–¹æ¡ˆï¼šä¸ä½¿ç”¨æœ¬åœ°dataç›®éŒ„")
        
        # ğŸ”§ 2. è³‡æ–™åº«é€£æ¥è¨­ç½®ï¼ˆä½†ä¸æ¸¬è©¦ï¼‰
        self.db_adapter = None
        self.connection_string = None
        self.use_postgres = False

        database_url = os.getenv("DATABASE_URL")
        if PGVECTOR_AVAILABLE and database_url:
            self.connection_string = database_url
            print("ğŸ” ç™¼ç¾DATABASE_URLï¼Œæº–å‚™æ¸¬è©¦PostgreSQLé€£æ¥...")
        else:
            print("âš ï¸ DATABASE_URLæœªè¨­ç½®æˆ–PGVectorä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨Chroma")

        if not PGVECTOR_AVAILABLE:
            print("âš ï¸ PGVectorä¾è³´æœªå®‰è£ï¼Œä½¿ç”¨Chromaä½œç‚ºå‚™ç”¨")
            self.persist_dir.mkdir(exist_ok=True)

        # âœ… 3. å…ˆåˆå§‹åŒ–Embeddingæ¨¡å‹ï¼ˆé—œéµï¼ï¼‰
        self._setup_embedding_model()
        print("âœ… Embeddingæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        # âœ… 4. ç¾åœ¨å¯ä»¥æ¸¬è©¦PostgreSQLé€£æ¥äº†ï¼ˆembeddingså·²å­˜åœ¨ï¼‰
        if PGVECTOR_AVAILABLE and database_url and hasattr(self, 'embeddings'):
            try:
                print("ğŸ” æ¸¬è©¦PostgreSQL + PGVectoré€£æ¥...")
                # æ¸¬è©¦é€£æ¥
                PGVector.from_existing_index(
                    collection_name="_test_connection",
                    embedding=self.embeddings,  # âœ… ç¾åœ¨å®‰å…¨äº†
                    connection_string=self.connection_string
                )
                self.use_postgres = True
                print("âœ… PostgreSQL (pgvector) é€£æ¥æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ PostgreSQL (pgvector) é€£æ¥æ¸¬è©¦å¤±æ•—: {e}")
                self.use_postgres = False
                print("ğŸ“„ å›é€€åˆ°Chromaæœ¬åœ°å­˜å„²")
                self.persist_dir.mkdir(exist_ok=True)
        
        if not self.use_postgres:
            print("ğŸ” ä½¿ç”¨Chromaä½œç‚ºå‘é‡å­˜å„²")
            self.persist_dir.mkdir(exist_ok=True)
        
        # ğŸ”§ 5. åˆå§‹åŒ–æ–‡æœ¬è™•ç†çµ„ä»¶
        self._setup_text_processing()
        
        # ğŸ”§ 6. åˆå§‹åŒ–è™•ç†å™¨
        self.batch_processor = AdaptiveBatchProcessor()
        self.text_splitter = OptimizedTextSplitter()
        
        # ğŸ”§ 7. åˆå§‹åŒ–å­˜å„²ï¼ˆç§»é™¤æª”æ¡ˆè¨˜éŒ„ï¼‰
        self._vector_stores = {}
        
        # âœ… æ·»åŠ file_recordsåˆå§‹åŒ–
        self.file_records = {}
        
        # âœ… æ”¹ç‚ºç´”PostgreSQLåˆå§‹åŒ–
        print("ğŸš€ ç´”PostgreSQLæ–¹æ¡ˆï¼šæ‰€æœ‰æª”æ¡ˆæ•¸æ“šå°‡ç›´æ¥å­˜å„²åœ¨PostgreSQLä¸­")
        print("ğŸ“„ ä¸å†ç¶­è­·æœ¬åœ°æª”æ¡ˆè¨˜éŒ„ (file_records.json)")
        
        self.processing_lock = threading.Lock()
        
        print(f"ğŸš€ å‘é‡æ“ä½œæ ¸å¿ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ¤– åµŒå…¥æ¨¡å‹: {self.model_type}")
        print(f"   ğŸ” è³‡æ–™ç›®éŒ„: ä¸ä½¿ç”¨ (ç´”PostgreSQL)")
        print(f"   ğŸ—„ï¸ å‘é‡åº«: {'PostgreSQL + PGVector' if self.use_postgres else 'Chroma (æœ¬åœ°)'}")
        print(f"   ğŸ§  æ™ºèƒ½æ–‡æœ¬è™•ç†: âœ…")
        print(f"   ğŸ”§ è‡ªé©æ‡‰æ‰¹æ¬¡: âœ…")
        print(f"   ğŸ“¦ ç´”PostgreSQLæ–¹æ¡ˆ: {'âœ…' if self.use_postgres else 'âŒ'}")

    def _setup_embedding_model(self):
        """è¨­å®šåµŒå…¥æ¨¡å‹"""
        try:
            if self.model_type == "openai":
                if not OPENAI_EMBEDDINGS_AVAILABLE:
                    raise ImportError("OpenAI Embeddingsä¸å¯ç”¨")
                
                print(f"ğŸ”§ åˆå§‹åŒ–OpenAI Embeddings...")
                
                api_key = os.getenv("OPENAI_API_KEY")
                base_url = os.getenv("OPENAI_API_BASE")
                
                embedding_params = {
                    "model": "text-embedding-3-small",
                    "api_key": api_key,
                    "max_retries": 3,
                    "request_timeout": 60  # å¢åŠ è¶…æ™‚æ™‚é–“åˆ°60ç§’
                }
                
                if base_url:
                    embedding_params["base_url"] = base_url
                    print(f"ğŸ”§ ä½¿ç”¨è‡ªå®šç¾©APIç«¯é»: {base_url}")
                
                self.embeddings = OpenAIEmbeddings(**embedding_params)
                print(f"âœ… OpenAI Embeddingsåˆå§‹åŒ–æˆåŠŸ")
                
            else:
                # HuggingFaceæ¨¡å‹
                print(f"ğŸ”§ åˆå§‹åŒ–HuggingFace Embeddings...")
                
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="BAAI/bge-small-zh-v1.5",
                    model_kwargs={"device": "cpu"},
                    encode_kwargs={"batch_size": 16, "normalize_embeddings": True}
                )
                print(f"âœ… HuggingFace Embeddingsåˆå§‹åŒ–æˆåŠŸ")
                
        except Exception as e:
            print(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
            
            # å›é€€æ©Ÿåˆ¶
            if self.model_type == "openai":
                print("ğŸ“„ å˜—è©¦HuggingFaceå‚™é¸...")
                try:
                    self.embeddings = HuggingFaceEmbeddings(
                        model_name="BAAI/bge-small-zh-v1.5",
                        model_kwargs={"device": "cpu"}
                    )
                    self.model_type = "huggingface"
                    print("âœ… å·²å›é€€åˆ°HuggingFace")
                except Exception as e2:
                    raise RuntimeError(f"æ‰€æœ‰åµŒå…¥æ¨¡å‹éƒ½åˆå§‹åŒ–å¤±æ•—: {e2}")
            else:
                raise

    def _setup_text_processing(self):
        """è¨­å®šæ–‡æœ¬è™•ç†çµ„ä»¶"""
        self.normalizer = ChineseTextNormalizer()
        self.analyzer = SmartTextAnalyzer()
        print("âœ… æ–‡æœ¬è™•ç†çµ„ä»¶åˆå§‹åŒ–å®Œæˆ")

    def get_or_create_vectorstore(self, collection_name: str):
        """ç²å–æˆ–å‰µå»ºå‘é‡å­˜å„² - PostgreSQLå„ªå…ˆ"""
        if collection_name not in self._vector_stores:
            try:
                if self.use_postgres and PGVECTOR_AVAILABLE:
                    # ğŸ”§ ä½¿ç”¨PGVector
                    try:
                        from langchain_postgres import PGVector
                        self._vector_stores[collection_name] = PGVector(
                            embeddings=self.embeddings,
                            collection_name=collection_name,
                            connection=self.connection_string,
                            use_jsonb=True,
                        )
                    except ImportError:
                        from langchain_community.vectorstores import PGVector
                        self._vector_stores[collection_name] = PGVector(
                            connection_string=self.connection_string,
                            collection_name=collection_name,
                            embedding_function=self.embeddings,
                            distance_strategy="cosine",
                            pre_delete_collection=False,
                            logger=logger
                        )
                    print(f"âœ… PGVectorå‘é‡å­˜å„²å°±ç·’: {collection_name}")
                else:
                    # ğŸ”§ å‚™ç”¨Chroma - ç¢ºä¿å°å…¥
                    if CHROMA_AVAILABLE:
                        from langchain_community.vectorstores import Chroma
                        self._vector_stores[collection_name] = Chroma(
                            collection_name=collection_name,
                            embedding_function=self.embeddings,
                            persist_directory=str(self.persist_dir)
                        )
                        print(f"âœ… Chromaå‘é‡å­˜å„²å°±ç·’: {collection_name}")
                    else:
                        raise ImportError("Chromaä¸å¯ç”¨ä¸”PostgreSQLä¹Ÿä¸å¯ç”¨")
                        
            except Exception as e:
                logger.error(f"å‘é‡å­˜å„²å‰µå»ºå¤±æ•—: {e}")
                raise RuntimeError(f"ç„¡æ³•å‰µå»ºå‘é‡å­˜å„²: {e}")
        
        return self._vector_stores[collection_name]

    def _generate_doc_id(self, file_path: Path) -> str:
        """ç”Ÿæˆæ–‡æª”ID"""
        # âœ… ä¿®æ­£èªæ³•éŒ¯èª¤
        content_hash = hashlib.md5(str(file_path).encode()).hexdigest()[:8]
        return f"doc_{file_path.stem}_{content_hash}"

    def load_document(self, file_path: Path) -> List[Document]:
        """è¼‰å…¥ä¸¦æ™ºèƒ½è™•ç†æ–‡æª”"""
        try:
            extension = file_path.suffix.lower()
            
            # æ ¹æ“šæª”æ¡ˆé¡å‹è¼‰å…¥
            if extension == '.pdf':
                loader = PyPDFLoader(str(file_path))
                documents = loader.load()
                full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            elif extension in ['.docx', '.doc'] and DOCX2TXT_AVAILABLE:
                if DOCX_METHOD == "docx2txt":
                    import docx2txt
                    full_text = docx2txt.process(str(file_path))
                else:
                    loader = Docx2txtLoader(str(file_path))
                    documents = loader.load()
                    full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            elif extension == '.epub' and EPUB_AVAILABLE:
                # ğŸ†• EPUBè™•ç†é‚è¼¯
                epub_processor = EpubProcessor()
                full_text = epub_processor.extract_epub_content(file_path)
            elif extension == '.csv':
                loader = CSVLoader(str(file_path), encoding='utf-8')
                documents = loader.load()
                full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            elif extension == '.json':
                loader = JSONLoader(str(file_path), jq_schema='.', text_content=False)
                documents = loader.load()
                full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            else:
                # å˜—è©¦è‡ªå‹•æª¢æ¸¬ç·¨ç¢¼
                encodings = ['utf-8', 'gbk', 'gb2312', 'big5']
                full_text = None
                
                for encoding in encodings:
                    try:
                        with open(file_path, 'r', encoding=encoding) as f:
                            full_text = f.read()
                        break
                    except UnicodeDecodeError:
                        continue
                
                if full_text is None:
                    raise ValueError(f"ç„¡æ³•è§£ç¢¼æ–‡ä»¶: {file_path}")
            
            if not full_text or not full_text.strip():
                logger.warning(f"æ–‡ä»¶å…§å®¹ç‚ºç©ºæˆ–ç„¡æ³•æå–: {file_path.name}")
                return []
            
            # ç”Ÿæˆæ–‡æª”ID
            doc_id = self._generate_doc_id(file_path)
            
            # ä½¿ç”¨å„ªåŒ–çš„æ–‡æœ¬åˆ†å‰²å™¨
            source_info = {
                'file_path': str(file_path),
                'file_type': extension,
                'file_size': file_path.stat().st_size if file_path.exists() else 0
            }
            
            documents = self.text_splitter.smart_split_documents(full_text, doc_id, source_info)
            
            # æ·»åŠ çµ±ä¸€å…ƒæ•¸æ“š
            for doc in documents:
                doc.metadata.update({
                    'source': str(file_path),
                    'filename': file_path.name,
                    'extension': extension,
                    'file_size': source_info['file_size'],
                    'load_timestamp': time.time()
                })
            
            print(f"ğŸ“„ æ–‡æª”è¼‰å…¥å®Œæˆ: {file_path.name} ({len(documents)} å€‹åˆ†å¡Š)")
            
            return documents
            
        except Exception as e:
            logger.error(f"è¼‰å…¥æ–‡æª”å¤±æ•— {file_path}: {e}")
            print(f"âŒ æ–‡æª”è¼‰å…¥å¤±æ•—: {file_path.name} - {e}")
            return []

    def get_file_info(self, file_path: Path) -> Optional[FileInfo]:
        """ç²å–æª”æ¡ˆåŸºæœ¬ä¿¡æ¯"""
        try:
            if not file_path.exists():
                return None
                
            stat = file_path.stat()
            
            # è¨ˆç®—æª”æ¡ˆå“ˆå¸Œï¼ˆç”¨æ–¼è®Šæ›´æª¢æ¸¬ï¼‰
            hash_md5 = hashlib.md5()
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            
            return FileInfo(
                path=str(file_path),
                size=stat.st_size,
                mtime=stat.st_mtime,
                hash=hash_md5.hexdigest(),
                encoding='utf-8',
                file_type=file_path.suffix.lower()
            )
            
        except Exception as e:
            logger.error(f"ç²å–æª”æ¡ˆä¿¡æ¯å¤±æ•— {file_path}: {e}")
            return None

    def _parallel_load_documents(self, file_paths: List[Path], collection_name: str) -> List[Document]:
        """ä¸¦ç™¼è¼‰å…¥æ–‡æª”"""
        all_documents = []
        max_workers = min(SYSTEM_CONFIG.get("max_workers", 4), len(file_paths))
        
        print(f"   ğŸš€ ä¸¦ç™¼è¼‰å…¥ (å·¥ä½œç·šç¨‹: {max_workers})")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_file = {
                executor.submit(self.load_document, file_path): file_path 
                for file_path in file_paths
            }
            
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    documents = future.result()
                    if documents:
                        for doc in documents:
                            doc.metadata['collection'] = collection_name
                        all_documents.extend(documents)
                        print(f"   âœ… {file_path.name}: {len(documents)} åˆ†å¡Š")
                    else:
                        print(f"   âš ï¸ {file_path.name}: ç„¡æœ‰æ•ˆå…§å®¹")
                except Exception as e:
                    print(f"   âŒ {file_path.name}: {e}")
                    logger.error(f"ä¸¦ç™¼è¼‰å…¥å¤±æ•— {file_path}: {e}")
        
        return all_documents

    def _sequential_load_documents(self, file_paths: List[Path], collection_name: str) -> List[Document]:
        """é †åºè¼‰å…¥æ–‡æª”"""
        all_documents = []
        
        print(f"   ğŸ“„ é †åºè¼‰å…¥")
        
        for i, file_path in enumerate(file_paths, 1):
            try:
                print(f"   [{i}/{len(file_paths)}] è™•ç†: {file_path.name}")
                documents = self.load_document(file_path)
                
                if documents:
                    for doc in documents:
                        doc.metadata['collection'] = collection_name
                    all_documents.extend(documents)
                    print(f"      âœ… è¼‰å…¥ {len(documents)} å€‹åˆ†å¡Š")
                else:
                    print(f"      âš ï¸ ç„¡æœ‰æ•ˆå…§å®¹")
                    
            except Exception as e:
                print(f"      âŒ è¼‰å…¥å¤±æ•—: {e}")
                logger.error(f"æ–‡ä»¶è¼‰å…¥å¤±æ•— {file_path}: {e}")
        
        return all_documents

    def _load_file_records(self) -> Dict[str, Dict[str, FileInfo]]:
        """è¼‰å…¥æª”æ¡ˆè¨˜éŒ„ - åŠ å¼·éŒ¯èª¤è™•ç†å’Œæ¢å¾©æ©Ÿåˆ¶"""
        record_file = self.data_dir / "file_records.json"
        
        # ğŸ”§ æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not record_file.exists():
            print("ğŸ” æª”æ¡ˆè¨˜éŒ„ä¸å­˜åœ¨ï¼Œå°‡å»ºç«‹æ–°çš„è¨˜éŒ„")
            return {}
        
        try:
            # ğŸ”§ è®€å–ä¸¦æª¢æŸ¥æª”æ¡ˆå…§å®¹
            with open(record_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            # ğŸ”§ æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºç©º
            if not content:
                print("âš ï¸ æª”æ¡ˆè¨˜éŒ„ç‚ºç©ºï¼Œå°‡å»ºç«‹æ–°çš„è¨˜éŒ„")
                return {}
            
            # ğŸ”§ æª¢æŸ¥æ˜¯å¦ä»¥{é–‹é ­ï¼ˆåŸºæœ¬JSONæ ¼å¼æª¢æŸ¥ï¼‰
            if not content.startswith('{'):
                print(f"âš ï¸ æª”æ¡ˆè¨˜éŒ„æ ¼å¼éŒ¯èª¤ï¼Œå…§å®¹é–‹é ­: {repr(content[:50])}")
                return self._handle_corrupted_records(record_file, content)
            
            # ğŸ”§ å˜—è©¦è§£æJSON
            try:
                data = json.loads(content)
            except json.JSONDecodeError as json_error:
                print(f"âŒ JSONè§£æå¤±æ•—: {json_error}")
                print(f"   éŒ¯èª¤ä½ç½®: line {json_error.lineno}, column {json_error.colno}")
                print(f"   æª”æ¡ˆå‰100å­—ç¬¦: {repr(content[:100])}")
                return self._handle_corrupted_records(record_file, content)
            
            # ğŸ”§ é©—è­‰è³‡æ–™æ ¼å¼
            if not isinstance(data, dict):
                print(f"âš ï¸ æª”æ¡ˆè¨˜éŒ„æ ¼å¼éŒ¯èª¤ï¼Œæ‡‰ç‚ºå­—å…¸ä½†å¾—åˆ°: {type(data)}")
                return {}
            
            # ğŸ”§ è½‰æ›ç‚ºFileInfoç‰©ä»¶
            records = {}
            for collection, files in data.items():
                records[collection] = {}
                for file_path, info in files.items():
                    try:
                        if isinstance(info, dict):
                            fileinfo_fields = {
                                'path': info.get('path', file_path),
                                'size': info.get('size', 0),
                                'mtime': info.get('mtime', time.time()),
                                'hash': info.get('hash', ''),
                                'encoding': info.get('encoding', 'utf-8'),
                                'file_type': info.get('file_type', '')
                            }
                            
                            file_info_obj = FileInfo(**fileinfo_fields)
                            
                            # ğŸ”§ æ¢å¾©é¡å¤–å±¬æ€§
                            if 'uploaded_by' in info:
                                file_info_obj.uploaded_by = info['uploaded_by']
                            if 'uploaded_at' in info:
                                file_info_obj.uploaded_at = info['uploaded_at']
                            if 'file_source' in info:
                                file_info_obj.file_source = info['file_source']
                            
                            records[collection][file_path] = file_info_obj
                        else:
                            records[collection][file_path] = info
                            
                    except Exception as e:
                        logger.warning(f"è¼‰å…¥æª”æ¡ˆè¨˜éŒ„å¤±æ•— {file_path}: {e}")
                        # ğŸ”§ å»ºç«‹é è¨­è¨˜éŒ„
                        try:
                            default_info = FileInfo(
                                path=file_path,
                                size=0,
                                mtime=time.time(),
                                hash='',
                                encoding='utf-8',
                                file_type=''
                            )
                            records[collection][file_path] = default_info
                        except Exception:
                            logger.error(f"ç„¡æ³•å»ºç«‹é è¨­FileInfo for {file_path}")
                            continue
            
            print(f"âœ… æª”æ¡ˆè¨˜éŒ„è¼‰å…¥æˆåŠŸ: {len(records)} å€‹é›†åˆ")
            return records
            
        except Exception as e:
            logger.error(f"è¼‰å…¥æª”æ¡ˆè¨˜éŒ„å¤±æ•—: {e}")
            print(f"âŒ åš´é‡éŒ¯èª¤ï¼Œè¼‰å…¥æª”æ¡ˆè¨˜éŒ„å¤±æ•—: {e}")
            return self._handle_corrupted_records(record_file, "")

    def _save_file_records(self):
        """ä¿å­˜æª”æ¡ˆè¨˜éŒ„"""
        record_file = self.data_dir / "file_records.json"
        
        try:
            # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            record_file.parent.mkdir(parents=True, exist_ok=True)
            
            # è½‰æ›FileInfoç‰©ä»¶ç‚ºå­—å…¸
            serializable_records = {}
            for collection, files in self.file_records.items():
                serializable_records[collection] = {}
                for file_path, file_info in files.items():
                    if isinstance(file_info, FileInfo):
                        serializable_records[collection][file_path] = asdict(file_info)
                    else:
                        serializable_records[collection][file_path] = file_info
            
            # å…ˆå¯«å…¥è‡¨æ™‚æª”æ¡ˆï¼Œç„¶å¾ŒåŸå­æ€§æ›¿æ›
            temp_file = record_file.with_suffix('.tmp')
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_records, f, indent=2, ensure_ascii=False)
            
            temp_file.replace(record_file)
            logger.info(f"æª”æ¡ˆè¨˜éŒ„ä¿å­˜æˆåŠŸ: {len(serializable_records)} å€‹é›†åˆ")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æª”æ¡ˆè¨˜éŒ„å¤±æ•—: {e}")

    def _handle_corrupted_records(self, record_file: Path, content: str) -> Dict:
        """è™•ç†æå£çš„æª”æ¡ˆè¨˜éŒ„"""
        print(f"ğŸ”§ è™•ç†æå£çš„æª”æ¡ˆè¨˜éŒ„...")
        
        # å‰µå»ºå‚™ä»½
        try:
            backup_file = record_file.with_suffix(f'.backup_{int(time.time())}')
            if record_file.exists():
                shutil.copy2(record_file, backup_file)
                print(f"   ğŸ“¦ å·²å‰µå»ºå‚™ä»½: {backup_file.name}")
        except Exception as e:
            logger.warning(f"å‰µå»ºå‚™ä»½å¤±æ•—: {e}")
        
        # å˜—è©¦æ¢å¾©
        try:
            return self._rebuild_file_records()
        except Exception as e:
            logger.error(f"é‡å»ºæª”æ¡ˆè¨˜éŒ„å¤±æ•—: {e}")
            return {}

    def _rebuild_file_records(self) -> Dict:
        """é‡å»ºæª”æ¡ˆè¨˜éŒ„"""
        print("ğŸ”§ é‡å»ºæª”æ¡ˆè¨˜éŒ„...")
        
        new_records = {}
        
        if not self.data_dir.exists():
            return new_records
        
        # æƒææ‰€æœ‰å­ç›®éŒ„
        for subdir in self.data_dir.iterdir():
            if subdir.is_dir() and not subdir.name.startswith('.'):
                collection_name = f"collection_{subdir.name}"
                new_records[collection_name] = {}
                
                # æƒæç›®éŒ„ä¸­çš„æª”æ¡ˆ
                for file_path in subdir.rglob('*'):
                    if (file_path.is_file() and 
                        file_path.suffix.lower() in SUPPORTED_EXTENSIONS):
                        
                        file_info = self.get_file_info(file_path)
                        if file_info:
                            new_records[collection_name][str(file_path)] = file_info
        
        print(f"âœ… é‡å»ºå®Œæˆ: {len(new_records)} å€‹é›†åˆ")
        return new_records

    def get_file_source_statistics(self) -> Dict[str, Dict[str, int]]:
        """ç²å–æª”æ¡ˆä¾†æºçµ±è¨ˆ"""
        stats = {}
        
        for collection_name, files in self.file_records.items():
            stats[collection_name] = {
                'total': len(files),
                'upload': 0,
                'sync': 0,
                'unknown': 0
            }
            
            for file_info in files.values():
                source = getattr(file_info, 'file_source', 'unknown')
                if source in stats[collection_name]:
                    stats[collection_name][source] += 1
                else:
                    stats[collection_name]['unknown'] += 1
        
        return stats

    def diagnose_file_records(self) -> Dict:
        """è¨ºæ–·æª”æ¡ˆè¨˜éŒ„"""
        diagnosis = {
            'total_collections': len(self.file_records),
            'total_files': sum(len(files) for files in self.file_records.values()),
            'missing_files': 0,
            'outdated_files': 0,
            'corrupted_records': 0
        }
        
        for collection_name, files in self.file_records.items():
            for file_path, file_info in files.items():
                try:
                    path_obj = Path(file_path)
                    
                    if not path_obj.exists():
                        diagnosis['missing_files'] += 1
                        continue
                    
                    current_mtime = path_obj.stat().st_mtime
                    if abs(current_mtime - file_info.mtime) > 1:
                        diagnosis['outdated_files'] += 1
                        
                except Exception:
                    diagnosis['corrupted_records'] += 1
        
        return diagnosis

    def cleanup_invalid_records(self) -> Dict:
        """æ¸…ç†ç„¡æ•ˆè¨˜éŒ„"""
        removed_count = 0
        
        for collection_name in list(self.file_records.keys()):
            files = self.file_records[collection_name]
            
            for file_path in list(files.keys()):
                try:
                    if not Path(file_path).exists():
                        del files[file_path]
                        removed_count += 1
                except Exception:
                    del files[file_path]
                    removed_count += 1
            
            # ç§»é™¤ç©ºé›†åˆ
            if not files:
                del self.file_records[collection_name]
        
        if removed_count > 0:
            self._save_file_records()
        
        return {'removed_records': removed_count}

    def _process_batches(self, vectorstore: Union["Chroma", Any], batches: List[Tuple[List[Document], Dict]]) -> int:
        """è™•ç†æ‰¹æ¬¡å‘é‡åŒ– - å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œå…ƒæ•¸æ“šä¿®å¾©"""

        success_count = 0
        total_docs = sum(len(batch_docs) for batch_docs, _ in batches)
        
        print(f"\nğŸ“„ é–‹å§‹æ‰¹æ¬¡å‘é‡åŒ–...")
        print(f"   ğŸ“¦ ç¸½æ‰¹æ¬¡æ•¸: {len(batches)}")
        print(f"   ğŸ“„ ç¸½æ–‡æª”æ•¸: {total_docs}")
        
        for batch_num, (batch_docs, batch_info) in enumerate(batches, 1):
            print(f"\n   ğŸ“¦ æ‰¹æ¬¡ {batch_num}/{len(batches)}")
            print(f"      ğŸ“„ æ–‡æª”æ•¸: {batch_info['documents']}")
            print(f"      ğŸ” tokens: {batch_info['tokens']:,}")
            print(f"      ğŸ“Š ä½¿ç”¨ç‡: {(batch_info['tokens']/TOKEN_LIMITS['max_tokens_per_request']*100):.1f}%")
            
            # é¡¯ç¤ºæ–‡æª”é¡å‹åˆ†å¸ƒ
            type_info = ", ".join([f"{k}:{v}" for k, v in batch_info['types'].items()])
            print(f"      ğŸ·ï¸ é¡å‹: {type_info}")
            
            start_time = time.time()
            
            try:
                print(f"      ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ {batch_num}...")
                print(f"      ğŸ“¡ æ­£åœ¨èª¿ç”¨OpenAI API... (é€™å¯èƒ½éœ€è¦30-60ç§’)")
                
                # ğŸ› ï¸ ä¿®å¾©ï¼šçµ±ä¸€è™•ç†å…ƒæ•¸æ“šï¼Œç¢ºä¿é¡å‹æ­£ç¢º
                safe_docs = []
                for doc in batch_docs:
                    safe_metadata = self._ensure_simple_metadata(doc.metadata)
                    safe_doc = Document(page_content=doc.page_content, metadata=safe_metadata)
                    safe_docs.append(safe_doc)

                print(f"      ğŸ”§ å·²è™•ç† {len(safe_docs)} å€‹æ–‡æª”çš„å…ƒæ•¸æ“šï¼Œç¢ºä¿é¡å‹å…¼å®¹")
                
                vectorstore.add_documents(safe_docs)
                processing_time = time.time() - start_time
                
                success_count += len(batch_docs)
                self.batch_processor.record_batch_result(True, processing_time)
                
                print(f"      âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ ({processing_time:.1f}s)")
                print(f"      ğŸ“Š ç¸½é€²åº¦: {success_count}/{total_docs} ({success_count/total_docs*100:.1f}%)")
                
                # æ‰¹æ¬¡é–“å»¶é²
                if batch_num < len(batches):
                    delay = TOKEN_LIMITS["batch_delay"]
                    print(f"      â±±ï¸ ç­‰å¾… {delay} ç§’...")
                    time.sleep(delay)
                    
            except Exception as e:
                processing_time = time.time() - start_time
                self.batch_processor.record_batch_result(False, processing_time)
                
                error_msg = str(e)
                print(f"      âŒ æ‰¹æ¬¡ {batch_num} å¤±æ•— ({processing_time:.1f}s)")
                print(f"         éŒ¯èª¤: {error_msg}")
                
                # ğŸ”§ ç‰¹åˆ¥è™•ç†å…ƒæ•¸æ“šéŒ¯èª¤
                if "metadata" in error_msg.lower():
                    print(f"         ğŸ”§ æª¢æ¸¬åˆ°å…ƒæ•¸æ“šéŒ¯èª¤ï¼Œå˜—è©¦æ›´åš´æ ¼çš„è™•ç†...")
                    try:
                        # é‡æ–°å˜—è©¦ï¼Œä½¿ç”¨æœ€åš´æ ¼çš„å…ƒæ•¸æ“šéæ¿¾
                        ultra_safe_docs = []
                        for doc in batch_docs:
                            # åªä¿ç•™æœ€åŸºæœ¬çš„å…ƒæ•¸æ“šå­—æ®µ
                            minimal_metadata = {
                                'doc_id': str(doc.metadata.get('doc_id', 'unknown')),
                                'chunk_id': str(doc.metadata.get('chunk_id', 'unknown')),
                                'chunk_index': int(doc.metadata.get('chunk_index', 0)),
                                'text_type': str(doc.metadata.get('text_type', 'unknown')),
                                'source': str(doc.metadata.get('source', 'unknown')),
                                'filename': str(doc.metadata.get('filename', 'unknown')),
                                'token_count': int(doc.metadata.get('token_count', 0)),
                                'chunk_length': int(doc.metadata.get('chunk_length', 0)),
                                # URLè™•ç†
                                'contained_urls': str(doc.metadata.get('contained_urls', '')),
                                'url_count': int(doc.metadata.get('url_count', 0)),
                                'has_urls': bool(doc.metadata.get('has_urls', False))
                            }
                            
                            ultra_safe_doc = Document(
                                page_content=doc.page_content,
                                metadata=minimal_metadata
                            )
                            ultra_safe_docs.append(ultra_safe_doc)
                        
                        vectorstore.add_documents(ultra_safe_docs)
                        success_count += len(batch_docs)
                        print(f"         âœ… ä½¿ç”¨æœ€å°åŒ–å…ƒæ•¸æ“šé‡æ–°è™•ç†æˆåŠŸ")
                        continue
                        
                    except Exception as retry_e:
                        print(f"         âŒ é‡æ–°è™•ç†ä¹Ÿå¤±æ•—: {retry_e}")
                
                # å…¶ä»–éŒ¯èª¤è™•ç†
                if "timeout" in error_msg.lower():
                    print(f"         ğŸ• è¶…æ™‚éŒ¯èª¤ï¼Œå»¶é•·ç­‰å¾…æ™‚é–“...")
                    time.sleep(30)
                elif "rate_limit" in error_msg.lower() or "429" in error_msg:
                    print(f"         ğŸš¦ é€Ÿç‡é™åˆ¶ï¼Œå»¶é•·ç­‰å¾…...")
                    time.sleep(60)
                elif "token" in error_msg.lower() and batch_info['documents'] > 1:
                    print(f"         ğŸ”§ Tokenè¶…é™ï¼Œå˜—è©¦å–®å€‹è™•ç†...")
                    single_success = self._process_documents_individually(vectorstore, batch_docs)
                    success_count += single_success
                elif "connection" in error_msg.lower():
                    print(f"         ğŸŒ é€£æ¥éŒ¯èª¤ï¼Œç­‰å¾…é‡è©¦...")
                    time.sleep(20)
                    try:
                        print(f"         ğŸ“„ é‡è©¦æ‰¹æ¬¡ {batch_num}...")
                        # ä½¿ç”¨å®‰å…¨çš„å…ƒæ•¸æ“šé‡è©¦
                        safe_docs = []
                        for doc in batch_docs:
                            safe_metadata = self._ensure_simple_metadata(doc.metadata)
                            safe_doc = Document(page_content=doc.page_content, metadata=safe_metadata)
                            safe_docs.append(safe_doc)
                        
                        vectorstore.add_documents(safe_docs)
                        success_count += len(batch_docs)
                        print(f"         âœ… é‡è©¦æˆåŠŸ")
                    except Exception as retry_e:
                        print(f"         âŒ é‡è©¦å¤±æ•—: {retry_e}")
                else:
                    print(f"         âš ï¸ è·³éæ­¤æ‰¹æ¬¡")
                    
                # æ¯æ¬¡éŒ¯èª¤å¾Œæ·»åŠ é¡å¤–å»¶é²
                print(f"         â¸ºï¸ éŒ¯èª¤å¾Œæš«åœ10ç§’...")
                time.sleep(10)
        
        return success_count

    def _ensure_simple_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """ç¢ºä¿å…ƒæ•¸æ“šåªåŒ…å«Chromaæ”¯æŒçš„ç°¡å–®é¡å‹ï¼šstring, int, float, bool"""
        safe_metadata = {}
        
        for key, value in metadata.items():
            if isinstance(value, (str, int, float, bool)) or value is None:
                # âœ… å·²ç¶“æ˜¯Chromaæ”¯æŒçš„ç°¡å–®é¡å‹ï¼Œç›´æ¥ä¿ç•™
                safe_metadata[key] = value
            elif isinstance(value, list):
                # âŒ åˆ—è¡¨ä¸è¢«æ”¯æŒ â†’ è½‰ç‚ºåˆ†éš”ç¬¦å­—ç¬¦ä¸²
                if key == 'contained_urls' or 'url' in key.lower():
                    safe_metadata[key] = '|'.join(str(v) for v in value) if value else ''
                    safe_metadata[f'{key}_count'] = len(value)
                else:
                    safe_metadata[key] = '|'.join(str(v) for v in value) if value else ''
                    safe_metadata[f'{key}_count'] = len(value)
            elif isinstance(value, dict):
                # âŒ å­—å…¸ä¸è¢«æ”¯æŒ â†’ è½‰ç‚ºJSONå­—ç¬¦ä¸²
                safe_metadata[key] = json.dumps(value, ensure_ascii=False)
            else:
                # âŒ å…¶ä»–é¡å‹ä¸è¢«æ”¯æŒ â†’ è½‰ç‚ºå­—ç¬¦ä¸²
                safe_metadata[key] = str(value)
        
        return safe_metadata

    def _process_documents_individually(self, vectorstore, documents: List[Document]) -> int:
        """å–®å€‹è™•ç†æ–‡æª”"""
        success_count = 0
        
        for i, doc in enumerate(documents):
            try:
                doc_tokens = doc.metadata.get('token_count', 0)
                if doc_tokens > TOKEN_LIMITS['max_tokens_per_request']:
                    print(f"         âš ï¸ æ–‡æª” {i+1} ä»ç„¶éå¤§ ({doc_tokens:,} tokens)ï¼Œè·³é")
                    continue
                
                vectorstore.add_documents([doc])
                success_count += 1
                print(f"         âœ… å–®å€‹æ–‡æª” {i+1}/{len(documents)} å®Œæˆ")
                time.sleep(1)  # å–®å€‹è™•ç†æ™‚çŸ­æš«å»¶é²
                
            except Exception as e:
                print(f"         âŒ å–®å€‹æ–‡æª” {i+1} å¤±æ•—: {e}")
        
        return success_count

    def incremental_update(self, collection_name: str, added_files: List[Path], 
                          modified_files: List[Path], deleted_files: List[str],
                          current_files: Dict[str, FileInfo]) -> bool:
        """ğŸš€ å„ªåŒ–ç‰ˆå¢é‡æ›´æ–°"""
        with self.processing_lock:
            try:
                vectorstore = self.get_or_create_vectorstore(collection_name)
                
                # è™•ç†åˆªé™¤å’Œä¿®æ”¹
                files_to_delete = deleted_files + [str(f) for f in modified_files]
                if files_to_delete:
                    for file_path in files_to_delete:
                        try:
                            vectorstore.delete(filter={"source": file_path})
                            print(f"ğŸ—‘ï¸ å·²åˆªé™¤: {Path(file_path).name}")
                        except Exception as e:
                            logger.warning(f"åˆªé™¤æ–‡æª”å¤±æ•— {file_path}: {e}")
                
                # è™•ç†æ–°å¢å’Œä¿®æ”¹çš„æ–‡ä»¶
                files_to_process = added_files + modified_files
                if not files_to_process:
                    print("   âœ… ç„¡æ–°æ–‡ä»¶éœ€è¦è™•ç†")
                    return True
                
                print(f"ğŸ“„ é–‹å§‹è™•ç† {len(files_to_process)} å€‹æ–‡ä»¶...")
                print(f"   âš ï¸ è™•ç†å¤§æ–‡ä»¶å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å¾…...")
                
                # ä¸¦ç™¼è¼‰å…¥æ–‡æª”
                all_documents = []
                if PERFORMANCE_CONFIG.get("parallel_processing", True):
                    all_documents = self._parallel_load_documents(files_to_process, collection_name)
                else:
                    all_documents = self._sequential_load_documents(files_to_process, collection_name)
                
                if not all_documents:
                    print("   âš ï¸ æ²’æœ‰æœ‰æ•ˆæ–‡æª”éœ€è¦å‘é‡åŒ–")
                    return True
                
                # çµ±è¨ˆå’Œæˆæœ¬ä¼°ç®—
                total_tokens = sum(doc.metadata.get('token_count', 0) for doc in all_documents)
                estimated_cost = self.batch_processor.token_estimator.estimate_embedding_cost(total_tokens)
                
                print(f"\nğŸ“Š å‘é‡åŒ–çµ±è¨ˆ:")
                print(f"   ğŸ“„ ç¸½åˆ†å¡Šæ•¸: {len(all_documents)}")
                print(f"   ğŸ” ç¸½tokens: {total_tokens:,}")
                print(f"   ğŸ’° ä¼°ç®—æˆæœ¬: ${estimated_cost:.4f}")
                
                # å‰µå»ºæ™ºèƒ½æ‰¹æ¬¡ä¸¦è™•ç†
                batches = self.batch_processor.create_smart_batches(all_documents)
                success_count = self._process_batches(vectorstore, batches)
                
                print(f"\nğŸ‰ å‘é‡åŒ–å®Œæˆï¼")
                print(f"   âœ… æˆåŠŸ: {success_count}/{len(all_documents)} å€‹åˆ†å¡Š")
                print(f"   ğŸ“Š æˆåŠŸç‡: {(success_count/len(all_documents)*100):.1f}%")
                
                # æ›´æ–°æ–‡ä»¶è¨˜éŒ„
                self.file_records[collection_name] = current_files
                self._save_file_records()
                
                # è¨˜æ†¶é«”æ¸…ç†
                if success_count % PERFORMANCE_CONFIG.get("gc_frequency", 50) == 0:
                    gc.collect()
                
                return success_count > 0
                
            except Exception as e:
                logger.error(f"å¢é‡æ›´æ–°å¤±æ•— {collection_name}: {e}")
                print(f"âŒ ç³»çµ±éŒ¯èª¤: {e}")
                return False

    def get_collection_name(self, dir_path: Path) -> str:
        """ç²å–é›†åˆåç¨±"""
        try:
            relative_path = dir_path.relative_to(self.data_dir)
            if relative_path.parts:
                return f"collection_{relative_path.parts[0]}"
        except ValueError:
            pass
        return "collection_other"

    def sync_collections(self) -> int:
        """ç´”PostgreSQLæ–¹æ¡ˆï¼šä¸å†æƒææœ¬åœ°ç›®éŒ„"""
        print("ğŸ“„ ç´”PostgreSQLæ–¹æ¡ˆï¼šè·³éæœ¬åœ°ç›®éŒ„æƒæ")
        print("âœ… æ‰€æœ‰æ•¸æ“šéƒ½åœ¨PostgreSQLä¸­ï¼Œç„¡éœ€åŒæ­¥")
        return 0

    def scan_directory_changes(self, dir_path: Path, collection_name: str) -> Tuple[List[Path], List[Path], List[str], Dict[str, FileInfo]]:
        """æƒæç›®éŒ„è®Šæ›´ - ä¿®æ­£ç‰ˆï¼šæ­£ç¢ºè™•ç†ä¸Šå‚³æª”æ¡ˆ"""
        current_files = {}
        
        print(f"ğŸ” æƒæç›®éŒ„: {dir_path}")
        
        # éæ­¸æƒæç›®éŒ„
        file_count = 0
        for file_path in dir_path.rglob('*'):
            if (
                file_path.is_file() and 
                file_path.suffix.lower() in SUPPORTED_EXTENSIONS and
                not file_path.name.startswith('.') and
                file_path.stat().st_size > 0
            ):  # è·³éç©ºæ–‡ä»¶
                
                file_info = self.get_file_info(file_path)
                if file_info:
                    # ğŸ†• ä¿®æ­£ï¼šä½¿ç”¨æ¨™æº–åŒ–çš„çµ•å°è·¯å¾‘ä½œç‚ºéµå€¼
                    try:
                        # ä½¿ç”¨absolute()é¿å…ç¬¦è™Ÿé€£çµå•é¡Œ
                        absolute_path = str(file_path.absolute())
                        current_files[absolute_path] = file_info
                        file_count += 1
                    except Exception as e:
                        logger.warning(f"è·¯å¾‘æ¨™æº–åŒ–å¤±æ•— {file_path}: {e}")
                        # å›é€€åˆ°åŸå§‹è·¯å¾‘
                        current_files[str(file_path)] = file_info
                        file_count += 1
        
        print(f"ğŸ“„ æ‰¾åˆ° {file_count} å€‹æœ‰æ•ˆæª”æ¡ˆ")
        
        old_files = self.file_records.get(collection_name, {})
        print(f"ğŸ“‹ èˆŠè¨˜éŒ„ä¸­æœ‰ {len(old_files)} å€‹æª”æ¡ˆ")
        
        # ğŸ†• ä¿®æ­£ï¼šæ­£è¦åŒ–èˆŠè¨˜éŒ„çš„è·¯å¾‘éµå€¼
        normalized_old_files = {}
        normalization_errors = 0
        
        for old_path, old_info in old_files.items():
            try:
                old_path_obj = Path(old_path)
                
                if old_path_obj.is_absolute():
                    # å·²ç¶“æ˜¯çµ•å°è·¯å¾‘
                    normalized_key = str(old_path_obj.absolute())
                else:
                    # ç›¸å°è·¯å¾‘è½‰çµ•å°è·¯å¾‘
                    try:
                        abs_path = (dir_path / old_path).absolute()
                        normalized_key = str(abs_path)
                    except Exception:
                        # å¦‚æœç„¡æ³•è½‰æ›ï¼Œä¿æŒåŸæ¨£
                        normalized_key = old_path
                        
                normalized_old_files[normalized_key] = old_info
                
            except Exception as e:
                logger.warning(f"èˆŠè·¯å¾‘æ­£è¦åŒ–å¤±æ•— {old_path}: {e}")
                # ä¿æŒåŸè·¯å¾‘
                normalized_old_files[old_path] = old_info
                normalization_errors += 1
        
        if normalization_errors > 0:
            print(f"âš ï¸ {normalization_errors} å€‹èˆŠè·¯å¾‘æ­£è¦åŒ–å¤±æ•—")
        
        # ğŸ†• ä¿®æ­£ï¼šæ™ºèƒ½è®Šæ›´æª¢æ¸¬
        added_files = []
        modified_files = []
        
        print("ğŸ” æª¢æ¸¬è®Šæ›´...")
        
        for file_path, file_info in current_files.items():
            current_file_name = Path(file_path).name
            current_hash = file_info.hash
            
            # é¦–å…ˆå˜—è©¦ç²¾ç¢ºè·¯å¾‘åŒ¹é…
            if file_path in normalized_old_files:
                old_info = normalized_old_files[file_path]
                if old_info.hash != current_hash:
                    modified_files.append(Path(file_path))
                    print(f"ğŸ” ä¿®æ”¹æª”æ¡ˆ: {current_file_name}")
            else:
                # ğŸ†• æ™ºèƒ½æª”æ¡ˆåŒ¹é…ï¼šæª¢æŸ¥æ˜¯å¦æ˜¯åŒä¸€æª”æ¡ˆçš„ä¸åŒè·¯å¾‘è¡¨ç¤º
                file_found = False
                
                for old_path, old_info in normalized_old_files.items():
                    old_file_name = Path(old_path).name
                    
                    # æª”æ¡ˆåç›¸åŒä¸”å“ˆå¸Œç›¸åŒ = åŒä¸€æª”æ¡ˆ
                    if (
                        current_file_name == old_file_name and 
                        current_hash == old_info.hash
                    ):
                        file_found = True
                        print(f"ğŸ“„ è·¯å¾‘è®Šæ›´ä½†å…§å®¹ç›¸åŒ: {current_file_name}")
                        break
                        
                    # æª”æ¡ˆåç›¸åŒä½†å“ˆå¸Œä¸åŒ = æª”æ¡ˆè¢«ä¿®æ”¹
                    elif (
                        current_file_name == old_file_name and 
                        current_hash != old_info.hash
                    ):
                        modified_files.append(Path(file_path))
                        file_found = True
                        print(f"ğŸ” ä¿®æ”¹æª”æ¡ˆ (è·¯å¾‘è®Šæ›´): {current_file_name}")
                        break
                
                if not file_found:
                    added_files.append(Path(file_path))
                    print(f"ğŸ“„ æ–°æª”æ¡ˆ: {current_file_name}")
        
        # ğŸ†• ä¿®æ­£ï¼šæ™ºèƒ½åˆªé™¤æª¢æ¸¬
        deleted_files = []
        
        for old_path in normalized_old_files.keys():
            old_file_name = Path(old_path).name
            
            if old_path not in current_files:
                # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦çœŸçš„ä¸å­˜åœ¨ï¼ˆå¯èƒ½åªæ˜¯è·¯å¾‘è¡¨ç¤ºä¸åŒï¼‰
                file_still_exists = False
                
                for current_path in current_files.keys():
                    current_file_name = Path(current_path).name
                    if current_file_name == old_file_name:
                        # é€²ä¸€æ­¥æª¢æŸ¥æ˜¯å¦æ˜¯åŒä¸€æª”æ¡ˆï¼ˆé€šéå…§å®¹å“ˆå¸Œï¼‰
                        current_hash = current_files[current_path].hash
                        old_hash = normalized_old_files[old_path].hash
                        
                        if current_hash == old_hash:
                            file_still_exists = True
                            break
                
                if not file_still_exists:
                    deleted_files.append(old_path)
                    print(f"ğŸ—‘ï¸ åˆªé™¤æª”æ¡ˆ: {old_file_name}")
        
        print(f"ğŸ“Š è®Šæ›´çµ±è¨ˆ:")
        print(f"   ğŸ“„ æ–°å¢: {len(added_files)}")
        print(f"   ğŸ” ä¿®æ”¹: {len(modified_files)}")
        print(f"   ğŸ—‘ï¸ åˆªé™¤: {len(deleted_files)}")
        
        return added_files, modified_files, deleted_files, current_files


# âœ… ç¢ºä¿æ‰€æœ‰importéƒ½èƒ½æ­£å¸¸å·¥ä½œ
__all__ = ['VectorOperationsCore']

"""
ğŸ“‹ æª”æ¡ˆ4AåŒ…å«çš„26å€‹åº•å±¤æ ¸å¿ƒæ–¹æ³•ï¼š
ğŸ”§ ç³»çµ±åˆå§‹åŒ– (5å€‹): __init__, _setup_embedding_model, _setup_text_processing, 
                   get_or_create_vectorstore, _generate_doc_id
ğŸ“„ æ–‡æª”è¼‰å…¥è™•ç† (4å€‹): load_document, get_file_info, _parallel_load_documents, _sequential_load_documents  
ğŸ“‹ æ–‡ä»¶è¨˜éŒ„ç®¡ç† (7å€‹): _load_file_records, _save_file_records, _handle_corrupted_records, 
                    _rebuild_file_records, get_file_source_statistics, diagnose_file_records, 
                    cleanup_invalid_records
ğŸ”— å‘é‡è™•ç†æ ¸å¿ƒ (4å€‹): _process_batches, _ensure_simple_metadata, _process_documents_individually, 
                    incremental_update
ğŸ“Š é›†åˆç®¡ç† (6å€‹): get_collection_name, sync_collections, scan_directory_changes, ä»¥åŠåŸæ–¹æ¡ˆä¸­
                å¾4Bç§»å‹•éä¾†çš„3å€‹åŸºç¤æ–¹æ³•æ§‹æˆäº†å®Œæ•´çš„åº•å±¤æ“ä½œæ”¯æ’

âš ï¸  è¢«ä¾è³´é—œä¿‚ï¼šmanagement_api.py (æª”æ¡ˆ4B) ä¾è³´æ­¤æª”æ¡ˆ
âœ… æ‹†åˆ†å®Œæˆï¼šæ­¤æª”æ¡ˆåŒ…å«æ‰€æœ‰åº•å±¤æ•¸æ“šæ“ä½œå’Œå‘é‡è™•ç†åŠŸèƒ½
"""