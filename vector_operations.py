#!/usr/bin/env python3
"""
å‘é‡æ“ä½œæ ¸å¿ƒå±¤ - VectorOperationsCore
è·è²¬ï¼šåº•å±¤æ•¸æ“šæ“ä½œã€å‘é‡è™•ç†ã€æ–‡æª”è¼‰å…¥ã€é›†åˆç®¡ç†
åŒ…å«26å€‹åº•å±¤æ ¸å¿ƒæ–¹æ³•ï¼Œå¾åŸ OptimizedVectorSystem ç²¾ç¢ºç§»å‹•è€Œä¾†
"""

import time
import json
import hashlib
import re
import logging
import os
import gc
import threading
import tempfile
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set, Any, Union
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict

# LangChainæ ¸å¿ƒ
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    TextLoader, PyPDFLoader, Docx2txtLoader, 
    CSVLoader, JSONLoader, PandasExcelLoader
)

# --- Start of content from core_config.py ---

# ğŸ†• 1. æ·»åŠ  Railway ç’°å¢ƒæª¢æ¸¬å‡½æ•¸ï¼ˆæ”¾åœ¨æ–‡ä»¶é ‚éƒ¨ï¼Œå°å…¥èªå¥ä¹‹å¾Œï¼‰
def detect_railway_environment():
    """æª¢æ¸¬æ˜¯å¦åœ¨ Railway ç’°å¢ƒä¸­é‹è¡Œ"""
    railway_indicators = [
        os.getenv("RAILWAY_PROJECT_ID"),
        os.getenv("RAILWAY_SERVICE_ID"), 
        os.getenv("DATABASE_URL"),
        "railway.internal" in os.getenv("POSTGRES_HOST", "")
    ]
    
    is_railway = any(railway_indicators)
    if is_railway:
        print("ğŸš‚ æª¢æ¸¬åˆ° Railway éƒ¨ç½²ç’°å¢ƒ")
        # é¡¯ç¤º Railway ç‰¹å®šä¿¡æ¯
        project_id = os.getenv("RAILWAY_PROJECT_ID", "unknown")
        service_id = os.getenv("RAILWAY_SERVICE_ID", "unknown")  
        print(f"   é …ç›®ID: {project_id[:8]}***")
        print(f"   æœå‹™ID: {service_id[:8]}***")
    
    return is_railway

# ğŸ”§ å„ªåŒ–ç‰ˆç³»çµ±é…ç½®
SYSTEM_CONFIG = {
    "persist_dir": "./chroma_langchain_db",
    "data_dir": "./data", 
    "device": "cpu",  
    "log_level": "INFO",
    "max_workers": 4,  # ä¸¦ç™¼è™•ç†æ•¸
    "cache_embeddings": True,  # å¿«å–åµŒå…¥å‘é‡
    "backup_enabled": True  # è‡ªå‹•å‚™ä»½
}

# ğŸ§  æ™ºèƒ½æ–‡æœ¬åˆ†é¡é…ç½®
SMART_TEXT_CONFIG = {
    # æ–‡æœ¬é•·åº¦é–¾å€¼ï¼ˆå­—ç¬¦æ•¸ï¼‰
    "micro_text_threshold": 100,      # å¾®æ–‡æœ¬ï¼šæ¨™é¡Œã€æ‘˜è¦ç­‰
    "short_text_threshold": 500,      # çŸ­æ–‡æœ¬ï¼šçŸ­æ–‡ç« ã€æ®µè½
    "medium_text_threshold": 2000,    # ä¸­ç­‰æ–‡æœ¬ï¼šä¸­ç¯‡æ–‡ç« 
    "long_text_threshold": 8000,    # é•·æ–‡æœ¬ï¼šé•·ç¯‡æ–‡ç« 
    "ultra_long_threshold": 20000,    # è¶…é•·æ–‡æœ¬ï¼šæ›¸ç±ç« ç¯€
    
    # å„é¡æ–‡æœ¬çš„è™•ç†ç­–ç•¥
    "micro_text": {
        "min_length": 20,
        "process_whole": True,
        "merge_with_next": True,  # å˜—è©¦èˆ‡ä¸‹ä¸€å€‹ç‰‡æ®µåˆä½µ
        "chunk_size": 0,
        "description": "å¾®æ–‡æœ¬æ•´é«”è™•ç†"
    },
    
    "short_text": {
        "min_length": 50,
        "process_whole": True,
        "preserve_structure": True,
        "chunk_size": 0,
        "allow_merge": True,  # å…è¨±åˆä½µç›¸é„°çŸ­æ–‡æœ¬
        "description": "çŸ­æ–‡æœ¬æ•´é«”ä¿å­˜"
    },
    
    "medium_text": {
        "chunk_size": 800,
        "chunk_overlap": 120,
        "preserve_paragraphs": True,
        "smart_boundaries": True,  # æ™ºèƒ½é‚Šç•Œæª¢æ¸¬
        "min_chunk_ratio": 0.3,   # æœ€å°ç‰‡æ®µæ¯”ä¾‹
        "description": "ä¸­ç­‰æ–‡æœ¬æ®µè½æ„ŸçŸ¥åˆ†å‰²"
    },
    
    "long_text": {
        "chunk_size": 500,
        "chunk_overlap": 80,
        "hierarchical_split": True,
        "section_aware": True,    # ç« ç¯€æ„ŸçŸ¥
        "quality_check": True,    # è³ªé‡æª¢æŸ¥
        "description": "é•·æ–‡æœ¬ç²¾ç´°åŒ–åˆ†å‰²"
    },
    
    "ultra_long": {
        "chunk_size": 600,
        "chunk_overlap": 100,
        "multi_level_split": True,  # å¤šå±¤ç´šåˆ†å‰²
        "chapter_detection": True,  # ç« ç¯€æª¢æ¸¬
        "summary_chunks": True,     # ç”Ÿæˆæ‘˜è¦ç‰‡æ®µ
        "description": "è¶…é•·æ–‡æœ¬éšå±¤å¼è™•ç†"
    },
    
    "mega_text": {
        "chunk_size": 500,
        "chunk_overlap": 80,
        "multi_level_split": True,  # å¤šå±¤ç´šåˆ†å‰²
        "chapter_detection": True,  # ç« ç¯€æª¢æ¸¬
        "aggressive_split": True,   # ç©æ¥µåˆ†å‰²
        "quality_filter": True,     # è³ªé‡éæ¿¾
        "description": "è¶…å¤§æ–‡æœ¬ç©æ¥µåˆ†å‰²è™•ç†"
    }
}

# ğŸ”§ å„ªåŒ–ç‰ˆ Token é™åˆ¶é…ç½®
TOKEN_LIMITS = {
    "max_tokens_per_request": 150000,  # æ›´ä¿å®ˆçš„é™åˆ¶
    "max_batch_size": 8,               # æ¸›å°æ‰¹æ¬¡å¤§å°
    "min_batch_size": 1,
    "batch_delay": 5.0,                # å¢åŠ æ‰¹æ¬¡é–“å»¶é²
    "retry_delay": 10,
    "max_retries": 3,
    "token_safety_margin": 0.2,        # 20% å®‰å…¨é‚Šéš›
    "adaptive_batching": True           # è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°
}

# ğŸ”§ æ€§èƒ½å„ªåŒ–é…ç½®
PERFORMANCE_CONFIG = {
    "embedding_batch_size": 12,
    "parallel_processing": True,
    "memory_limit_mb": 1024,      # è¨˜æ†¶é«”é™åˆ¶
    "chunk_cache_size": 1000,     # åˆ†å¡Šå¿«å–å¤§å°
    "preload_models": True,       # é è¼‰å…¥æ¨¡å‹
    "gc_frequency": 50            # åƒåœ¾å›æ”¶é »ç‡
}

# æ”¯æŒçš„æ–‡ä»¶æ ¼å¼æ“´å±•
SUPPORTED_EXTENSIONS = {
    '.txt', '.md', '.pdf', '.csv', '.json', '.py', '.js', 
    '.docx', '.doc', '.epub', '.rst', '.org', '.xlsx', '.xls'
}

from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from langchain_community.vectorstores import Chroma

try:
    from langchain_community.vectorstores import Chroma
    CHROMA_AVAILABLE = True
except ImportError:
    CHROMA_AVAILABLE = False
    # å‰µå»ºä¸€å€‹è™›æ“¬çš„ Chroma é¡å‹ç”¨æ–¼é¡å‹è¨»è§£
    class Chroma:
        pass

# ğŸ”§ æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
def check_openai_api_key():
    """æª¢æŸ¥ OpenAI API Key"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸ æœªè¨­ç½® OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
        print("   è«‹åœ¨ .env æ–‡ä»¶ä¸­è¨­ç½®: OPENAI_API_KEY=sk-your-api-key")
        return False
    
    if api_key.startswith("sk-") and len(api_key) > 20:
        print("âœ… OpenAI API Key æ ¼å¼æ­£ç¢º")
        print(f"   API Key: {api_key[:8]}***{api_key[-4:]}")
        return True
    else:
        print("âš ï¸ OpenAI API Key æ ¼å¼å¯èƒ½ä¸æ­£ç¢º")
        return False

# LangChain æ ¸å¿ƒçµ„ä»¶å°å…¥
try:
    from langchain_postgres import PGVector
    PGVECTOR_AVAILABLE = True
    print("âœ… PGVector å¯ç”¨")
except ImportError:
    try:
        from langchain_community.vectorstores import PGVector
        PGVECTOR_AVAILABLE = True
        print("âœ… PGVector (community) å¯ç”¨")
    except ImportError:
        PGVECTOR_AVAILABLE = False
        print("âŒ PGVector ä¸å¯ç”¨")
        # å¯ä»¥å›é€€åˆ° Chroma
        from langchain_community.vectorstores import Chroma

# OpenAI embeddings å°å…¥
try:
    from langchain_openai import OpenAIEmbeddings
    OPENAI_EMBEDDINGS_AVAILABLE = True
    print("âœ… OpenAI Embeddings å¯ç”¨")
    
    if check_openai_api_key():
        print("âœ… OpenAI ç’°å¢ƒæª¢æŸ¥é€šé")
    else:
        print("âš ï¸ OpenAI ç’°å¢ƒé…ç½®å¯èƒ½æœ‰å•é¡Œ")
        
except ImportError as e:
    OPENAI_EMBEDDINGS_AVAILABLE = False
    print(f"âš ï¸ OpenAI Embeddings ä¸å¯ç”¨: {e}")

# ğŸ”§ PostgreSQL ä¾è³´æª¢æŸ¥
try:
    import psycopg2
    PSYCOPG2_AVAILABLE = True
except ImportError:
    PSYCOPG2_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: psycopg2 æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install psycopg2-binary")

# ä¸­æ–‡åˆ†è©å’Œè½‰æ›
try:
    import jieba
    jieba.initialize()
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: jieba æœªå®‰è£ï¼Œä¸­æ–‡åˆ†è©åŠŸèƒ½å°‡è¢«ç¦ç”¨")

try:
    import opencc
    OPENCC_AVAILABLE = True
except ImportError:
    OPENCC_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: opencc æœªå®‰è£ï¼Œç¹ç°¡è½‰æ›åŠŸèƒ½å°‡è¢«ç¦ç”¨")

# æ–‡æª”è™•ç†æ”¯æŒ
try:
    import docx2txt
    DOCX2TXT_AVAILABLE = True
    DOCX_METHOD = "docx2txt"
    print("âœ… DOCX æ”¯æŒå·²å•Ÿç”¨ (docx2txt)")
except ImportError:
    try:
        from docx import Document as DocxDocument
        DOCX2TXT_AVAILABLE = True
        DOCX_METHOD = "python-docx"
        print("âœ… DOCX æ”¯æŒå·²å•Ÿç”¨ (python-docx)")
    except ImportError:
        DOCX2TXT_AVAILABLE = False
        DOCX_METHOD = None
        print("âš ï¸ è­¦å‘Š: docx2txt æˆ– python-docx æœªå®‰è£")

try:
    import ebooklib
    from ebooklib import epub
    from bs4 import BeautifulSoup
    EPUB_AVAILABLE = True
    print("âœ… EPUB æ”¯æŒå·²å•Ÿç”¨")
except ImportError:
    EPUB_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: ebooklib æˆ– beautifulsoup4 æœªå®‰è£")

@dataclass
class FileInfo:
    """æ–‡ä»¶è³‡è¨Š"""
    path: str
    size: int
    mtime: float
    hash: str
    encoding: str = "utf-8"
    file_type: str = ""

@dataclass
class TextAnalysis:
    """æ–‡æœ¬åˆ†æçµæœ"""
    length: int
    text_type: str
    language: str
    encoding: str
    structure_info: Dict
    quality_score: float
    processing_strategy: str

@dataclass
class ChunkInfo:
    """åˆ†å¡Šè³‡è¨Š"""
    chunk_id: str
    content: str
    metadata: Dict
    token_count: int
    quality_score: float
    relationships: List[str]  # èˆ‡å…¶ä»–åˆ†å¡Šçš„é—œä¿‚

@dataclass
class SearchResult:
    content: str
    score: float
    metadata: Dict
    collection: str
    chunk_info: Optional[ChunkInfo] = None

# --- End of content from core_config.py ---


# --- Start of content from text_processing.py ---

logger = logging.getLogger(__name__)

class AdvancedTokenEstimator:
    """ğŸ”§ é«˜ç´š Token ä¼°ç®—å™¨"""
    
    def __init__(self):
        # ä¸åŒèªè¨€å’Œå…§å®¹é¡å‹çš„ Token ä¿‚æ•¸
        self.token_ratios = {
            "chinese": 2.5,      # ä¸­æ–‡å­—ç¬¦/token
            "english": 4.0,      # è‹±æ–‡å­—ç¬¦/token
            "mixed": 3.0,        # ä¸­è‹±æ··åˆ
            "code": 3.5,         # ç¨‹å¼ç¢¼
            "punctuation": 1.0,  # æ¨™é»ç¬¦è™Ÿ
            "numbers": 2.0       # æ•¸å­—
        }
        self.safety_margin = TOKEN_LIMITS.get("token_safety_margin", 0.15)
    
    def analyze_text_composition(self, text: str) -> Dict[str, int]:
        """åˆ†ææ–‡æœ¬çµ„æˆ"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        numbers = len(re.findall(r'\d', text))
        punctuation = len(re.findall(r'[^\w\s\u4e00-\u9fff]', text))
        other_chars = len(text) - chinese_chars - english_chars - numbers - punctuation
        
        return {
            "chinese": chinese_chars,
            "english": english_chars,
            "numbers": numbers,
            "punctuation": punctuation,
            "other": other_chars,
            "total": len(text)
        }
    
    def estimate_tokens(self, text: str, content_type: str = "mixed") -> int:
        """ç²¾ç¢ºä¼°ç®— Token æ•¸é‡"""
        if not text:
            return 0
        
        composition = self.analyze_text_composition(text)
        
        # æ ¹æ“šæ–‡æœ¬çµ„æˆè¨ˆç®— token
        estimated_tokens = (
            composition["chinese"] / self.token_ratios["chinese"] +
            composition["english"] / self.token_ratios["english"] +
            composition["numbers"] / self.token_ratios["numbers"] +
            composition["punctuation"] / self.token_ratios["punctuation"] +
            composition["other"] / self.token_ratios["mixed"]
        )
        
        # æ ¹æ“šå…§å®¹é¡å‹èª¿æ•´
        if content_type == "code":
            estimated_tokens *= 1.2  # ç¨‹å¼ç¢¼é€šå¸¸ token å¯†åº¦æ›´é«˜
        elif content_type == "academic":
            estimated_tokens *= 1.1  # å­¸è¡“æ–‡æœ¬å°ˆæ¥­è©å½™å¤š
        
        # åŠ ä¸Šå®‰å…¨é‚Šéš›
        final_estimate = int(estimated_tokens * (1 + self.safety_margin))
        
        return max(final_estimate, 1)  # è‡³å°‘1å€‹token
    
    def estimate_embedding_cost(self, total_tokens: int, model: str = "text-embedding-3-small") -> float:
        """ä¼°ç®— Embedding æˆæœ¬"""
        cost_per_1k_tokens = {
            "text-embedding-3-small": 0.00002,
            "text-embedding-3-large": 0.00013,
            "text-embedding-ada-002": 0.0001
        }
        
        rate = cost_per_1k_tokens.get(model, 0.00002)
        return (total_tokens / 1000) * rate

class ChineseTextNormalizer:
    """å„ªåŒ–ç‰ˆä¸­æ–‡æ–‡æœ¬æ¨™æº–åŒ–è™•ç†å™¨"""
    
    def __init__(self):
        self.s2t_converter = None
        self.t2s_converter = None
        
        if OPENCC_AVAILABLE:
            try:
                self.s2t_converter = opencc.OpenCC('s2t')
                self.t2s_converter = opencc.OpenCC('t2s')
                print("ğŸ”¤ ç¹ç°¡è½‰æ›å™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.warning(f"OpenCC åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # ä¸­æ–‡æ–‡æœ¬æ­£è¦åŒ–è¦å‰‡
        self.normalization_rules = [
            (r'[\u3000]+', ' '),
            (r'\r\n|\r', '\n'),                       # çµ±ä¸€æ›è¡Œ
            (r'\n{3,}', '\n\n'),                      # é™åˆ¶é€£çºŒæ›è¡Œ
            (r'[\u201C\u201D\u2018\u2019\u201E\u201A\u2033\u2032]', '"'),  # çµ±ä¸€å¼•è™Ÿ
            (r'[â€”â€”â€“âˆ¶]', '-'),                          # çµ±ä¸€ç ´æŠ˜è™Ÿ
            (r'[â€¦â‹¯]', '...'),                         # çµ±ä¸€çœç•¥è™Ÿ
        ]
    
    def normalize_text(self, text: str) -> Tuple[str, Dict]:
        """æ¨™æº–åŒ–æ–‡æœ¬ä¸¦è¿”å›è™•ç†è³‡è¨Š"""
        if not text:
            return "", {}
        
        original_length = len(text)
        processed_text = text
        
        # æ‡‰ç”¨æ­£è¦åŒ–è¦å‰‡
        for pattern, replacement in self.normalization_rules:
            processed_text = re.sub(pattern, replacement, processed_text)
        
        # ç§»é™¤é¦–å°¾ç©ºç™½
        processed_text = processed_text.strip()
        
        # æª¢æ¸¬å’Œè½‰æ›ç¹ç°¡
        variant, confidence = self.detect_chinese_variant(processed_text)
        if variant == 'simplified' and self.s2t_converter:
            try:
                processed_text = self.s2t_converter.convert(processed_text)
                variant = 'traditional_converted'
            except Exception as e:
                logger.warning(f"ç¹ç°¡è½‰æ›å¤±æ•—: {e}")
        
        processing_info = {
            'original_length': original_length,
            'processed_length': len(processed_text),
            'variant': variant,
            'confidence': confidence,
            'normalized': original_length != len(processed_text)
        }
        
        return processed_text, processing_info
    
    def detect_chinese_variant(self, text: str) -> Tuple[str, float]:
        """æª¢æ¸¬ç¹é«”æˆ–ç°¡é«”ä¸­æ–‡"""
        if not text:
            return 'unknown', 0.0
        
        simplified_chars = set('å›½å‘ä¼šå­¦ä¹ è®ºé—®é¢˜ä¸šä¸“é•¿æ—¶é—´ç»æµ')
        traditional_chars = set('åœ‹ç™¼æœƒå­¸ç¿’è«–å•é¡Œæ¥­å°ˆé•·æ™‚é–“ç¶“æ¿Ÿ')
        
        simplified_count = sum(1 for char in text if char in simplified_chars)
        traditional_count = sum(1 for char in text if char in traditional_chars)
        total_chinese = len(re.findall(r'[\u4e00-\u9fff]', text))
        
        if total_chinese == 0:
            return 'unknown', 0.0
        
        simplified_ratio = simplified_count / total_chinese
        traditional_ratio = traditional_count / total_chinese
        
        if simplified_ratio > traditional_ratio:
            return 'simplified', simplified_ratio
        elif traditional_ratio > simplified_ratio:
            return 'traditional', traditional_ratio
        else:
            return 'mixed', max(simplified_ratio, traditional_ratio)
    
    def create_search_variants(self, query: str) -> List[str]:
        """å‰µå»ºç¹ç°¡æœç´¢è®Šé«”"""
        variants = [query]
        
        if not self.s2t_converter or not self.t2s_converter:
            return variants
        
        try:
            traditional = self.s2t_converter.convert(query)
            if traditional != query:
                variants.append(traditional)
            
            simplified = self.t2s_converter.convert(query)
            if simplified != query:
                variants.append(simplified)
        except Exception as e:
            logger.warning(f"å‰µå»ºæœç´¢è®Šé«”å¤±æ•—: {e}")
        
        return list(set(variants))

class EpubProcessor:
    """ğŸ“š EPUB æ–‡ä»¶è™•ç†å™¨"""
    
    def __init__(self):
        self.normalizer = ChineseTextNormalizer()
        
    def extract_epub_content(self, file_path: Path) -> str:
        """æå– EPUB å…§å®¹"""
        try:
            if not EPUB_AVAILABLE:
                raise ImportError("EPUB è™•ç†åº«æœªå®‰è£")
            
            print(f"ğŸ“š æ­£åœ¨è™•ç† EPUB æ–‡ä»¶: {file_path.name}")
            
            # è®€å– EPUB æ–‡ä»¶
            book = epub.read_epub(str(file_path))
            
            # æå–æ‰€æœ‰æ–‡æœ¬å…§å®¹
            content_parts = []
            chapter_count = 0
            
            # ç²å–æ›¸ç±ä¿¡æ¯
            title = book.get_metadata('DC', 'title')
            author = book.get_metadata('DC', 'creator')
            
            book_info = []
            if title:
                book_info.append(f"æ›¸å: {title[0][0] if title else 'æœªçŸ¥'}")
            if author:
                book_info.append(f"ä½œè€…: {author[0][0] if author else 'æœªçŸ¥'}")
            
            if book_info:
                content_parts.append("\n".join(book_info) + "\n\n")
            
            # æŒ‰é †åºè™•ç†ç« ç¯€
            spine_items = book.get_items_of_type(ebooklib.ITEM_DOCUMENT)
            
            for item in spine_items:
                try:
                    # è§£æ HTML å…§å®¹
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    
                    # ç§»é™¤è…³æœ¬å’Œæ¨£å¼
                    for script in soup(["script", "style"]):
                        script.decompose()
                    
                    # æå–æ–‡æœ¬
                    text = soup.get_text()
                    
                    if text and len(text.strip()) > 50:  # éæ¿¾å¤ªçŸ­çš„å…§å®¹
                        # æ¸…ç†æ–‡æœ¬
                        cleaned_text = self._clean_epub_text(text)
                        
                        if cleaned_text.strip():
                            chapter_count += 1
                            
                            # æ·»åŠ ç« ç¯€æ¨™è¨˜
                            chapter_title = self._extract_chapter_title(soup, cleaned_text)
                            if chapter_title:
                                content_parts.append(f"\n\n=== {chapter_title} ===\n")
                            else:
                                content_parts.append(f"\n\n=== ç¬¬ {chapter_count} ç«  ===\n")
                            
                            content_parts.append(cleaned_text)
                            
                except Exception as e:
                    print(f"   âš ï¸ ç« ç¯€è™•ç†å¤±æ•—: {e}")
                    continue
            
            if not content_parts:
                raise ValueError("EPUB æ–‡ä»¶ä¸­æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆå…§å®¹")
            
            full_content = "".join(content_parts)
            
            print(f"   âœ… EPUB è™•ç†å®Œæˆ: {chapter_count} å€‹ç« ç¯€, {len(full_content):,} å­—ç¬¦")
            
            return full_content
            
        except Exception as e:
            print(f"   âŒ EPUB è™•ç†å¤±æ•—: {e}")
            raise
    
    def _clean_epub_text(self, text: str) -> str:
        """æ¸…ç† EPUB æ–‡æœ¬"""
        if not text:
            return ""
        
        # æ¨™æº–åŒ–æ–‡æœ¬
        normalized_text, _ = self.normalizer.normalize_text(text)
        
        # EPUB ç‰¹å®šæ¸…ç†
        epub_cleaning_rules = [
            (r'\n{4,}', '\n\n'),                    # é™åˆ¶é€£çºŒæ›è¡Œ
            (r'[ \t]{3,}', ' ' ),                    # é™åˆ¶é€£çºŒç©ºæ ¼
            (r'^[ \t]+', '', re.MULTILINE),          # ç§»é™¤è¡Œé¦–ç©ºç™½
            (r'[ \t]+$', '', re.MULTILINE),          # ç§»é™¤è¡Œå°¾ç©ºç™½
            (r'\n[ \t]*\n', '\n\n'),                # æ¸…ç†ç©ºè¡Œ
            (r'[\x00-\x08\x0B\x0C\x0E-\x1F]', ''),  # ç§»é™¤æ§åˆ¶å­—ç¬¦
        ]
        
        cleaned_text = normalized_text
        for pattern, replacement, *flags in epub_cleaning_rules:
            flag = flags[0] if flags else 0
            cleaned_text = re.sub(pattern, replacement, cleaned_text, flags=flag)
        
        return cleaned_text.strip()
    
    def _extract_chapter_title(self, soup, text: str) -> Optional[str]:
        """å˜—è©¦æå–ç« ç¯€æ¨™é¡Œ"""
        try:
            # å¾ HTML æ¨™ç±¤ä¸­æŸ¥æ‰¾æ¨™é¡Œ
            for tag in ['h1', 'h2', 'h3', 'title']:
                title_element = soup.find(tag)
                if title_element and title_element.get_text().strip():
                    title = title_element.get_text().strip()
                    if 5 <= len(title) <= 100:  # åˆç†çš„æ¨™é¡Œé•·åº¦
                        return title
            
            # å¾æ–‡æœ¬é–‹é ­æŸ¥æ‰¾æ¨™é¡Œ
            lines = text.split('\n')[:5]  # æª¢æŸ¥å‰5è¡Œ
            for line in lines:
                line = line.strip()
                if line and 5 <= len(line) <= 100:
                    # æª¢æŸ¥æ˜¯å¦åŒ…å«ç« ç¯€é—œéµè©
                    chapter_keywords = ['ç« ', 'Chapter', 'ç¬¬', 'å·', 'Part']
                    if any(keyword in line for keyword in chapter_keywords):
                        return line
                    # æˆ–è€…æ˜¯çŸ­è¡Œä¸”çœ‹èµ·ä¾†åƒæ¨™é¡Œ
                    elif len(line) <= 50 and not line.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
                        return line
            
            return None
            
        except Exception:
            return None

class SmartTextAnalyzer:
    """ğŸ§  æ™ºèƒ½æ–‡æœ¬åˆ†æå™¨"""
    
    def __init__(self):
        self.normalizer = ChineseTextNormalizer()
        self.token_estimator = AdvancedTokenEstimator()
        
    def analyze_text(self, text: str, source_info: Dict = None) -> TextAnalysis:
        """ç¶œåˆåˆ†ææ–‡æœ¬"""
        if not text:
            return TextAnalysis(
                length=0, text_type="empty", language="unknown",
                encoding="utf-8", structure_info={}, quality_score=0.0,
                processing_strategy="skip"
            )
        
        # æ¨™æº–åŒ–æ–‡æœ¬
        normalized_text, norm_info = self.normalizer.normalize_text(text)
        length = len(normalized_text)
        
        # åˆ†é¡æ–‡æœ¬é•·åº¦
        text_type = self._classify_text_length(length)
        
        # åˆ†ææ–‡æœ¬çµæ§‹
        structure_info = self._analyze_structure(normalized_text)
        
        # æª¢æ¸¬èªè¨€
        language = self._detect_language(normalized_text)
        
        # è©•ä¼°è³ªé‡
        quality_score = self._evaluate_quality(normalized_text, structure_info)
        
        # æ±ºå®šè™•ç†ç­–ç•¥
        processing_strategy = self._determine_strategy(text_type, structure_info, quality_score)
        
        return TextAnalysis(
            length=length,
            text_type=text_type,
            language=language,
            encoding=norm_info.get('encoding', 'utf-8'),
            structure_info=structure_info,
            quality_score=quality_score,
            processing_strategy=processing_strategy
        )
    
    def _classify_text_length(self, length: int) -> str:
        """åˆ†é¡æ–‡æœ¬é•·åº¦"""
        config = SMART_TEXT_CONFIG
        
        if length < config["micro_text_threshold"]:
            return "micro_text"
        elif length < config["short_text_threshold"]:
            return "short_text"
        elif length < config["medium_text_threshold"]:
            return "medium_text"
        elif length < config["long_text_threshold"]:
            return "long_text"
        elif length < config["ultra_long_threshold"]:
            return "ultra_long"
        else:
            return "mega_text"  # è¶…å¤§æ–‡æœ¬
    
    def _analyze_structure(self, text: str) -> Dict:
        """åˆ†ææ–‡æœ¬çµæ§‹"""
        structure_info = {
            'paragraphs': len(text.split('\n\n')),
            'lines': len(text.split('\n')),
            'sentences': len(re.findall(r'[ã€‚ï¼ï¼Ÿ.!?]+', text)),
            'has_chapters': False,
            'has_sections': False,
            'has_lists': False,
            'has_tables': False,
            'chapter_count': 0,
            'section_count': 0
        }
        
        # æª¢æ¸¬ç« ç¯€çµæ§‹
        chapter_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ',
            r'Chapter\s+\d+',
            r'\d+\.\s*[^\n]{1,50}\n'
        ]
        
        for pattern in chapter_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                structure_info['has_chapters'] = True
                structure_info['chapter_count'] = len(matches)
                break
        
        # æª¢æ¸¬å°ç¯€çµæ§‹
        section_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç¯€',
            r'\d+\.\d+',
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€'
        ]
        
        for pattern in section_patterns:
            matches = re.findall(pattern, text)
            if matches:
                structure_info['has_sections'] = True
                structure_info['section_count'] = len(matches)
                break
        
        # æª¢æ¸¬åˆ—è¡¨
        if re.search(r'^\s*[â€¢\-*]\s+', text, re.MULTILINE) or \
           re.search(r'^\s*\d+\.\s+', text, re.MULTILINE):
            structure_info['has_lists'] = True
        
        # æª¢æ¸¬è¡¨æ ¼
        if '|' in text and text.count('|') > 4:
            structure_info['has_tables'] = True
        
        return structure_info
    
    def _detect_language(self, text: str) -> str:
        """æª¢æ¸¬æ–‡æœ¬ä¸»è¦èªè¨€"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_chars = len(text)
        
        if total_chars == 0:
            return "unknown"
        
        chinese_ratio = chinese_chars / total_chars
        english_ratio = english_chars / total_chars
        
        if chinese_ratio > 0.3:
            if english_ratio > 0.2:
                return "mixed_zh_en"
            else:
                return "chinese"
        elif english_ratio > 0.5:
            return "english"
        else:
            return "mixed"
    
    def _evaluate_quality(self, text: str, structure_info: Dict) -> float:
        """è©•ä¼°æ–‡æœ¬è³ªé‡ï¼ˆ0-1åˆ†æ•¸ï¼‰"""
        score = 0.5  # åŸºç¤åˆ†æ•¸
        
        # é•·åº¦åˆç†æ€§ï¼ˆå¤ªçŸ­æˆ–å¤ªé•·éƒ½æ‰£åˆ†ï¼‰
        length = len(text)
        if 50 <= length <= 10000:
            score += 0.2
        elif length < 20:
            score -= 0.3
        
        # çµæ§‹å®Œæ•´æ€§
        if structure_info['paragraphs'] > 1:
            score += 0.1
        if structure_info['sentences'] > 2:
            score += 0.1
        
        # å…§å®¹è±å¯Œåº¦
        unique_chars = len(set(text))
        if unique_chars > 20:
            score += 0.1
        
        # é¿å…é‡è¤‡å…§å®¹
        lines = text.split('\n')
        unique_lines = len(set(lines))
        if len(lines) > 0:
            uniqueness_ratio = unique_lines / len(lines)
            score += uniqueness_ratio * 0.1
        
        return min(max(score, 0.0), 1.0)
    
    def _determine_strategy(self, text_type: str, structure_info: Dict, quality_score: float) -> str:
        """æ±ºå®šè™•ç†ç­–ç•¥"""
        if quality_score < 0.3:
            return "low_quality_skip"
        
        if text_type in ["micro_text", "short_text"]:
            return "whole_document"
        elif text_type == "medium_text":
            if structure_info['has_chapters'] or structure_info['paragraphs'] > 5:
                return "paragraph_aware"
            else:
                return "simple_split"
        elif text_type == "long_text":
            if structure_info['has_chapters']:
                return "chapter_aware"
            elif structure_info['has_sections']:
                return "section_aware"
            else:
                return "fine_split"
        elif text_type in ["ultra_long", "mega_text"]:
            return "hierarchical_split"
        else:
            # å›é€€ç­–ç•¥
            return "hierarchical_split"

class OptimizedTextSplitter:
    """ğŸ”§ å„ªåŒ–ç‰ˆæ–‡æœ¬åˆ†å‰²å™¨"""
    
    def __init__(self):
        self.analyzer = SmartTextAnalyzer()
        self.token_estimator = AdvancedTokenEstimator()
        self.config = SMART_TEXT_CONFIG
        
        # åˆ†å‰²å™¨å¯¦ä¾‹æ± 
        self._splitter_cache = {}
        
        print("ğŸ”§ å„ªåŒ–ç‰ˆæ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ”¯æŒ {len(self.config)} ç¨®æ–‡æœ¬é¡å‹")
        print(f"   æ™ºèƒ½ç­–ç•¥é¸æ“‡")
        print(f"   æ€§èƒ½å„ªåŒ–")
    
    def get_splitter(self, chunk_size: int, chunk_overlap: int) -> RecursiveCharacterTextSplitter:
        """ç²å–åˆ†å‰²å™¨å¯¦ä¾‹ï¼ˆå¸¶å¿«å–ï¼‰"""
        cache_key = f"{chunk_size}_{chunk_overlap}"
        
        if cache_key not in self._splitter_cache:
            self._splitter_cache[cache_key] = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=[
                    "\n\nç¬¬", "\nç¬¬",      # ç« ç¯€æ¨™é¡Œ
                    "\n\n", "\n",          # æ®µè½
                    "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›",  # å¥å­çµæŸ
                    "ï¼Œ", "ã€",            # çŸ­èªåˆ†éš”
                    " ", ""
                ],
                keep_separator=True,
                length_function=len
            )
        
        return self._splitter_cache[cache_key]
    
    def smart_split_documents(self, text: str, doc_id: str, source_info: Dict = None) -> List[Document]:
        """ğŸ§  æ™ºèƒ½æ–‡æª”åˆ†å‰²ä¸»å…¥å£"""
        if not text or not text.strip():
            return []
        
        # åˆ†ææ–‡æœ¬
        analysis = self.analyzer.analyze_text(text, source_info)
        
        print(f"ğŸ“Š æ–‡æœ¬åˆ†æ: {analysis.text_type} ({analysis.length:,} å­—ç¬¦)")
        print(f"   è™•ç†ç­–ç•¥: {analysis.processing_strategy}")
        print(f"   è³ªé‡åˆ†æ•¸: {analysis.quality_score:.2f}")
        print(f"   ä¸»è¦èªè¨€: {analysis.language}")
        
        # æ ¹æ“šç­–ç•¥é¸æ“‡è™•ç†æ–¹æ³•
        documents = []
        if analysis.processing_strategy == "low_quality_skip":
            print("   è·³éè™•ç†")
            return []
        elif analysis.processing_strategy == "whole_document":
            documents = self._process_whole_document(text, doc_id, analysis)
        elif analysis.processing_strategy == "paragraph_aware":
            documents = self._process_paragraph_aware(text, doc_id, analysis)
        elif analysis.processing_strategy == "simple_split":
            documents = self._process_simple_split(text, doc_id, analysis)
        elif analysis.processing_strategy == "chapter_aware":
            documents = self._process_chapter_aware(text, doc_id, analysis)
        elif analysis.processing_strategy == "section_aware":
            documents = self._process_section_aware(text, doc_id, analysis)
        elif analysis.processing_strategy == "fine_split":
            documents = self._process_fine_split(text, doc_id, analysis)
        elif analysis.processing_strategy == "hierarchical_split":
            documents = self._process_hierarchical_split(text, doc_id, analysis)
        else:
            # å›é€€åˆ°ç°¡å–®åˆ†å‰²
            documents = self._process_simple_split(text, doc_id, analysis)

        # æœ€çµ‚é©—è­‰å’Œéæ¿¾ï¼Œç¢ºä¿æ²’æœ‰ç©ºå…§å®¹çš„æ–‡æª”
        final_documents = [doc for doc in documents if doc.page_content and doc.page_content.strip()]
        if len(final_documents) != len(documents):
            logger.warning(f"éæ¿¾æ‰ {len(documents) - len(final_documents)} å€‹ç©ºå…§å®¹çš„å€«å¡Š")
        
        return final_documents
    
    def _process_whole_document(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """è™•ç†æ•´å€‹æ–‡æª”ï¼ˆä¸åˆ†å‰²ï¼‰"""
        config = self.config[analysis.text_type]
        
        if len(text.strip()) < config.get("min_length", 20):
            print("   æ–‡æª”éçŸ­ï¼Œè·³éè™•ç†")
            return []
        
        print(f"   {config['description']}")
        
        # è¨ˆç®— token æ•¸
        token_count = self.token_estimator.estimate_tokens(text)
        
        metadata = {
            'doc_id': doc_id,
            'chunk_id': f"{doc_id}_whole",
            'chunk_index': 0,
            'total_chunks': 1,
            'text_type': analysis.text_type,
            'processing_strategy': analysis.processing_strategy,
            'is_complete_document': True,
            'token_count': token_count,
            'quality_score': analysis.quality_score,
            'language': analysis.language,
            'structure_info': analysis.structure_info
        }
        
        return [Document(page_content=text, metadata=metadata)]
    
    def _process_paragraph_aware(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """æ®µè½æ„ŸçŸ¥è™•ç†"""
        config = self.config[analysis.text_type]
        chunk_size = config["chunk_size"]
        chunk_overlap = config["chunk_overlap"]
        
        print(f"   {config['description']} (ç›®æ¨™å¤§å°: {chunk_size})")
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        if not paragraphs:
            return self._process_simple_split(text, doc_id, analysis)
        
        documents = []
        current_chunk = ""
        chunk_index = 0
        
        for para_idx, paragraph in enumerate(paragraphs):
            # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŠ å…¥ç•¶å‰åˆ†å¡Š
            potential_chunk = current_chunk + ("\n\n" if current_chunk else "") + paragraph
            
            if len(potential_chunk) <= chunk_size or not current_chunk:
                current_chunk = potential_chunk
            else:
                # ç•¶å‰åˆ†å¡Šå·²æ»¿ï¼Œå‰µå»ºæ–‡æª”
                if current_chunk:
                    doc = self._create_chunk_document(
                        current_chunk, doc_id, chunk_index, analysis, "paragraph_aware"
                    )
                    documents.append(doc)
                    chunk_index += 1
                
                current_chunk = paragraph
        
        # è™•ç†æœ€å¾Œä¸€å€‹åˆ†å¡Š
        if current_chunk:
            doc = self._create_chunk_document(
                current_chunk, doc_id, chunk_index, analysis, "paragraph_aware"
            )
            documents.append(doc)
        
        # è™•ç†é‡ç–Š
        if chunk_overlap > 0 and len(documents) > 1:
            documents = self._add_overlap_to_documents(documents, chunk_overlap)
        
        return documents
    
    def _process_simple_split(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """ç°¡å–®åˆ†å‰²è™•ç†"""
        config = self.config[analysis.text_type]
        chunk_size = config["chunk_size"]
        chunk_overlap = config["chunk_overlap"]
        
        print(f"   {config['description']} (å¤§å°: {chunk_size}, é‡ç–Š: {chunk_overlap})")
        
        splitter = self.get_splitter(chunk_size, chunk_overlap)
        chunks = splitter.split_text(text)
        
        documents = []
        for i, chunk in enumerate(chunks):
            if len(chunk.strip()) >= config.get("min_length", 20):
                doc = self._create_chunk_document(
                    chunk, doc_id, i, analysis, "simple_split"
                )
                documents.append(doc)
        
        return documents
    
    def _process_chapter_aware(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """ç« ç¯€æ„ŸçŸ¥è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   {config['description']} - æª¢æ¸¬åˆ° {analysis.structure_info.get('chapter_count', 0)} å€‹ç« ç¯€")
        
        # æŒ‰ç« ç¯€åˆ†å‰²
        chapters = self._split_by_chapters(text)
        
        if len(chapters) <= 1:
            # æ²’æœ‰æª¢æ¸¬åˆ°ç« ç¯€ï¼Œå›é€€åˆ°æ®µè½æ„ŸçŸ¥
            return self._process_paragraph_aware(text, doc_id, analysis)
        
        all_documents = []
        
        for chapter_idx, chapter_text in enumerate(chapters):
            chapter_doc_id = f"{doc_id}_ch{chapter_idx+1:02d}"
            
            # æ¯å€‹ç« ç¯€å…§éƒ¨ä½¿ç”¨æ®µè½æ„ŸçŸ¥åˆ†å‰²
            chapter_analysis = analysis  # ä½¿ç”¨ç›¸åŒçš„åˆ†æçµæœ
            chapter_docs = self._process_paragraph_aware(chapter_text, chapter_doc_id, chapter_analysis)
            
            # æ·»åŠ ç« ç¯€ä¿¡æ¯åˆ°å…ƒæ•¸æ“š
            for doc in chapter_docs:
                doc.metadata.update({
                    'chapter_index': chapter_idx,
                    'chapter_total': len(chapters),
                    'parent_doc_id': doc_id,
                    'split_method': 'chapter_aware'
                })
            
            all_documents.extend(chapter_docs)
        
        return all_documents
    
    def _process_section_aware(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """å°ç¯€æ„ŸçŸ¥è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   {config['description']} - æª¢æ¸¬åˆ° {analysis.structure_info.get('section_count', 0)} å€‹å°ç¯€")
        
        # æŒ‰å°ç¯€åˆ†å‰²
        sections = self._split_by_sections(text)
        
        if len(sections) <= 1:
            return self._process_simple_split(text, doc_id, analysis)
        
        all_documents = []
        
        for section_idx, section_text in enumerate(sections):
            section_doc_id = f"{doc_id}_sec{section_idx+1:02d}"
            
            # å°ç¯€å…§éƒ¨ç°¡å–®åˆ†å‰²
            section_docs = self._process_simple_split(section_text, section_doc_id, analysis)
            
            # æ·»åŠ å°ç¯€ä¿¡æ¯
            for doc in section_docs:
                doc.metadata.update({
                    'section_index': section_idx,
                    'section_total': len(sections),
                    'parent_doc_id': doc_id,
                    'split_method': 'section_aware'
                })
            
            all_documents.extend(section_docs)
        
        return all_documents
    
    def _process_fine_split(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """ç²¾ç´°åˆ†å‰²è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   {config['description']}")
        
        return self._process_simple_split(text, doc_id, analysis)
    
    def _process_hierarchical_split(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """éšå±¤å¼åˆ†å‰²è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   {config['description']}")
        
        # ç¬¬ä¸€å±¤ï¼šç« ç¯€åˆ†å‰²
        chapters = self._split_by_chapters(text)
        
        if len(chapters) > 1:
            return self._process_chapter_aware(text, doc_id, analysis)
        
        # ç¬¬äºŒå±¤ï¼šæ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        if len(paragraphs) > 10:  # æ®µè½è¼ƒå¤šï¼Œç”¨æ®µè½æ„ŸçŸ¥
            return self._process_paragraph_aware(text, doc_id, analysis)
        
        # ç¬¬ä¸‰å±¤ï¼šç²¾ç´°åˆ†å‰²
        return self._process_fine_split(text, doc_id, analysis)
    
    def _split_by_chapters(self, text: str) -> List[str]:
        """æŒ‰ç« ç¯€åˆ†å‰²æ–‡æœ¬"""
        chapter_patterns = [
            r'\n(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« [^\n]*)',
            r'\n(Chapter\s+\d+[^\n]*)',
            r'\n(\d+\.\d+[^\n]*)',
            r'\n([A-Z][^\n]{10,50})\n(?=[A-Z]|$)'
        ]
        
        best_split = None
        best_count = 0
        
        for pattern in chapter_patterns:
            matches = list(re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE))
            
            if len(matches) >= 2:
                splits = []
                start = 0
                
                for match in matches:
                    if start < match.start():
                        splits.append(text[start:match.start()].strip())
                    start = match.start()
                
                splits.append(text[start:].strip())
                splits = [s for s in splits if s and len(s) > 50]
                
                if len(splits) > best_count:
                    best_split = splits
                    best_count = len(splits)
        
        return best_split if best_split else [text]
    
    def _split_by_sections(self, text: str) -> List[str]:
        """æŒ‰å°ç¯€åˆ†å‰²æ–‡æœ¬"""
        section_patterns = [
            r'\n(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç¯€[^\n]*)',
            r'\n(\d+\.\d+[^\n]*)',
            r'\n([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€[^\n]*)',
            r'\n([A-Z]\.[^\n]*)'
        ]
        
        for pattern in section_patterns:
            matches = list(re.finditer(pattern, text, re.MULTILINE))
            
            if len(matches) >= 2:
                splits = []
                start = 0
                
                for match in matches:
                    if start < match.start():
                        splits.append(text[start:match.start()].strip())
                    start = match.start()
                
                splits.append(text[start:].strip())
                return [s for s in splits if s and len(s) > 30]
        
        return [text]
    
    def _add_overlap_to_documents(self, documents: List[Document], overlap_size: int) -> List[Document]:
        """ç‚ºæ–‡æª”æ·»åŠ é‡ç–Šéƒ¨åˆ†"""
        if len(documents) <= 1:
            return documents
        
        for i in range(1, len(documents)):
            # å¾å‰ä¸€å€‹æ–‡æª”æœ«å°¾å–é‡ç–Šå…§å®¹
            prev_content = documents[i-1].page_content
            current_content = documents[i].page_content
            
            if len(prev_content) > overlap_size:
                overlap_text = prev_content[-overlap_size:]
                # å°‹æ‰¾å®Œæ•´çš„å¥å­é‚Šç•Œ
                sentence_end = overlap_text.rfind('ã€‚')
                if sentence_end == -1:
                    sentence_end = overlap_text.rfind('.')
                
                if sentence_end > overlap_size // 2:
                    overlap_text = overlap_text[sentence_end+1:]
                
                documents[i].page_content = overlap_text + "\n" + current_content
                documents[i].metadata['has_overlap'] = True
                documents[i].metadata['overlap_length'] = len(overlap_text)
        
        return documents
    
    def _create_chunk_document(self, content: str, doc_id: str, chunk_index: int, 
                      analysis: TextAnalysis, split_method: str) -> Document:
        """å‰µå»ºåˆ†å¡Šæ–‡æ¡£ - ä¿®å¾©ç‰ˆï¼šå®Œæ•´URLå’Œæ¨™é¡Œæå–"""
        token_count = self.token_estimator.estimate_tokens(content)
        
        # å®Œæ•´çš„URLå’Œæ¨™é¡Œæå–#!/usr/bin/env python3
"""
å‘é‡æ“ä½œæ ¸å¿ƒå±¤ - VectorOperationsCore
è·è²¬ï¼šåº•å±¤æ•¸æ“šæ“ä½œã€å‘é‡è™•ç†ã€æ–‡æª”è¼‰å…¥ã€é›†åˆç®¡ç†
åŒ…å«26å€‹åº•å±¤æ ¸å¿ƒæ–¹æ³•ï¼Œå¾åŸ OptimizedVectorSystem ç²¾ç¢ºç§»å‹•è€Œä¾†
"""

import time
import json
import hashlib
import re
import logging
import os
import gc
import threading
import tempfile
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set, Any, Union
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict

# LangChainæ ¸å¿ƒ
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    TextLoader, PyPDFLoader, Docx2txtLoader, 
    CSVLoader, JSONLoader, PandasExcelLoader
)

# --- Start of content from core_config.py ---

# ğŸ†• 1. æ·»åŠ  Railway ç’°å¢ƒæª¢æ¸¬å‡½æ•¸ï¼ˆæ”¾åœ¨æ–‡ä»¶é ‚éƒ¨ï¼Œå°å…¥èªå¥ä¹‹å¾Œï¼‰
def detect_railway_environment():
    """æª¢æ¸¬æ˜¯å¦åœ¨ Railway ç’°å¢ƒä¸­é‹è¡Œ"""
    railway_indicators = [
        os.getenv("RAILWAY_PROJECT_ID"),
        os.getenv("RAILWAY_SERVICE_ID"), 
        os.getenv("DATABASE_URL"),
        "railway.internal" in os.getenv("POSTGRES_HOST", "")
    ]
    
    is_railway = any(railway_indicators)
    if is_railway:
        print("ğŸš‚ æª¢æ¸¬åˆ° Railway éƒ¨ç½²ç’°å¢ƒ")
        # é¡¯ç¤º Railway ç‰¹å®šä¿¡æ¯
        project_id = os.getenv("RAILWAY_PROJECT_ID", "unknown")
        service_id = os.getenv("RAILWAY_SERVICE_ID", "unknown")  
        print(f"   é …ç›®ID: {project_id[:8]}***")
        print(f"   æœå‹™ID: {service_id[:8]}***")
    
    return is_railway

# ğŸ”§ å„ªåŒ–ç‰ˆç³»çµ±é…ç½®
SYSTEM_CONFIG = {
    "persist_dir": "./chroma_langchain_db",
    "data_dir": "./data", 
    "device": "cpu",  
    "log_level": "INFO",
    "max_workers": 4,  # ä¸¦ç™¼è™•ç†æ•¸
    "cache_embeddings": True,  # å¿«å–åµŒå…¥å‘é‡
    "backup_enabled": True  # è‡ªå‹•å‚™ä»½
}

# ğŸ§  æ™ºèƒ½æ–‡æœ¬åˆ†é¡é…ç½®
SMART_TEXT_CONFIG = {
    # æ–‡æœ¬é•·åº¦é–¾å€¼ï¼ˆå­—ç¬¦æ•¸ï¼‰
    "micro_text_threshold": 100,      # å¾®æ–‡æœ¬ï¼šæ¨™é¡Œã€æ‘˜è¦ç­‰
    "short_text_threshold": 500,      # çŸ­æ–‡æœ¬ï¼šçŸ­æ–‡ç« ã€æ®µè½
    "medium_text_threshold": 2000,    # ä¸­ç­‰æ–‡æœ¬ï¼šä¸­ç¯‡æ–‡ç« 
    "long_text_threshold": 8000,    # é•·æ–‡æœ¬ï¼šé•·ç¯‡æ–‡ç« 
    "ultra_long_threshold": 20000,    # è¶…é•·æ–‡æœ¬ï¼šæ›¸ç±ç« ç¯€
    
    # å„é¡æ–‡æœ¬çš„è™•ç†ç­–ç•¥
    "micro_text": {
        "min_length": 20,
        "process_whole": True,
        "merge_with_next": True,  # å˜—è©¦èˆ‡ä¸‹ä¸€å€‹ç‰‡æ®µåˆä½µ
        "chunk_size": 0,
        "description": "å¾®æ–‡æœ¬æ•´é«”è™•ç†"
    },
    
    "short_text": {
        "min_length": 50,
        "process_whole": True,
        "preserve_structure": True,
        "chunk_size": 0,
        "allow_merge": True,  # å…è¨±åˆä½µç›¸é„°çŸ­æ–‡æœ¬
        "description": "çŸ­æ–‡æœ¬æ•´é«”ä¿å­˜"
    },
    
    "medium_text": {
        "chunk_size": 800,
        "chunk_overlap": 120,
        "preserve_paragraphs": True,
        "smart_boundaries": True,  # æ™ºèƒ½é‚Šç•Œæª¢æ¸¬
        "min_chunk_ratio": 0.3,   # æœ€å°ç‰‡æ®µæ¯”ä¾‹
        "description": "ä¸­ç­‰æ–‡æœ¬æ®µè½æ„ŸçŸ¥åˆ†å‰²"
    },
    
    "long_text": {
        "chunk_size": 500,
        "chunk_overlap": 80,
        "hierarchical_split": True,
        "section_aware": True,    # ç« ç¯€æ„ŸçŸ¥
        "quality_check": True,    # è³ªé‡æª¢æŸ¥
        "description": "é•·æ–‡æœ¬ç²¾ç´°åŒ–åˆ†å‰²"
    },
    
    "ultra_long": {
        "chunk_size": 600,
        "chunk_overlap": 100,
        "multi_level_split": True,  # å¤šå±¤ç´šåˆ†å‰²
        "chapter_detection": True,  # ç« ç¯€æª¢æ¸¬
        "summary_chunks": True,     # ç”Ÿæˆæ‘˜è¦ç‰‡æ®µ
        "description": "è¶…é•·æ–‡æœ¬éšå±¤å¼è™•ç†"
    },
    
    "mega_text": {
        "chunk_size": 500,
        "chunk_overlap": 80,
        "multi_level_split": True,  # å¤šå±¤ç´šåˆ†å‰²
        "chapter_detection": True,  # ç« ç¯€æª¢æ¸¬
        "aggressive_split": True,   # ç©æ¥µåˆ†å‰²
        "quality_filter": True,     # è³ªé‡éæ¿¾
        "description": "è¶…å¤§æ–‡æœ¬ç©æ¥µåˆ†å‰²è™•ç†"
    }
}

# ğŸ”§ å„ªåŒ–ç‰ˆ Token é™åˆ¶é…ç½®
TOKEN_LIMITS = {
    "max_tokens_per_request": 150000,  # æ›´ä¿å®ˆçš„é™åˆ¶
    "max_batch_size": 8,               # æ¸›å°æ‰¹æ¬¡å¤§å°
    "min_batch_size": 1,
    "batch_delay": 5.0,                # å¢åŠ æ‰¹æ¬¡é–“å»¶é²
    "retry_delay": 10,
    "max_retries": 3,
    "token_safety_margin": 0.2,        # 20% å®‰å…¨é‚Šéš›
    "adaptive_batching": True           # è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°
}

# ğŸ”§ æ€§èƒ½å„ªåŒ–é…ç½®
PERFORMANCE_CONFIG = {
    "embedding_batch_size": 12,
    "parallel_processing": True,
    "memory_limit_mb": 1024,      # è¨˜æ†¶é«”é™åˆ¶
    "chunk_cache_size": 1000,     # åˆ†å¡Šå¿«å–å¤§å°
    "preload_models": True,       # é è¼‰å…¥æ¨¡å‹
    "gc_frequency": 50            # åƒåœ¾å›æ”¶é »ç‡
}

# æ”¯æŒçš„æ–‡ä»¶æ ¼å¼æ“´å±•
SUPPORTED_EXTENSIONS = {
    '.txt', '.md', '.pdf', '.csv', '.json', '.py', '.js', 
    '.docx', '.doc', '.epub', '.rst', '.org', '.xlsx', '.xls'
}

from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from langchain_community.vectorstores import Chroma

try:
    from langchain_community.vectorstores import Chroma
    CHROMA_AVAILABLE = True
except ImportError:
    CHROMA_AVAILABLE = False
    # å‰µå»ºä¸€å€‹è™›æ“¬çš„ Chroma é¡å‹ç”¨æ–¼é¡å‹è¨»è§£
    class Chroma:
        pass

# ğŸ”§ æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
def check_openai_api_key():
    """æª¢æŸ¥ OpenAI API Key"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸ æœªè¨­ç½® OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
        print("   è«‹åœ¨ .env æ–‡ä»¶ä¸­è¨­ç½®: OPENAI_API_KEY=sk-your-api-key")
        return False
    
    if api_key.startswith("sk-") and len(api_key) > 20:
        print("âœ… OpenAI API Key æ ¼å¼æ­£ç¢º")
        print(f"   API Key: {api_key[:8]}***{api_key[-4:]}")
        return True
    else:
        print("âš ï¸ OpenAI API Key æ ¼å¼å¯èƒ½ä¸æ­£ç¢º")
        return False

# LangChain æ ¸å¿ƒçµ„ä»¶å°å…¥
try:
    from langchain_postgres import PGVector
    PGVECTOR_AVAILABLE = True
    print("âœ… PGVector å¯ç”¨")
except ImportError:
    try:
        from langchain_community.vectorstores import PGVector
        PGVECTOR_AVAILABLE = True
        print("âœ… PGVector (community) å¯ç”¨")
    except ImportError:
        PGVECTOR_AVAILABLE = False
        print("âŒ PGVector ä¸å¯ç”¨")
        # å¯ä»¥å›é€€åˆ° Chroma
        from langchain_community.vectorstores import Chroma

# OpenAI embeddings å°å…¥
try:
    from langchain_openai import OpenAIEmbeddings
    OPENAI_EMBEDDINGS_AVAILABLE = True
    print("âœ… OpenAI Embeddings å¯ç”¨")
    
    if check_openai_api_key():
        print("âœ… OpenAI ç’°å¢ƒæª¢æŸ¥é€šé")
    else:
        print("âš ï¸ OpenAI ç’°å¢ƒé…ç½®å¯èƒ½æœ‰å•é¡Œ")
        
except ImportError as e:
    OPENAI_EMBEDDINGS_AVAILABLE = False
    print(f"âš ï¸ OpenAI Embeddings ä¸å¯ç”¨: {e}")

# ğŸ”§ PostgreSQL ä¾è³´æª¢æŸ¥
try:
    import psycopg2
    PSYCOPG2_AVAILABLE = True
except ImportError:
    PSYCOPG2_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: psycopg2 æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install psycopg2-binary")

# ä¸­æ–‡åˆ†è©å’Œè½‰æ›
try:
    import jieba
    jieba.initialize()
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: jieba æœªå®‰è£ï¼Œä¸­æ–‡åˆ†è©åŠŸèƒ½å°‡è¢«ç¦ç”¨")

try:
    import opencc
    OPENCC_AVAILABLE = True
except ImportError:
    OPENCC_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: opencc æœªå®‰è£ï¼Œç¹ç°¡è½‰æ›åŠŸèƒ½å°‡è¢«ç¦ç”¨")

# æ–‡æª”è™•ç†æ”¯æŒ
try:
    import docx2txt
    DOCX2TXT_AVAILABLE = True
    DOCX_METHOD = "docx2txt"
    print("âœ… DOCX æ”¯æŒå·²å•Ÿç”¨ (docx2txt)")
except ImportError:
    try:
        from docx import Document as DocxDocument
        DOCX2TXT_AVAILABLE = True
        DOCX_METHOD = "python-docx"
        print("âœ… DOCX æ”¯æŒå·²å•Ÿç”¨ (python-docx)")
    except ImportError:
        DOCX2TXT_AVAILABLE = False
        DOCX_METHOD = None
        print("âš ï¸ è­¦å‘Š: docx2txt æˆ– python-docx æœªå®‰è£")

try:
    import ebooklib
    from ebooklib import epub
    from bs4 import BeautifulSoup
    EPUB_AVAILABLE = True
    print("âœ… EPUB æ”¯æŒå·²å•Ÿç”¨")
except ImportError:
    EPUB_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: ebooklib æˆ– beautifulsoup4 æœªå®‰è£")

@dataclass
class FileInfo:
    """æ–‡ä»¶è³‡è¨Š"""
    path: str
    size: int
    mtime: float
    hash: str
    encoding: str = "utf-8"
    file_type: str = ""

@dataclass
class TextAnalysis:
    """æ–‡æœ¬åˆ†æçµæœ"""
    length: int
    text_type: str
    language: str
    encoding: str
    structure_info: Dict
    quality_score: float
    processing_strategy: str

@dataclass
class ChunkInfo:
    """åˆ†å¡Šè³‡è¨Š"""
    chunk_id: str
    content: str
    metadata: Dict
    token_count: int
    quality_score: float
    relationships: List[str]  # èˆ‡å…¶ä»–åˆ†å¡Šçš„é—œä¿‚

@dataclass
class SearchResult:
    content: str
    score: float
    metadata: Dict
    collection: str
    chunk_info: Optional[ChunkInfo] = None

# --- End of content from core_config.py ---


# --- Start of content from text_processing.py ---

logger = logging.getLogger(__name__)

class AdvancedTokenEstimator:
    """ğŸ”§ é«˜ç´š Token ä¼°ç®—å™¨"""
    
    def __init__(self):
        # ä¸åŒèªè¨€å’Œå…§å®¹é¡å‹çš„ Token ä¿‚æ•¸
        self.token_ratios = {
            "chinese": 2.5,      # ä¸­æ–‡å­—ç¬¦/token
            "english": 4.0,      # è‹±æ–‡å­—ç¬¦/token
            "mixed": 3.0,        # ä¸­è‹±æ··åˆ
            "code": 3.5,         # ç¨‹å¼ç¢¼
            "punctuation": 1.0,  # æ¨™é»ç¬¦è™Ÿ
            "numbers": 2.0       # æ•¸å­—
        }
        self.safety_margin = TOKEN_LIMITS.get("token_safety_margin", 0.15)
    
    def analyze_text_composition(self, text: str) -> Dict[str, int]:
        """åˆ†ææ–‡æœ¬çµ„æˆ"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        numbers = len(re.findall(r'\d', text))
        punctuation = len(re.findall(r'[^\w\s\u4e00-\u9fff]', text))
        other_chars = len(text) - chinese_chars - english_chars - numbers - punctuation
        
        return {
            "chinese": chinese_chars,
            "english": english_chars,
            "numbers": numbers,
            "punctuation": punctuation,
            "other": other_chars,
            "total": len(text)
        }
    
    def estimate_tokens(self, text: str, content_type: str = "mixed") -> int:
        """ç²¾ç¢ºä¼°ç®— Token æ•¸é‡"""
        if not text:
            return 0
        
        composition = self.analyze_text_composition(text)
        
        # æ ¹æ“šæ–‡æœ¬çµ„æˆè¨ˆç®— token
        estimated_tokens = (
            composition["chinese"] / self.token_ratios["chinese"] +
            composition["english"] / self.token_ratios["english"] +
            composition["numbers"] / self.token_ratios["numbers"] +
            composition["punctuation"] / self.token_ratios["punctuation"] +
            composition["other"] / self.token_ratios["mixed"]
        )
        
        # æ ¹æ“šå…§å®¹é¡å‹èª¿æ•´
        if content_type == "code":
            estimated_tokens *= 1.2  # ç¨‹å¼ç¢¼é€šå¸¸ token å¯†åº¦æ›´é«˜
        elif content_type == "academic":
            estimated_tokens *= 1.1  # å­¸è¡“æ–‡æœ¬å°ˆæ¥­è©å½™å¤š
        
        # åŠ ä¸Šå®‰å…¨é‚Šéš›
        final_estimate = int(estimated_tokens * (1 + self.safety_margin))
        
        return max(final_estimate, 1)  # è‡³å°‘1å€‹token
    
    def estimate_embedding_cost(self, total_tokens: int, model: str = "text-embedding-3-small") -> float:
        """ä¼°ç®— Embedding æˆæœ¬"""
        cost_per_1k_tokens = {
            "text-embedding-3-small": 0.00002,
            "text-embedding-3-large": 0.00013,
            "text-embedding-ada-002": 0.0001
        }
        
        rate = cost_per_1k_tokens.get(model, 0.00002)
        return (total_tokens / 1000) * rate

class ChineseTextNormalizer:
    """å„ªåŒ–ç‰ˆä¸­æ–‡æ–‡æœ¬æ¨™æº–åŒ–è™•ç†å™¨"""
    
    def __init__(self):
        self.s2t_converter = None
        self.t2s_converter = None
        
        if OPENCC_AVAILABLE:
            try:
                self.s2t_converter = opencc.OpenCC('s2t')
                self.t2s_converter = opencc.OpenCC('t2s')
                print("ğŸ”¤ ç¹ç°¡è½‰æ›å™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.warning(f"OpenCC åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # ä¸­æ–‡æ–‡æœ¬æ­£è¦åŒ–è¦å‰‡
        self.normalization_rules = [
            (r'[\u3000]+', ' '),
            (r'\r\n|\r', '\n'),                       # çµ±ä¸€æ›è¡Œ
            (r'\n{3,}', '\n\n'),                      # é™åˆ¶é€£çºŒæ›è¡Œ
            (r'[\u201C\u201D\u2018\u2019\u201E\u201A\u2033\u2032]', '"'),  # çµ±ä¸€å¼•è™Ÿ
            (r'[â€”â€”â€“âˆ¶]', '-'),                          # çµ±ä¸€ç ´æŠ˜è™Ÿ
            (r'[â€¦â‹¯]', '...'),                         # çµ±ä¸€çœç•¥è™Ÿ
        ]
    
    def normalize_text(self, text: str) -> Tuple[str, Dict]:
        """æ¨™æº–åŒ–æ–‡æœ¬ä¸¦è¿”å›è™•ç†è³‡è¨Š"""
        if not text:
            return "", {}
        
        original_length = len(text)
        processed_text = text
        
        # æ‡‰ç”¨æ­£è¦åŒ–è¦å‰‡
        for pattern, replacement in self.normalization_rules:
            processed_text = re.sub(pattern, replacement, processed_text)
        
        # ç§»é™¤é¦–å°¾ç©ºç™½
        processed_text = processed_text.strip()
        
        # æª¢æ¸¬å’Œè½‰æ›ç¹ç°¡
        variant, confidence = self.detect_chinese_variant(processed_text)
        if variant == 'simplified' and self.s2t_converter:
            try:
                processed_text = self.s2t_converter.convert(processed_text)
                variant = 'traditional_converted'
            except Exception as e:
                logger.warning(f"ç¹ç°¡è½‰æ›å¤±æ•—: {e}")
        
        processing_info = {
            'original_length': original_length,
            'processed_length': len(processed_text),
            'variant': variant,
            'confidence': confidence,
            'normalized': original_length != len(processed_text)
        }
        
        return processed_text, processing_info
    
    def detect_chinese_variant(self, text: str) -> Tuple[str, float]:
        """æª¢æ¸¬ç¹é«”æˆ–ç°¡é«”ä¸­æ–‡"""
        if not text:
            return 'unknown', 0.0
        
        simplified_chars = set('å›½å‘ä¼šå­¦ä¹ è®ºé—®é¢˜ä¸šä¸“é•¿æ—¶é—´ç»æµ')
        traditional_chars = set('åœ‹ç™¼æœƒå­¸ç¿’è«–å•é¡Œæ¥­å°ˆé•·æ™‚é–“ç¶“æ¿Ÿ')
        
        simplified_count = sum(1 for char in text if char in simplified_chars)
        traditional_count = sum(1 for char in text if char in traditional_chars)
        total_chinese = len(re.findall(r'[\u4e00-\u9fff]', text))
        
        if total_chinese == 0:
            return 'unknown', 0.0
        
        simplified_ratio = simplified_count / total_chinese
        traditional_ratio = traditional_count / total_chinese
        
        if simplified_ratio > traditional_ratio:
            return 'simplified', simplified_ratio
        elif traditional_ratio > simplified_ratio:
            return 'traditional', traditional_ratio
        else:
            return 'mixed', max(simplified_ratio, traditional_ratio)
    
    def create_search_variants(self, query: str) -> List[str]:
        """å‰µå»ºç¹ç°¡æœç´¢è®Šé«”"""
        variants = [query]
        
        if not self.s2t_converter or not self.t2s_converter:
            return variants
        
        try:
            traditional = self.s2t_converter.convert(query)
            if traditional != query:
                variants.append(traditional)
            
            simplified = self.t2s_converter.convert(query)
            if simplified != query:
                variants.append(simplified)
        except Exception as e:
            logger.warning(f"å‰µå»ºæœç´¢è®Šé«”å¤±æ•—: {e}")
        
        return list(set(variants))

class EpubProcessor:
    """ğŸ“š EPUB æ–‡ä»¶è™•ç†å™¨"""
    
    def __init__(self):
        self.normalizer = ChineseTextNormalizer()
        
    def extract_epub_content(self, file_path: Path) -> str:
        """æå– EPUB å…§å®¹"""
        try:
            if not EPUB_AVAILABLE:
                raise ImportError("EPUB è™•ç†åº«æœªå®‰è£")
            
            print(f"ğŸ“š æ­£åœ¨è™•ç† EPUB æ–‡ä»¶: {file_path.name}")
            
            # è®€å– EPUB æ–‡ä»¶
            book = epub.read_epub(str(file_path))
            
            # æå–æ‰€æœ‰æ–‡æœ¬å…§å®¹
            content_parts = []
            chapter_count = 0
            
            # ç²å–æ›¸ç±ä¿¡æ¯
            title = book.get_metadata('DC', 'title')
            author = book.get_metadata('DC', 'creator')
            
            book_info = []
            if title:
                book_info.append(f"æ›¸å: {title[0][0] if title else 'æœªçŸ¥'}")
            if author:
                book_info.append(f"ä½œè€…: {author[0][0] if author else 'æœªçŸ¥'}")
            
            if book_info:
                content_parts.append("\n".join(book_info) + "\n\n")
            
            # æŒ‰é †åºè™•ç†ç« ç¯€
            spine_items = book.get_items_of_type(ebooklib.ITEM_DOCUMENT)
            
            for item in spine_items:
                try:
                    # è§£æ HTML å…§å®¹
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    
                    # ç§»é™¤è…³æœ¬å’Œæ¨£å¼
                    for script in soup(["script", "style"]):
                        script.decompose()
                    
                    # æå–æ–‡æœ¬
                    text = soup.get_text()
                    
                    if text and len(text.strip()) > 50:  # éæ¿¾å¤ªçŸ­çš„å…§å®¹
                        # æ¸…ç†æ–‡æœ¬
                        cleaned_text = self._clean_epub_text(text)
                        
                        if cleaned_text.strip():
                            chapter_count += 1
                            
                            # æ·»åŠ ç« ç¯€æ¨™è¨˜
                            chapter_title = self._extract_chapter_title(soup, cleaned_text)
                            if chapter_title:
                                content_parts.append(f"\n\n=== {chapter_title} ===\n")
                            else:
                                content_parts.append(f"\n\n=== ç¬¬ {chapter_count} ç«  ===\n")
                            
                            content_parts.append(cleaned_text)
                            
                except Exception as e:
                    print(f"   âš ï¸ ç« ç¯€è™•ç†å¤±æ•—: {e}")
                    continue
            
            if not content_parts:
                raise ValueError("EPUB æ–‡ä»¶ä¸­æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆå…§å®¹")
            
            full_content = "".join(content_parts)
            
            print(f"   âœ… EPUB è™•ç†å®Œæˆ: {chapter_count} å€‹ç« ç¯€, {len(full_content):,} å­—ç¬¦")
            
            return full_content
            
        except Exception as e:
            print(f"   âŒ EPUB è™•ç†å¤±æ•—: {e}")
            raise
    
    def _clean_epub_text(self, text: str) -> str:
        """æ¸…ç† EPUB æ–‡æœ¬"""
        if not text:
            return ""
        
        # æ¨™æº–åŒ–æ–‡æœ¬
        normalized_text, _ = self.normalizer.normalize_text(text)
        
        # EPUB ç‰¹å®šæ¸…ç†
        epub_cleaning_rules = [
            (r'\n{4,}', '\n\n'),                    # é™åˆ¶é€£çºŒæ›è¡Œ
            (r'[ \t]{3,}', ' ' ),                    # é™åˆ¶é€£çºŒç©ºæ ¼
            (r'^[ \t]+', '', re.MULTILINE),          # ç§»é™¤è¡Œé¦–ç©ºç™½
            (r'[ \t]+$', '', re.MULTILINE),          # ç§»é™¤è¡Œå°¾ç©ºç™½
            (r'\n[ \t]*\n', '\n\n'),                # æ¸…ç†ç©ºè¡Œ
            (r'[\x00-\x08\x0B\x0C\x0E-\x1F]', ''),  # ç§»é™¤æ§åˆ¶å­—ç¬¦
        ]
        
        cleaned_text = normalized_text
        for pattern, replacement, *flags in epub_cleaning_rules:
            flag = flags[0] if flags else 0
            cleaned_text = re.sub(pattern, replacement, cleaned_text, flags=flag)
        
        return cleaned_text.strip()
    
    def _extract_chapter_title(self, soup, text: str) -> Optional[str]:
        """å˜—è©¦æå–ç« ç¯€æ¨™é¡Œ"""
        try:
            # å¾ HTML æ¨™ç±¤ä¸­æŸ¥æ‰¾æ¨™é¡Œ
            for tag in ['h1', 'h2', 'h3', 'title']:
                title_element = soup.find(tag)
                if title_element and title_element.get_text().strip():
                    title = title_element.get_text().strip()
                    if 5 <= len(title) <= 100:  # åˆç†çš„æ¨™é¡Œé•·åº¦
                        return title
            
            # å¾æ–‡æœ¬é–‹é ­æŸ¥æ‰¾æ¨™é¡Œ
            lines = text.split('\n')[:5]  # æª¢æŸ¥å‰5è¡Œ
            for line in lines:
                line = line.strip()
                if line and 5 <= len(line) <= 100:
                    # æª¢æŸ¥æ˜¯å¦åŒ…å«ç« ç¯€é—œéµè©
                    chapter_keywords = ['ç« ', 'Chapter', 'ç¬¬', 'å·', 'Part']
                    if any(keyword in line for keyword in chapter_keywords):
                        return line
                    # æˆ–è€…æ˜¯çŸ­è¡Œä¸”çœ‹èµ·ä¾†åƒæ¨™é¡Œ
                    elif len(line) <= 50 and not line.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
                        return line
            
            return None
            
        except Exception:
            return None

class SmartTextAnalyzer:
    """ğŸ§  æ™ºèƒ½æ–‡æœ¬åˆ†æå™¨"""
    
    def __init__(self):
        self.normalizer = ChineseTextNormalizer()
        self.token_estimator = AdvancedTokenEstimator()
        
    def analyze_text(self, text: str, source_info: Dict = None) -> TextAnalysis:
        """ç¶œåˆåˆ†ææ–‡æœ¬"""
        if not text:
            return TextAnalysis(
                length=0, text_type="empty", language="unknown",
                encoding="utf-8", structure_info={}, quality_score=0.0,
                processing_strategy="skip"
            )
        
        # æ¨™æº–åŒ–æ–‡æœ¬
        normalized_text, norm_info = self.normalizer.normalize_text(text)
        length = len(normalized_text)
        
        # åˆ†é¡æ–‡æœ¬é•·åº¦
        text_type = self._classify_text_length(length)
        
        # åˆ†ææ–‡æœ¬çµæ§‹
        structure_info = self._analyze_structure(normalized_text)
        
        # æª¢æ¸¬èªè¨€
        language = self._detect_language(normalized_text)
        
        # è©•ä¼°è³ªé‡
        quality_score = self._evaluate_quality(normalized_text, structure_info)
        
        # æ±ºå®šè™•ç†ç­–ç•¥
        processing_strategy = self._determine_strategy(text_type, structure_info, quality_score)
        
        return TextAnalysis(
            length=length,
            text_type=text_type,
            language=language,
            encoding=norm_info.get('encoding', 'utf-8'),
            structure_info=structure_info,
            quality_score=quality_score,
            processing_strategy=processing_strategy
        )
    
    def _classify_text_length(self, length: int) -> str:
        """åˆ†é¡æ–‡æœ¬é•·åº¦"""
        config = SMART_TEXT_CONFIG
        
        if length < config["micro_text_threshold"]:
            return "micro_text"
        elif length < config["short_text_threshold"]:
            return "short_text"
        elif length < config["medium_text_threshold"]:
            return "medium_text"
        elif length < config["long_text_threshold"]:
            return "long_text"
        elif length < config["ultra_long_threshold"]:
            return "ultra_long"
        else:
            return "mega_text"  # è¶…å¤§æ–‡æœ¬
    
    def _analyze_structure(self, text: str) -> Dict:
        """åˆ†ææ–‡æœ¬çµæ§‹"""
        structure_info = {
            'paragraphs': len(text.split('\n\n')),
            'lines': len(text.split('\n')),
            'sentences': len(re.findall(r'[ã€‚ï¼ï¼Ÿ.!?]+', text)),
            'has_chapters': False,
            'has_sections': False,
            'has_lists': False,
            'has_tables': False,
            'chapter_count': 0,
            'section_count': 0
        }
        
        # æª¢æ¸¬ç« ç¯€çµæ§‹
        chapter_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ',
            r'Chapter\s+\d+',
            r'\d+\.\s*[^\n]{1,50}\n'
        ]
        
        for pattern in chapter_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                structure_info['has_chapters'] = True
                structure_info['chapter_count'] = len(matches)
                break
        
        # æª¢æ¸¬å°ç¯€çµæ§‹
        section_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç¯€',
            r'\d+\.\d+',
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€'
        ]
        
        for pattern in section_patterns:
            matches = re.findall(pattern, text)
            if matches:
                structure_info['has_sections'] = True
                structure_info['section_count'] = len(matches)
                break
        
        # æª¢æ¸¬åˆ—è¡¨
        if re.search(r'^\s*[â€¢\-*]\s+', text, re.MULTILINE) or \
           re.search(r'^\s*\d+\.\s+', text, re.MULTILINE):
            structure_info['has_lists'] = True
        
        # æª¢æ¸¬è¡¨æ ¼
        if '|' in text and text.count('|') > 4:
            structure_info['has_tables'] = True
        
        return structure_info
    
    def _detect_language(self, text: str) -> str:
        """æª¢æ¸¬æ–‡æœ¬ä¸»è¦èªè¨€"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_chars = len(text)
        
        if total_chars == 0:
            return "unknown"
        
        chinese_ratio = chinese_chars / total_chars
        english_ratio = english_chars / total_chars
        
        if chinese_ratio > 0.3:
            if english_ratio > 0.2:
                return "mixed_zh_en"
            else:
                return "chinese"
        elif english_ratio > 0.5:
            return "english"
        else:
            return "mixed"
    
    def _evaluate_quality(self, text: str, structure_info: Dict) -> float:
        """è©•ä¼°æ–‡æœ¬è³ªé‡ï¼ˆ0-1åˆ†æ•¸ï¼‰"""
        score = 0.5  # åŸºç¤åˆ†æ•¸
        
        # é•·åº¦åˆç†æ€§ï¼ˆå¤ªçŸ­æˆ–å¤ªé•·éƒ½æ‰£åˆ†ï¼‰
        length = len(text)
        if 50 <= length <= 10000:
            score += 0.2
        elif length < 20:
            score -= 0.3
        
        # çµæ§‹å®Œæ•´æ€§
        if structure_info['paragraphs'] > 1:
            score += 0.1
        if structure_info['sentences'] > 2:
            score += 0.1
        
        # å…§å®¹è±å¯Œåº¦
        unique_chars = len(set(text))
        if unique_chars > 20:
            score += 0.1
        
        # é¿å…é‡è¤‡å…§å®¹
        lines = text.split('\n')
        unique_lines = len(set(lines))
        if len(lines) > 0:
            uniqueness_ratio = unique_lines / len(lines)
            score += uniqueness_ratio * 0.1
        
        return min(max(score, 0.0), 1.0)
    
    def _determine_strategy(self, text_type: str, structure_info: Dict, quality_score: float) -> str:
        """æ±ºå®šè™•ç†ç­–ç•¥"""
        if quality_score < 0.3:
            return "low_quality_skip"
        
        if text_type in ["micro_text", "short_text"]:
            return "whole_document"
        elif text_type == "medium_text":
            if structure_info['has_chapters'] or structure_info['paragraphs'] > 5:
                return "paragraph_aware"
            else:
                return "simple_split"
        elif text_type == "long_text":
            if structure_info['has_chapters']:
                return "chapter_aware"
            elif structure_info['has_sections']:
                return "section_aware"
            else:
                return "fine_split"
        elif text_type in ["ultra_long", "mega_text"]:
            return "hierarchical_split"
        else:
            # å›é€€ç­–ç•¥
            return "hierarchical_split"

class OptimizedTextSplitter:
    """ğŸ”§ å„ªåŒ–ç‰ˆæ–‡æœ¬åˆ†å‰²å™¨"""
    
    def __init__(self):
        self.analyzer = SmartTextAnalyzer()
        self.token_estimator = AdvancedTokenEstimator()
        self.config = SMART_TEXT_CONFIG
        
        # åˆ†å‰²å™¨å¯¦ä¾‹æ± 
        self._splitter_cache = {}
        
        print("ğŸ”§ å„ªåŒ–ç‰ˆæ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ”¯æŒ {len(self.config)} ç¨®æ–‡æœ¬é¡å‹")
        print(f"   æ™ºèƒ½ç­–ç•¥é¸æ“‡")
        print(f"   æ€§èƒ½å„ªåŒ–")
    
    def get_splitter(self, chunk_size: int, chunk_overlap: int) -> RecursiveCharacterTextSplitter:
        """ç²å–åˆ†å‰²å™¨å¯¦ä¾‹ï¼ˆå¸¶å¿«å–ï¼‰"""
        cache_key = f"{chunk_size}_{chunk_overlap}"
        
        if cache_key not in self._splitter_cache:
            self._splitter_cache[cache_key] = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=[
                    "\n\nç¬¬", "\nç¬¬",      # ç« ç¯€æ¨™é¡Œ
                    "\n\n", "\n",          # æ®µè½
                    "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›",  # å¥å­çµæŸ
                    "ï¼Œ", "ã€",            # çŸ­èªåˆ†éš”
                    " ", ""
                ],
                keep_separator=True,
                length_function=len
            )
        
        return self._splitter_cache[cache_key]
    
    def smart_split_documents(self, text: str, doc_id: str, source_info: Dict = None) -> List[Document]:
        """ğŸ§  æ™ºèƒ½æ–‡æª”åˆ†å‰²ä¸»å…¥å£"""
        if not text or not text.strip():
            return []
        
        # åˆ†ææ–‡æœ¬
        analysis = self.analyzer.analyze_text(text, source_info)
        
        print(f"ğŸ“Š æ–‡æœ¬åˆ†æ: {analysis.text_type} ({analysis.length:,} å­—ç¬¦)")
        print(f"   è™•ç†ç­–ç•¥: {analysis.processing_strategy}")
        print(f"   è³ªé‡åˆ†æ•¸: {analysis.quality_score:.2f}")
        print(f"   ä¸»è¦èªè¨€: {analysis.language}")
        
        # æ ¹æ“šç­–ç•¥é¸æ“‡è™•ç†æ–¹æ³•
        documents = []
        if analysis.processing_strategy == "low_quality_skip":
            print("   è·³éè™•ç†")
            return []
        elif analysis.processing_strategy == "whole_document":
            documents = self._process_whole_document(text, doc_id, analysis)
        elif analysis.processing_strategy == "paragraph_aware":
            documents = self._process_paragraph_aware(text, doc_id, analysis)
        elif analysis.processing_strategy == "simple_split":
            documents = self._process_simple_split(text, doc_id, analysis)
        elif analysis.processing_strategy == "chapter_aware":
            documents = self._process_chapter_aware(text, doc_id, analysis)
        elif analysis.processing_strategy == "section_aware":
            documents = self._process_section_aware(text, doc_id, analysis)
        elif analysis.processing_strategy == "fine_split":
            documents = self._process_fine_split(text, doc_id, analysis)
        elif analysis.processing_strategy == "hierarchical_split":
            documents = self._process_hierarchical_split(text, doc_id, analysis)
        else:
            # å›é€€åˆ°ç°¡å–®åˆ†å‰²
            documents = self._process_simple_split(text, doc_id, analysis)

        # æœ€çµ‚é©—è­‰å’Œéæ¿¾ï¼Œç¢ºä¿æ²’æœ‰ç©ºå…§å®¹çš„æ–‡æª”
        final_documents = [doc for doc in documents if doc.page_content and doc.page_content.strip()]
        if len(final_documents) != len(documents):
            logger.warning(f"éæ¿¾æ‰ {len(documents) - len(final_documents)} å€‹ç©ºå…§å®¹çš„å€«å¡Š")
        
        return final_documents
    
    def _process_whole_document(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """è™•ç†æ•´å€‹æ–‡æª”ï¼ˆä¸åˆ†å‰²ï¼‰"""
        config = self.config[analysis.text_type]
        
        if len(text.strip()) < config.get("min_length", 20):
            print("   æ–‡æª”éçŸ­ï¼Œè·³éè™•ç†")
            return []
        
        print(f"   {config['description']}")
        
        # è¨ˆç®— token æ•¸
        token_count = self.token_estimator.estimate_tokens(text)
        
        metadata = {
            'doc_id': doc_id,
            'chunk_id': f"{doc_id}_whole",
            'chunk_index': 0,
            'total_chunks': 1,
            'text_type': analysis.text_type,
            'processing_strategy': analysis.processing_strategy,
            'is_complete_document': True,
            'token_count': token_count,
            'quality_score': analysis.quality_score,
            'language': analysis.language,
            'structure_info': analysis.structure_info
        }
        
        return [Document(page_content=text, metadata=metadata)]
    
    def _process_paragraph_aware(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """æ®µè½æ„ŸçŸ¥è™•ç†"""
        config = self.config[analysis.text_type]
        chunk_size = config["chunk_size"]
        chunk_overlap = config["chunk_overlap"]
        
        print(f"   {config['description']} (ç›®æ¨™å¤§å°: {chunk_size})")
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        if not paragraphs:
            return self._process_simple_split(text, doc_id, analysis)
        
        documents = []
        current_chunk = ""
        chunk_index = 0
        
        for para_idx, paragraph in enumerate(paragraphs):
            # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŠ å…¥ç•¶å‰åˆ†å¡Š
            potential_chunk = current_chunk + ("\n\n" if current_chunk else "") + paragraph
            
            if len(potential_chunk) <= chunk_size or not current_chunk:
                current_chunk = potential_chunk
            else:
                # ç•¶å‰åˆ†å¡Šå·²æ»¿ï¼Œå‰µå»ºæ–‡æª”
                if current_chunk:
                    doc = self._create_chunk_document(
                        current_chunk, doc_id, chunk_index, analysis, "paragraph_aware"
                    )
                    documents.append(doc)
                    chunk_index += 1
                
                current_chunk = paragraph
        
        # è™•ç†æœ€å¾Œä¸€å€‹åˆ†å¡Š
        if current_chunk:
            doc = self._create_chunk_document(
                current_chunk, doc_id, chunk_index, analysis, "paragraph_aware"
            )
            documents.append(doc)
        
        # è™•ç†é‡ç–Š
        if chunk_overlap > 0 and len(documents) > 1:
            documents = self._add_overlap_to_documents(documents, chunk_overlap)
        
        return documents
    
    def _process_simple_split(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """ç°¡å–®åˆ†å‰²è™•ç†"""
        config = self.config[analysis.text_type]
        chunk_size = config["chunk_size"]
        chunk_overlap = config["chunk_overlap"]
        
        print(f"   {config['description']} (å¤§å°: {chunk_size}, é‡ç–Š: {chunk_overlap})")
        
        splitter = self.get_splitter(chunk_size, chunk_overlap)
        chunks = splitter.split_text(text)
        
        documents = []
        for i, chunk in enumerate(chunks):
            if len(chunk.strip()) >= config.get("min_length", 20):
                doc = self._create_chunk_document(
                    chunk, doc_id, i, analysis, "simple_split"
                )
                documents.append(doc)
        
        return documents
    
    def _process_chapter_aware(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """ç« ç¯€æ„ŸçŸ¥è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   {config['description']} - æª¢æ¸¬åˆ° {analysis.structure_info.get('chapter_count', 0)} å€‹ç« ç¯€")
        
        # æŒ‰ç« ç¯€åˆ†å‰²
        chapters = self._split_by_chapters(text)
        
        if len(chapters) <= 1:
            # æ²’æœ‰æª¢æ¸¬åˆ°ç« ç¯€ï¼Œå›é€€åˆ°æ®µè½æ„ŸçŸ¥
            return self._process_paragraph_aware(text, doc_id, analysis)
        
        all_documents = []
        
        for chapter_idx, chapter_text in enumerate(chapters):
            chapter_doc_id = f"{doc_id}_ch{chapter_idx+1:02d}"
            
            # æ¯å€‹ç« ç¯€å…§éƒ¨ä½¿ç”¨æ®µè½æ„ŸçŸ¥åˆ†å‰²
            chapter_analysis = analysis  # ä½¿ç”¨ç›¸åŒçš„åˆ†æçµæœ
            chapter_docs = self._process_paragraph_aware(chapter_text, chapter_doc_id, chapter_analysis)
            
            # æ·»åŠ ç« ç¯€ä¿¡æ¯åˆ°å…ƒæ•¸æ“š
            for doc in chapter_docs:
                doc.metadata.update({
                    'chapter_index': chapter_idx,
                    'chapter_total': len(chapters),
                    'parent_doc_id': doc_id,
                    'split_method': 'chapter_aware'
                })
            
            all_documents.extend(chapter_docs)
        
        return all_documents
    
    def _process_section_aware(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """å°ç¯€æ„ŸçŸ¥è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   {config['description']} - æª¢æ¸¬åˆ° {analysis.structure_info.get('section_count', 0)} å€‹å°ç¯€")
        
        # æŒ‰å°ç¯€åˆ†å‰²
        sections = self._split_by_sections(text)
        
        if len(sections) <= 1:
            return self._process_simple_split(text, doc_id, analysis)
        
        all_documents = []
        
        for section_idx, section_text in enumerate(sections):
            section_doc_id = f"{doc_id}_sec{section_idx+1:02d}"
            
            # å°ç¯€å…§éƒ¨ç°¡å–®åˆ†å‰²
            section_docs = self._process_simple_split(section_text, section_doc_id, analysis)
            
            # æ·»åŠ å°ç¯€ä¿¡æ¯
            for doc in section_docs:
                doc.metadata.update({
                    'section_index': section_idx,
                    'section_total': len(sections),
                    'parent_doc_id': doc_id,
                    'split_method': 'section_aware'
                })
            
            all_documents.extend(section_docs)
        
        return all_documents
    
    def _process_fine_split(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """ç²¾ç´°åˆ†å‰²è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   {config['description']}")
        
        return self._process_simple_split(text, doc_id, analysis)
    
    def _process_hierarchical_split(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """éšå±¤å¼åˆ†å‰²è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   {config['description']}")
        
        # ç¬¬ä¸€å±¤ï¼šç« ç¯€åˆ†å‰²
        chapters = self._split_by_chapters(text)
        
        if len(chapters) > 1:
            return self._process_chapter_aware(text, doc_id, analysis)
        
        # ç¬¬äºŒå±¤ï¼šæ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        if len(paragraphs) > 10:  # æ®µè½è¼ƒå¤šï¼Œç”¨æ®µè½æ„ŸçŸ¥
            return self._process_paragraph_aware(text, doc_id, analysis)
        
        # ç¬¬ä¸‰å±¤ï¼šç²¾ç´°åˆ†å‰²
        return self._process_fine_split(text, doc_id, analysis)
    
    def _split_by_chapters(self, text: str) -> List[str]:
        """æŒ‰ç« ç¯€åˆ†å‰²æ–‡æœ¬"""
        chapter_patterns = [
            r'\n(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« [^\n]*)',
            r'\n(Chapter\s+\d+[^\n]*)',
            r'\n(\d+\.\d+[^\n]*)',
            r'\n([A-Z][^\n]{10,50})\n(?=[A-Z]|$)'
        ]
        
        best_split = None
        best_count = 0
        
        for pattern in chapter_patterns:
            matches = list(re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE))
            
            if len(matches) >= 2:
                splits = []
                start = 0
                
                for match in matches:
                    if start < match.start():
                        splits.append(text[start:match.start()].strip())
                    start = match.start()
                
                splits.append(text[start:].strip())
                splits = [s for s in splits if s and len(s) > 50]
                
                if len(splits) > best_count:
                    best_split = splits
                    best_count = len(splits)
        
        return best_split if best_split else [text]
    
    def _split_by_sections(self, text: str) -> List[str]:
        """æŒ‰å°ç¯€åˆ†å‰²æ–‡æœ¬"""
        section_patterns = [
            r'\n(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç¯€[^\n]*)',
            r'\n(\d+\.\d+[^\n]*)',
            r'\n([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€[^\n]*)',
            r'\n([A-Z]\.[^\n]*)'
        ]
        
        for pattern in section_patterns:
            matches = list(re.finditer(pattern, text, re.MULTILINE))
            
            if len(matches) >= 2:
                splits = []
                start = 0
                
                for match in matches:
                    if start < match.start():
                        splits.append(text[start:match.start()].strip())
                    start = match.start()
                
                splits.append(text[start:].strip())
                return [s for s in splits if s and len(s) > 30]
        
        return [text]
    
    def _add_overlap_to_documents(self, documents: List[Document], overlap_size: int) -> List[Document]:
        """ç‚ºæ–‡æª”æ·»åŠ é‡ç–Šéƒ¨åˆ†"""
        if len(documents) <= 1:
            return documents
        
        for i in range(1, len(documents)):
            # å¾å‰ä¸€å€‹æ–‡æª”æœ«å°¾å–é‡ç–Šå…§å®¹
            prev_content = documents[i-1].page_content
            current_content = documents[i].page_content
            
            if len(prev_content) > overlap_size:
                overlap_text = prev_content[-overlap_size:]
                # å°‹æ‰¾å®Œæ•´çš„å¥å­é‚Šç•Œ
                sentence_end = overlap_text.rfind('ã€‚')
                if sentence_end == -1:
                    sentence_end = overlap_text.rfind('.')
                
                if sentence_end > overlap_size // 2:
                    overlap_text = overlap_text[sentence_end+1:]
                
                documents[i].page_content = overlap_text + "\n" + current_content
                documents[i].metadata['has_overlap'] = True
                documents[i].metadata['overlap_length'] = len(overlap_text)
        
        return documents
    
    def _create_chunk_document(self, content: str, doc_id: str, chunk_index: int, 
                          analysis: TextAnalysis, split_method: str) -> Document:
        """å‰µå»ºåˆ†å¡Šæ–‡æª” - çµ±ä¸€å…ƒæ•¸æ“šæ ¼å¼"""
        token_count = self.token_estimator.estimate_tokens(content)
        
        # æœå°‹URL
        url_regex = r'https?://[^\s\'"<>\[\]]+'  # ä¿®æ­£å¾Œçš„æ­£å‰‡è¡¨é”å¼ï¼Œå¯ä»¥åŒ¹é…æ›´è¤‡é›œçš„URL
        found_urls = re.findall(url_regex, content)
        if found_urls:
            logger.info(f"ğŸ” æˆåŠŸå¾æ–‡æœ¬å¡Šä¸­æå–åˆ°URL: {found_urls}")

        # åŸºæœ¬å…ƒæ•¸æ“šï¼ˆç¢ºä¿éƒ½æ˜¯ç°¡å–®é¡å‹ï¼‰
        metadata = {
            'doc_id': str(doc_id),
            'chunk_id': f"{doc_id}_{chunk_index+1:03d}",
            'chunk_index': int(chunk_index),
            'text_type': str(analysis.text_type),
            'processing_strategy': str(analysis.processing_strategy),
            'split_method': str(split_method),
            'chunk_length': int(len(content)),
            'token_count': int(token_count),
            'quality_score': float(analysis.quality_score),
            'language': str(analysis.language),
            'has_overlap': False
        }

        # ğŸ¯ æ­£ç¢ºæ–¹æ¡ˆï¼šURLåˆ—è¡¨ â†’ åˆ†éš”ç¬¦å­—ä¸² (é€™æ˜¯å”¯ä¸€å¯è¡Œçš„æ–¹æ¡ˆ)
        if found_urls:
            metadata['contained_urls'] = '|'.join(found_urls)  # âœ… å­—ç¬¦ä¸²é¡å‹
            metadata['url_count'] = len(found_urls)           # âœ… æ•´æ•¸é¡å‹
            metadata['has_urls'] = True                       # âœ… å¸ƒæ—é¡å‹
        else:
            metadata['contained_urls'] = ''                   # âœ… ç©ºå­—ç¬¦ä¸²
            metadata['url_count'] = 0                        # âœ… æ•´æ•¸ 0
            metadata['has_urls'] = False                     # âœ… å¸ƒæ— False

        # ğŸ¯ æ­£ç¢ºæ–¹æ¡ˆï¼šè¤‡é›œçµæ§‹ â†’ JSONå­—ä¸²
        if analysis.structure_info:
            metadata['structure_info'] = json.dumps(analysis.structure_info, ensure_ascii=False)  # âœ… JSONå­—ç¬¦ä¸²
            # åŒæ™‚ä¿ç•™é—œéµä¿¡æ¯ä½œç‚ºç°¡å–®å­—æ®µï¼ˆä¾¿æ–¼æŸ¥è©¢ï¼‰
            metadata['has_chapters'] = bool(analysis.structure_info.get('has_chapters', False))    # âœ… å¸ƒæ—
            metadata['has_sections'] = bool(analysis.structure_info.get('has_sections', False))    # âœ… å¸ƒæ—  
            metadata['paragraph_count'] = int(analysis.structure_info.get('paragraphs', 0))       # âœ… æ•´æ•¸
            metadata['sentence_count'] = int(analysis.structure_info.get('sentences', 0))         # âœ… æ•´æ•¸
        else:
            metadata['structure_info'] = '{}'        # âœ… ç©ºJSONå­—ç¬¦ä¸²
            metadata['has_chapters'] = False         # âœ… å¸ƒæ—
            metadata['has_sections'] = False         # âœ… å¸ƒæ—
            metadata['paragraph_count'] = 0          # âœ… æ•´æ•¸
            metadata['sentence_count'] = 0           # âœ… æ•´æ•¸
        
        return Document(page_content=content, metadata=metadata)

# --- End of content from text_processing.py ---


# --- Start of content from vector_builder.py ---

class AdaptiveBatchProcessor:
    """ğŸ”§ è‡ªé©æ‡‰æ‰¹æ¬¡è™•ç†å™¨"""
    
    def __init__(self):
        self.token_estimator = AdvancedTokenEstimator()
        self.max_tokens_per_request = TOKEN_LIMITS["max_tokens_per_request"]
        self.max_batch_size = TOKEN_LIMITS["max_batch_size"]
        self.adaptive_batching = TOKEN_LIMITS.get("adaptive_batching", True)
        
        # æ€§èƒ½çµ±è¨ˆ
        self.batch_stats = {
            'total_batches': 0,
            'total_documents': 0,
            'total_tokens': 0,
            'avg_batch_time': 0,
            'success_rate': 0,
            'adaptive_adjustments': 0
        }
        
        # è‡ªé©æ‡‰åƒæ•¸
        self.current_batch_size = self.max_batch_size
        self.success_streak = 0
        self.failure_streak = 0
    
    def create_smart_batches(self, documents: List[Document]) -> List[Tuple[List[Document], Dict]]:
        """å‰µå»ºæ™ºèƒ½æ‰¹æ¬¡"""
        if not documents:
            return []
        
        print("ğŸ”§ å‰µå»ºæ™ºèƒ½æ‰¹æ¬¡...")
        print(f"   æ–‡æª”æ•¸: {len(documents)}")
        print(f"   Token é™åˆ¶: {self.max_tokens_per_request:,}")
        print(f"   æœ€å¤§æ‰¹æ¬¡å¤§å°: {self.max_batch_size}")
        print(f"   è‡ªé©æ‡‰æ‰¹æ¬¡: {'âœ…' if self.adaptive_batching else 'âŒ'}")
        
        batches = []
        current_batch = []
        current_tokens = 0
        batch_info = {'documents': 0, 'tokens': 0, 'types': defaultdict(int)}
        
        # æŒ‰ token æ•¸æ’åºæ–‡æª”ï¼ˆå¤§çš„åœ¨å‰é¢ï¼Œæ›´å®¹æ˜“è™•ç†ï¼‰
        sorted_docs = sorted(
            documents, 
            key=lambda doc: doc.metadata.get('token_count', 
                self.token_estimator.estimate_tokens(doc.page_content)),
            reverse=True
        )
        
        for doc_idx, doc in enumerate(sorted_docs):
            doc_tokens = doc.metadata.get('token_count') or \
                        self.token_estimator.estimate_tokens(doc.page_content)
            
            # æª¢æŸ¥å–®å€‹æ–‡æª”æ˜¯å¦éå¤§
            if doc_tokens > self.max_tokens_per_request:
                print(f"   æ–‡æª” {doc_idx+1} éå¤§ ({doc_tokens:,} tokens)ï¼Œéœ€è¦åˆ†å‰²")
                
                # å®Œæˆç•¶å‰æ‰¹æ¬¡
                if current_batch:
                    batches.append((current_batch, dict(batch_info)))
                    current_batch = []
                    current_tokens = 0
                    batch_info = {'documents': 0, 'tokens': 0, 'types': defaultdict(int)}
                
                # åˆ†å‰²å¤§æ–‡æª”
                split_docs = self._split_large_document(doc)
                for split_doc in split_docs:
                    split_tokens = self.token_estimator.estimate_tokens(split_doc.page_content)
                    split_doc.metadata['token_count'] = split_tokens
                    batches.append(([split_doc], {
                        'documents': 1, 
                        'tokens': split_tokens, 
                        'types': {split_doc.metadata.get('text_type', 'unknown'): 1},
                        'is_split': True
                    }))
                continue
            
            # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŠ å…¥ç•¶å‰æ‰¹æ¬¡
            would_exceed_tokens = current_tokens + doc_tokens > self.max_tokens_per_request
            would_exceed_size = len(current_batch) >= self._get_adaptive_batch_size()
            
            if would_exceed_tokens or would_exceed_size:
                # å®Œæˆç•¶å‰æ‰¹æ¬¡
                if current_batch:
                    batches.append((current_batch, dict(batch_info)))
                
                # é–‹å§‹æ–°æ‰¹æ¬¡
                current_batch = [doc]
                current_tokens = doc_tokens
                batch_info = {
                    'documents': 1, 
                    'tokens': doc_tokens, 
                    'types': defaultdict(int)
                }
                batch_info['types'][doc.metadata.get('text_type', 'unknown')] += 1
            else:
                # åŠ å…¥ç•¶å‰æ‰¹æ¬¡
                current_batch.append(doc)
                current_tokens += doc_tokens
                batch_info['documents'] += 1
                batch_info['tokens'] += doc_tokens
                batch_info['types'][doc.metadata.get('text_type', 'unknown')] += 1
        
        # è™•ç†æœ€å¾Œä¸€å€‹æ‰¹æ¬¡
        if current_batch:
            batches.append((current_batch, dict(batch_info)))
        
        # çµ±è¨ˆä¿¡æ¯
        total_docs = sum(info['documents'] for _, info in batches)
        total_tokens = sum(info['tokens'] for _, info in batches)
        avg_tokens_per_batch = total_tokens / len(batches) if batches else 0
        
        print(f"âœ… æ™ºèƒ½æ‰¹æ¬¡å‰µå»ºå®Œæˆ:")
        print(f"   ç¸½æ‰¹æ¬¡æ•¸: {len(batches)}")
        print(f"   ç¸½æ–‡æª”æ•¸: {total_docs}")
        print(f"   ç¸½ tokens: {total_tokens:,}")
        print(f"   å¹³å‡ tokens/æ‰¹æ¬¡: {avg_tokens_per_batch:.0f}")
        print(f"   ä¼°ç®—æˆæœ¬: ${self.token_estimator.estimate_embedding_cost(total_tokens):.4f}")
        
        # æ›´æ–°çµ±è¨ˆ
        self.batch_stats['total_batches'] += len(batches)
        self.batch_stats['total_documents'] += total_docs
        self.batch_stats['total_tokens'] += total_tokens
        
        return batches
    
    def _get_adaptive_batch_size(self) -> int:
        """ç²å–è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°"""
        if not self.adaptive_batching:
            return self.max_batch_size
        
        # æ ¹æ“šæˆåŠŸç‡èª¿æ•´æ‰¹æ¬¡å¤§å°
        if self.success_streak >= 3:
            # é€£çºŒæˆåŠŸï¼Œå¯ä»¥å¢åŠ æ‰¹æ¬¡å¤§å°
            self.current_batch_size = min(self.max_batch_size, self.current_batch_size + 1)
            self.batch_stats['adaptive_adjustments'] += 1
        elif self.failure_streak >= 2:
            # é€£çºŒå¤±æ•—ï¼Œæ¸›å°‘æ‰¹æ¬¡å¤§å°
            self.current_batch_size = max(1, self.current_batch_size - 2)
            self.batch_stats['adaptive_adjustments'] += 1
        
        return self.current_batch_size
    
    def _split_large_document(self, doc: Document) -> List[Document]:
        """åˆ†å‰²éå¤§çš„æ–‡æª”"""
        content = doc.page_content
        max_chars = int(self.max_tokens_per_request * 2.5)  # ä¼°ç®—å­—ç¬¦æ•¸
        
        # å˜—è©¦æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        split_docs = []
        current_content = ""
        part_index = 0
        
        for para in paragraphs:
            if len(current_content + para) <= max_chars or not current_content:
                current_content += ("\n\n" if current_content else "") + para
            else:
                if current_content.strip():
                    split_doc = Document(
                        page_content=current_content,
                        metadata={
                            **doc.metadata,
                            'chunk_id': f"{doc.metadata.get('chunk_id', 'unknown')}_part{part_index+1}",
                            'is_split_part': True,
                            'part_index': part_index,
                            'split_reason': 'token_limit'
                        }
                    )
                    split_docs.append(split_doc)
                    part_index += 1
                current_content = para
        
        # è™•ç†æœ€å¾Œä¸€éƒ¨åˆ†
        if current_content.strip():
            split_doc = Document(
                page_content=current_content,
                metadata={
                    **doc.metadata,
                    'chunk_id': f"{doc.metadata.get('chunk_id', 'unknown')}_part{part_index+1}",
                    'is_split_part': True,
                    'part_index': part_index,
                    'split_reason': 'token_limit'
                }
            )
            split_docs.append(split_doc)
        
        return split_docs or [doc]  # å¦‚æœåˆ†å‰²å¤±æ•—ï¼Œè¿”å›åŸæ–‡æª”
    
    def record_batch_result(self, success: bool, processing_time: float = 0):
        """è¨˜éŒ„æ‰¹æ¬¡è™•ç†çµæœ"""
        if success:
            self.success_streak += 1
            self.failure_streak = 0
        else:
            self.failure_streak += 1
            self.success_streak = 0
        
        # æ›´æ–°æˆåŠŸç‡
        total_attempts = self.batch_stats['total_batches']
        if total_attempts > 0:
            self.batch_stats['success_rate'] = (total_attempts - self.failure_streak) / total_attempts
        
        # æ›´æ–°å¹³å‡è™•ç†æ™‚é–“
        if processing_time > 0:
            current_avg = self.batch_stats['avg_batch_time']
            self.batch_stats['avg_batch_time'] = (current_avg + processing_time) / 2
    
    def get_performance_stats(self) -> Dict:
        """ç²å–æ€§èƒ½çµ±è¨ˆ"""
        return dict(self.batch_stats)

# --- End of content from vector_builder.py ---


# å‘é‡å­˜å„² (æ¢ä»¶å°å…¥)
if PGVECTOR_AVAILABLE:
    try:
        from langchain_postgres import PGVector
    except ImportError:
        from langchain_community.vectorstores import PGVector
else:
    from langchain_community.vectorstores import Chroma

# OpenAI (æ¢ä»¶å°å…¥)
if OPENAI_EMBEDDINGS_AVAILABLE:
    from langchain_openai import OpenAIEmbeddings

# PostgreSQL (æ¢ä»¶å°å…¥)  
if PSYCOPG2_AVAILABLE:
    import psycopg2

logger = logging.getLogger(__name__)

class VectorOperationsCore:
    """å‘é‡æ“ä½œæ ¸å¿ƒé¡åˆ¥ - è² è²¬åº•å±¤æ•¸æ“šæ“ä½œå’Œå‘é‡è™•ç†"""
    
    def __init__(self, data_dir: str = None, model_type: str = None):
        """âœ… ç´”PostgreSQLåˆå§‹åŒ– - ç§»é™¤file_recordsä¾è³´"""
        
        # ğŸ”§ 1. åŸºæœ¬è®Šæ•¸è¨­ç½®
        self.data_dir = Path(data_dir or SYSTEM_CONFIG["data_dir"])
        self.model_type = model_type or "openai"
        self.persist_dir = Path(SYSTEM_CONFIG["persist_dir"])  # Chromaå‚™ç”¨

        # âŒ ä¸å†å»ºç«‹æœ¬åœ°ç›®éŒ„ (ç´”PostgreSQLæ–¹æ¡ˆ)
        print("ğŸš€ ç´”PostgreSQLæ–¹æ¡ˆï¼šä¸ä½¿ç”¨æœ¬åœ°dataç›®éŒ„")
        
        # ğŸ”§ 2. è³‡æ–™åº«é€£æ¥è¨­ç½®ï¼ˆä½†ä¸æ¸¬è©¦ï¼‰
        self.db_adapter = None
        self.connection_string = None
        self.use_postgres = False

        database_url = os.getenv("DATABASE_URL")
        if PGVECTOR_AVAILABLE and database_url:
            self.connection_string = database_url
            print("ğŸ” ç™¼ç¾DATABASE_URLï¼Œæº–å‚™æ¸¬è©¦PostgreSQLé€£æ¥...")
        else:
            print("âš ï¸ DATABASE_URLæœªè¨­ç½®æˆ–PGVectorä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨Chroma")

        if not PGVECTOR_AVAILABLE:
            print("âš ï¸ PGVectorä¾è³´æœªå®‰è£ï¼Œä½¿ç”¨Chromaä½œç‚ºå‚™ç”¨")
            self.persist_dir.mkdir(exist_ok=True)

        # âœ… 3. å…ˆåˆå§‹åŒ–Embeddingæ¨¡å‹ï¼ˆé—œéµ!)
        self._setup_embedding_model()
        print("âœ… Embeddingæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        # âœ… 4. ç¾åœ¨å¯ä»¥æ¸¬è©¦PostgreSQLé€£æ¥äº†ï¼ˆembeddingså·²å­˜åœ¨ï¼‰
        if PGVECTOR_AVAILABLE and database_url and hasattr(self, 'embeddings'):
            try:
                print("ğŸ” æ¸¬è©¦PostgreSQL + PGVectoré€£æ¥...")
                # æ¸¬è©¦é€£æ¥
                PGVector.from_existing_index(
                    collection_name="_test_connection",
                    embedding=self.embeddings,
                    connection=self.connection_string  # âœ… ä¿®æ­£åƒæ•¸åç¨±
                )
                self.use_postgres = True
                print("âœ… PostgreSQL (pgvector) é€£æ¥æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ PostgreSQL (pgvector) é€£æ¥æ¸¬è©¦å¤±æ•—: {e}")
                self.use_postgres = False
                print("ğŸ“„ å›é€€åˆ°Chromaæœ¬åœ°å­˜å„²")
                self.persist_dir.mkdir(exist_ok=True)
        
        if not self.use_postgres:
            print("ğŸ” ä½¿ç”¨Chromaä½œç‚ºå‘é‡å­˜å„²")
            self.persist_dir.mkdir(exist_ok=True)
        
        # ğŸ”§ 5. åˆå§‹åŒ–æ–‡æœ¬è™•ç†çµ„ä»¶
        self._setup_text_processing()
        
        # ğŸ”§ 6. åˆå§‹åŒ–è™•ç†å™¨
        self.batch_processor = AdaptiveBatchProcessor()
        self.text_splitter = OptimizedTextSplitter()
        
        # ğŸ”§ 7. åˆå§‹åŒ–å­˜å„²ï¼ˆç§»é™¤æª”æ¡ˆè¨˜éŒ„ï¼‰
        self._vector_stores = {}
        
        # âœ… æ·»åŠ file_recordsåˆå§‹åŒ–
        self.file_records = {}
        
        # âœ… æ”¹ç‚ºç´”PostgreSQLåˆå§‹åŒ–
        print("ğŸš€ ç´”PostgreSQLæ–¹æ¡ˆï¼šæ‰€æœ‰æª”æ¡ˆæ•¸æ“šå°‡ç›´æ¥å­˜å„²åœ¨PostgreSQLä¸­")
        print("ğŸ“„ ä¸å†ç¶­è­·æœ¬åœ°æª”æ¡ˆè¨˜éŒ„ (file_records.json)")
        
        self.processing_lock = threading.Lock()
        
        print(f"ğŸš€ å‘é‡æ“ä½œæ ¸å¿ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"   åµŒå…¥æ¨¡å‹: {self.model_type}")
        print(f"   è³‡æ–™ç›®éŒ„: ä¸ä½¿ç”¨ (ç´”PostgreSQL)")
        print(f"   å‘é‡åº«: {'PostgreSQL + PGVector' if self.use_postgres else 'Chroma (æœ¬åœ°)'}")
        print(f"   æ™ºèƒ½æ–‡æœ¬è™•ç†: âœ…")
        print(f"   è‡ªé©æ‡‰æ‰¹æ¬¡: âœ…")
        print(f"   ç´”PostgreSQLæ–¹æ¡ˆ: {'âœ…' if self.use_postgres else 'âŒ'}")

    def _setup_embedding_model(self):
        """è¨­å®šåµŒå…¥æ¨¡å‹"""
        try:
            if self.model_type == "openai":
                if not OPENAI_EMBEDDINGS_AVAILABLE:
                    raise ImportError("OpenAI Embeddingsä¸å¯ç”¨")
                
                print(f"ğŸ”§ åˆå§‹åŒ–OpenAI Embeddings...")
                
                api_key = os.getenv("OPENAI_API_KEY")
                base_url = os.getenv("OPENAI_API_BASE")
                
                embedding_params = {
                    "model": "text-embedding-3-small",
                    "api_key": api_key,
                    "max_retries": 3,
                    "request_timeout": 60  # å¢åŠ è¶…æ™‚æ™‚é–“åˆ°60ç§’
                }
                
                if base_url:
                    embedding_params["base_url"] = base_url
                    print(f"ğŸ”§ ä½¿ç”¨è‡ªå®šç¾©APIç«¯é»: {base_url}")
                
                self.embeddings = OpenAIEmbeddings(**embedding_params)
                print("âœ… OpenAI Embeddingsåˆå§‹åŒ–æˆåŠŸ")
                
            else:
                # HuggingFaceæ¨¡å‹
                print(f"ğŸ”§ åˆå§‹åŒ–HuggingFace Embeddings...")
                
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="BAAI/bge-small-zh-v1.5",
                    model_kwargs={"device": "cpu"},
                    encode_kwargs={"batch_size": 16, "normalize_embeddings": True}
                )
                print("âœ… HuggingFace Embeddingsåˆå§‹åŒ–æˆåŠŸ")
                
        except Exception as e:
            print(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
            
            # å›é€€æ©Ÿåˆ¶
            if self.model_type == "openai":
                print("ğŸ“„ å˜—è©¦HuggingFaceå‚™é¸...")
                try:
                    self.embeddings = HuggingFaceEmbeddings(
                        model_name="BAAI/bge-small-zh-v1.5",
                        model_kwargs={"device": "cpu"}
                    )
                    self.model_type = "huggingface"
                    print("âœ… å·²å›é€€åˆ°HuggingFace")
                except Exception as e2:
                    raise RuntimeError(f"æ‰€æœ‰åµŒå…¥æ¨¡å‹éƒ½åˆå§‹åŒ–å¤±æ•—: {e2}")
            else:
                raise

    def _setup_text_processing(self):
        """è¨­å®šæ–‡æœ¬è™•ç†çµ„ä»¶"""
        self.normalizer = ChineseTextNormalizer()
        self.analyzer = SmartTextAnalyzer()
        print("âœ… æ–‡æœ¬è™•ç†çµ„ä»¶åˆå§‹åŒ–å®Œæˆ")

    def get_or_create_vectorstore(self, collection_name: str):
        """ç²å–æˆ–å‰µå»ºå‘é‡å­˜å„² - PostgreSQLå„ªå…ˆ"""
        if collection_name not in self._vector_stores:
            try:
                if self.use_postgres and PGVECTOR_AVAILABLE:
                    # ğŸ”§ ä½¿ç”¨PGVector
                    try:
                        from langchain_postgres import PGVector
                        self._vector_stores[collection_name] = PGVector(
                            embeddings=self.embeddings,
                            collection_name=collection_name,
                            connection=self.connection_string,
                            use_jsonb=True,
                        )
                    except ImportError:
                        from langchain_community.vectorstores import PGVector
                        self._vector_stores[collection_name] = PGVector(
                            connection=self.connection_string,  # âœ… ä¿®æ­£åƒæ•¸
                            collection_name=collection_name,
                            embeddings=self.embeddings,  # âœ… ä¿®æ­£åƒæ•¸
                            distance_strategy="cosine",
                            pre_delete_collection=False,
                            logger=logger
                        )
                    print(f"âœ… PGVectorå‘é‡å­˜å„²å°±ç·’: {collection_name}")
                else:
                    # ğŸ”§ å‚™ç”¨Chroma - ç¢ºä¿å°å…¥
                    if CHROMA_AVAILABLE:
                        from langchain_community.vectorstores import Chroma
                        self._vector_stores[collection_name] = Chroma(
                            collection_name=collection_name,
                            embedding_function=self.embeddings,
                            persist_directory=str(self.persist_dir)
                        )
                        print(f"âœ… Chromaå‘é‡å­˜å„²å°±ç·’: {collection_name}")
                    else:
                        raise ImportError("Chromaä¸å¯ç”¨ä¸”PostgreSQLä¹Ÿä¸å¯ç”¨")
                        
            except Exception as e:
                logger.error(f"å‘é‡å­˜å„²å‰µå»ºå¤±æ•—: {e}")
                raise RuntimeError(f"ç„¡æ³•å‰µå»ºå‘é‡å­˜å„²: {e}")
        
        return self._vector_stores[collection_name]

    def _generate_doc_id(self, file_path: Path) -> str:
        """ç”Ÿæˆæ–‡æª”ID"""
        # âœ… ä¿®æ­£èªæ³•éŒ¯èª¤
        content_hash = hashlib.md5(str(file_path).encode()).hexdigest()[:8]
        return f"doc_{file_path.stem}_{content_hash}"

    def load_document(self, file_path: Path) -> List[Document]:
        """è¼‰å…¥ä¸¦æ™ºèƒ½è™•ç†æ–‡æª”"""
        try:
            extension = file_path.suffix.lower()
            
            # æ ¹æ“šæª”æ¡ˆé¡å‹è¼‰å…¥
            if extension == '.pdf':
                loader = PyPDFLoader(str(file_path))
                documents = loader.load()
                full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            elif extension in ['.docx', '.doc'] and DOCX2TXT_AVAILABLE:
                if DOCX_METHOD == "docx2txt":
                    import docx2txt
                    full_text = docx2txt.process(str(file_path))
                else:
                    loader = Docx2txtLoader(str(file_path))
                    documents = loader.load()
                    full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            elif extension == '.epub' and EPUB_AVAILABLE:
                # ğŸ†• EPUBè™•ç†é‚è¼¯
                epub_processor = EpubProcessor()
                full_text = epub_processor.extract_epub_content(file_path)
            elif extension == '.csv':
                loader = CSVLoader(str(file_path), encoding='utf-8')
                documents = loader.load()
                full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            elif extension in ['.xlsx', '.xls']:
                loader = PandasExcelLoader(str(file_path), sheet_name=None)
                documents = loader.load()
                full_text = "\n\n".join([doc.page_content for doc in documents if doc.page_content])
            elif extension == '.json':
                loader = JSONLoader(str(file_path), jq_schema='.', text_content=False)
                documents = loader.load()
                full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            else:
                # å˜—è©¦è‡ªå‹•æª¢æ¸¬ç·¨ç¢¼
                encodings = ['utf-8', 'gbk', 'gb2312', 'big5']
                full_text = None
                
                for encoding in encodings:
                    try:
                        with open(file_path, 'r', encoding=encoding) as f:
                            full_text = f.read()
                        break
                    except UnicodeDecodeError:
                        continue
                
                if full_text is None:
                    raise ValueError(f"ç„¡æ³•è§£ç¢¼æ–‡ä»¶: {file_path}")
            
            if not full_text or not full_text.strip():
                logger.warning(f"æ–‡ä»¶å…§å®¹ç‚ºç©ºæˆ–ç„¡æ³•æå–: {file_path.name}")
                return []
            
            # ç”Ÿæˆæ–‡æª”ID
            doc_id = self._generate_doc_id(file_path)
            
            # ä½¿ç”¨å„ªåŒ–çš„æ–‡æœ¬åˆ†å‰²å™¨
            source_info = {
                'file_path': str(file_path),
                'file_type': extension,
                'file_size': file_path.stat().st_size if file_path.exists() else 0
            }
            
            documents = self.text_splitter.smart_split_documents(full_text, doc_id, source_info)
            
            # æ·»åŠ çµ±ä¸€å…ƒæ•¸æ“š
            for doc in documents:
                doc.metadata.update({
                    'source': str(file_path),
                    'filename': file_path.name,
                    'extension': extension,
                    'file_size': source_info['file_size'],
                    'load_timestamp': time.time()
                })
            
            print(f"ğŸ“„ æ–‡æª”è¼‰å…¥å®Œæˆ: {file_path.name} ({len(documents)} å€‹åˆ†å¡Š)")
            
            return documents
            
        except Exception as e:
            logger.error(f"è¼‰å…¥æ–‡æª”å¤±æ•— {file_path}: {e}")
            print(f"âŒ æ–‡æª”è¼‰å…¥å¤±æ•—: {file_path.name} - {e}")
            return []

    def get_file_info(self, file_path: Path) -> Optional[FileInfo]:
        """ç²å–æª”æ¡ˆåŸºæœ¬ä¿¡æ¯"""
        try:
            if not file_path.exists():
                return None
                
            stat = file_path.stat()
            
            # è¨ˆç®—æª”æ¡ˆå“ˆå¸Œï¼ˆç”¨æ–¼è®Šæ›´æª¢æ¸¬ï¼‰
            hash_md5 = hashlib.md5()
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            
            return FileInfo(
                path=str(file_path),
                size=stat.st_size,
                mtime=stat.st_mtime,
                hash=hash_md5.hexdigest(),
                encoding='utf-8',
                file_type=file_path.suffix.lower()
            )
            
        except Exception as e:
            logger.error(f"ç²å–æª”æ¡ˆä¿¡æ¯å¤±æ•— {file_path}: {e}")
            return None

    def _parallel_load_documents(self, file_paths: List[Path], collection_name: str) -> List[Document]:
        """ä¸¦ç™¼è¼‰å…¥æ–‡æª”"""
        all_documents = []
        max_workers = min(SYSTEM_CONFIG.get("max_workers", 4), len(file_paths))
        
        print(f"   ä¸¦ç™¼è¼‰å…¥ (å·¥ä½œç·šç¨‹: {max_workers})")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_file = {
                executor.submit(self.load_document, file_path): file_path 
                for file_path in file_paths
            }
            
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    documents = future.result()
                    if documents:
                        for doc in documents:
                            doc.metadata['collection'] = collection_name
                        all_documents.extend(documents)
                        print(f"   {file_path.name}: {len(documents)} åˆ†å¡Š")
                    else:
                        print(f"   {file_path.name}: ç„¡æœ‰æ•ˆå…§å®¹")
                except Exception as e:
                    print(f"   {file_path.name}: {e}")
                    logger.error(f"ä¸¦ç™¼è¼‰å…¥å¤±æ•— {file_path}: {e}")
        
        return all_documents

    def _sequential_load_documents(self, file_paths: List[Path], collection_name: str) -> List[Document]:
        """é †åºè¼‰å…¥æ–‡æª”"""
        all_documents = []
        
        print(f"   é †åºè¼‰å…¥")
        
        for i, file_path in enumerate(file_paths, 1):
            try:
                print(f"   [{i}/{len(file_paths)}] è™•ç†: {file_path.name}")
                documents = self.load_document(file_path)
                
                if documents:
                    for doc in documents:
                        doc.metadata['collection'] = collection_name
                    all_documents.extend(documents)
                    print(f"      è¼‰å…¥ {len(documents)} å€‹åˆ†å¡Š")
                else:
                    print(f"      ç„¡æœ‰æ•ˆå…§å®¹")
                    
            except Exception as e:
                print(f"      è¼‰å…¥å¤±æ•—: {e}")
                logger.error(f"æ–‡ä»¶è¼‰å…¥å¤±æ•— {file_path}: {e}")
        
        return all_documents

    def _load_file_records(self) -> Dict[str, Dict[str, FileInfo]]:
        """è¼‰å…¥æª”æ¡ˆè¨˜éŒ„ - åŠ å¼·éŒ¯èª¤è™•ç†å’Œæ¢å¾©æ©Ÿåˆ¶"""
        record_file = self.data_dir / "file_records.json"
        
        # ğŸ”§ æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not record_file.exists():
            print("ğŸ” æª”æ¡ˆè¨˜éŒ„ä¸å­˜åœ¨ï¼Œå°‡å»ºç«‹æ–°çš„è¨˜éŒ„")
            return {}
        
        try:
            # ğŸ”§ è®€å–ä¸¦æª¢æŸ¥æª”æ¡ˆå…§å®¹
            with open(record_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            # ğŸ”§ æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºç©º
            if not content:
                print("âš ï¸ æª”æ¡ˆè¨˜éŒ„ç‚ºç©ºï¼Œå°‡å»ºç«‹æ–°çš„è¨˜éŒ„")
                return {}
            
            # ğŸ”§ æª¢æŸ¥æ˜¯å¦ä»¥{é–‹é ­ï¼ˆåŸºæœ¬JSONæ ¼å¼æª¢æŸ¥ï¼‰
            if not content.startswith('{'):
                print(f"âš ï¸ æª”æ¡ˆè¨˜éŒ„æ ¼å¼éŒ¯èª¤ï¼Œå…§å®¹é–‹é ­: {repr(content[:50])}")
                return self._handle_corrupted_records(record_file, content)
            
            # ğŸ”§ å˜—è©¦è§£æJSON
            try:
                data = json.loads(content)
            except json.JSONDecodeError as json_error:
                print(f"âŒ JSONè§£æå¤±æ•—: {json_error}")
                print(f"   éŒ¯èª¤ä½ç½®: line {json_error.lineno}, column {json_error.colno}")
                print(f"   æª”æ¡ˆå‰100å­—ç¬¦: {repr(content[:100])}")
                return self._handle_corrupted_records(record_file, content)
            
            # ğŸ”§ é©—è­‰è³‡æ–™æ ¼å¼
            if not isinstance(data, dict):
                print(f"âš ï¸ æª”æ¡ˆè¨˜éŒ„æ ¼å¼éŒ¯èª¤ï¼Œæ‡‰ç‚ºå­—å…¸ä½†å¾—åˆ°: {type(data)}")
                return {}
            
            # ğŸ”§ è½‰æ›ç‚ºFileInfoç‰©ä»¶
            records = {}
            for collection, files in data.items():
                records[collection] = {}
                for file_path, info in files.items():
                    try:
                        if isinstance(info, dict):
                            fileinfo_fields = {
                                'path': info.get('path', file_path),
                                'size': info.get('size', 0),
                                'mtime': info.get('mtime', time.time()),
                                'hash': info.get('hash', ''),
                                'encoding': info.get('encoding', 'utf-8'),
                                'file_type': info.get('file_type', '')
                            }
                            
                            file_info_obj = FileInfo(**fileinfo_fields)
                            
                            # ğŸ”§ æ¢å¾©é¡å¤–å±¬æ€§
                            if 'uploaded_by' in info:
                                file_info_obj.uploaded_by = info['uploaded_by']
                            if 'uploaded_at' in info:
                                file_info_obj.uploaded_at = info['uploaded_at']
                            if 'file_source' in info:
                                file_info_obj.file_source = info['file_source']
                            
                            records[collection][file_path] = file_info_obj
                        else:
                            records[collection][file_path] = info
                            
                    except Exception as e:
                        logger.warning(f"è¼‰å…¥æª”æ¡ˆè¨˜éŒ„å¤±æ•— {file_path}: {e}")
                        # ğŸ”§ å»ºç«‹é è¨­è¨˜éŒ„
                        try:
                            default_info = FileInfo(
                                path=file_path,
                                size=0,
                                mtime=time.time(),
                                hash='',
                                encoding='utf-8',
                                file_type=''
                            )
                            records[collection][file_path] = default_info
                        except Exception:
                            logger.error(f"ç„¡æ³•å»ºç«‹é è¨­FileInfo for {file_path}")
                            continue
            
            print(f"âœ… æª”æ¡ˆè¨˜éŒ„è¼‰å…¥æˆåŠŸ: {len(records)} å€‹é›†åˆ")
            return records
            
        except Exception as e:
            logger.error(f"è¼‰å…¥æª”æ¡ˆè¨˜éŒ„å¤±æ•—: {e}")
            print(f"âŒ åš´é‡éŒ¯èª¤ï¼Œè¼‰å…¥æª”æ¡ˆè¨˜éŒ„å¤±æ•—: {e}")
            return self._handle_corrupted_records(record_file, "")

    def _save_file_records(self):
        """ä¿å­˜æª”æ¡ˆè¨˜éŒ„"""
        record_file = self.data_dir / "file_records.json"
        
        try:
            # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            record_file.parent.mkdir(parents=True, exist_ok=True)
            
            # è½‰æ›FileInfoç‰©ä»¶ç‚ºå­—å…¸
            serializable_records = {}
            for collection, files in self.file_records.items():
                serializable_records[collection] = {}
                for file_path, file_info in files.items():
                    if isinstance(file_info, FileInfo):
                        serializable_records[collection][file_path] = asdict(file_info)
                    else:
                        serializable_records[collection][file_path] = file_info
            
            # å…ˆå¯«å…¥è‡¨æ™‚æª”æ¡ˆï¼Œç„¶å¾ŒåŸå­æ€§æ›¿æ›
            temp_file = record_file.with_suffix('.tmp')
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_records, f, indent=2, ensure_ascii=False)
            
            temp_file.replace(record_file)
            logger.info(f"æª”æ¡ˆè¨˜éŒ„ä¿å­˜æˆåŠŸ: {len(serializable_records)} å€‹é›†åˆ")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æª”æ¡ˆè¨˜éŒ„å¤±æ•—: {e}")

    def _handle_corrupted_records(self, record_file: Path, content: str) -> Dict:
        """è™•ç†æå£çš„æª”æ¡ˆè¨˜éŒ„"""
        print(f"ğŸ”§ è™•ç†æå£çš„æª”æ¡ˆè¨˜éŒ„...")
        
        # å‰µå»ºå‚™ä»½
        try:
            backup_file = record_file.with_suffix(f'.backup_{int(time.time())}')
            if record_file.exists():
                shutil.copy2(record_file, backup_file)
                print(f"   å‚™ä»½: {backup_file.name}")
        except Exception as e:
            logger.warning(f"å‰µå»ºå‚™ä»½å¤±æ•—: {e}")
        
        # å˜—è©¦æ¢å¾©
        try:
            return self._rebuild_file_records()
        except Exception as e:
            logger.error(f"é‡å»ºæª”æ¡ˆè¨˜éŒ„å¤±æ•—: {e}")
            return {}

    def _rebuild_file_records(self) -> Dict:
        """é‡å»ºæª”æ¡ˆè¨˜éŒ„"""
        print("ğŸ”§ é‡å»ºæª”æ¡ˆè¨˜éŒ„...")
        
        new_records = {}
        
        if not self.data_dir.exists():
            return new_records
        
        # æƒææ‰€æœ‰å­ç›®éŒ„
        for subdir in self.data_dir.iterdir():
            if subdir.is_dir() and not subdir.name.startswith('.'):
                collection_name = f"collection_{subdir.name}"
                new_records[collection_name] = {}
                
                # æƒæç›®éŒ„ä¸­çš„æª”æ¡ˆ
                for file_path in subdir.rglob('*'):
                    if (
                        file_path.is_file() and 
                        file_path.suffix.lower() in SUPPORTED_EXTENSIONS
                    ):
                        
                        file_info = self.get_file_info(file_path)
                        if file_info:
                            new_records[collection_name][str(file_path)] = file_info
        
        print(f"âœ… é‡å»ºå®Œæˆ: {len(new_records)} å€‹é›†åˆ")
        return new_records

    def get_file_source_statistics(self) -> Dict[str, Dict[str, int]]:
        """ç²å–æª”æ¡ˆä¾†æºçµ±è¨ˆ"""
        stats = {}
        
        for collection_name, files in self.file_records.items():
            stats[collection_name] = {
                'total': len(files),
                'upload': 0,
                'sync': 0,
                'unknown': 0
            }
            
            for file_info in files.values():
                source = getattr(file_info, 'file_source', 'unknown')
                if source in stats[collection_name]:
                    stats[collection_name][source] += 1
                else:
                    stats[collection_name]['unknown'] += 1
        
        return stats

    def diagnose_file_records(self) -> Dict:
        """è¨ºæ–·æª”æ¡ˆè¨˜éŒ„"""
        diagnosis = {
            'total_collections': len(self.file_records),
            'total_files': sum(len(files) for files in self.file_records.values()),
            'missing_files': 0,
            'outdated_files': 0,
            'corrupted_records': 0
        }
        
        for collection_name, files in self.file_records.items():
            for file_path, file_info in files.items():
                try:
                    path_obj = Path(file_path)
                    
                    if not path_obj.exists():
                        diagnosis['missing_files'] += 1
                        continue
                    
                    current_mtime = path_obj.stat().st_mtime
                    if abs(current_mtime - file_info.mtime) > 1:
                        diagnosis['outdated_files'] += 1
                        
                except Exception:
                    diagnosis['corrupted_records'] += 1
        
        return diagnosis

    def cleanup_invalid_records(self) -> Dict:
        """æ¸…ç†ç„¡æ•ˆè¨˜éŒ„"""
        removed_count = 0
        
        for collection_name in list(self.file_records.keys()):
            files = self.file_records[collection_name]
            
            for file_path in list(files.keys()):
                try:
                    if not Path(file_path).exists():
                        del files[file_path]
                        removed_count += 1
                except Exception:
                    del files[file_path]
                    removed_count += 1
            
            # ç§»é™¤ç©ºé›†åˆ
            if not files:
                del self.file_records[collection_name]
        
        if removed_count > 0:
            self._save_file_records()
        
        return {'removed_records': removed_count}

    def _process_batches(self, vectorstore: Union["Chroma", Any], batches: List[Tuple[List[Document], Dict]]) -> int:
        """è™•ç†æ‰¹æ¬¡å‘é‡åŒ– - å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œå…ƒæ•¸æ“šä¿®å¾©"""

        success_count = 0
        total_docs = sum(len(batch_docs) for batch_docs, _ in batches)
        
        print(f"\né–‹å§‹æ‰¹æ¬¡å‘é‡åŒ–...")
        print(f"   ç¸½æ‰¹æ¬¡æ•¸: {len(batches)}")
        print(f"   ç¸½æ–‡æª”æ•¸: {total_docs}")
        
        for batch_num, (batch_docs, batch_info) in enumerate(batches, 1):
            print(f"\n   æ‰¹æ¬¡ {batch_num}/{len(batches)}")
            print(f"      æ–‡æª”æ•¸: {batch_info['documents']}")
            print(f"      tokens: {batch_info['tokens']:,}")
            print(f"      ä½¿ç”¨ç‡: {(batch_info['tokens']/TOKEN_LIMITS['max_tokens_per_request']*100):.1f}%")
            
            # é¡¯ç¤ºæ–‡æª”é¡å‹åˆ†å¸ƒ
            type_info = ", ".join([f"{k}:{v}" for k, v in batch_info['types'].items()])
            print(f"      é¡å‹: {type_info}")
            
            start_time = time.time()
            
            try:
                print(f"      é–‹å§‹è™•ç†æ‰¹æ¬¡ {batch_num}...")
                print(f"      æ­£åœ¨èª¿ç”¨OpenAI API... (é€™å¯èƒ½éœ€è¦30-60ç§’)")
                
                # ğŸ› ï¸ ä¿®å¾©ï¼šçµ±ä¸€è™•ç†å…ƒæ•¸æ“šï¼Œç¢ºä¿é¡å‹æ­£ç¢º
                safe_docs = []
                for doc in batch_docs:
                    safe_metadata = self._ensure_simple_metadata(doc.metadata)
                    safe_doc = Document(page_content=doc.page_content, metadata=safe_metadata)
                    safe_docs.append(safe_doc)

                print(f"      å·²è™•ç† {len(safe_docs)} å€‹æ–‡æª”çš„å…ƒæ•¸æ“šï¼Œç¢ºä¿é¡å‹å…¼å®¹")
                
                vectorstore.add_documents(safe_docs)
                processing_time = time.time() - start_time
                
                success_count += len(batch_docs)
                self.batch_processor.record_batch_result(True, processing_time)
                
                print(f"      æ‰¹æ¬¡ {batch_num} å®Œæˆ ({processing_time:.1f}s)")
                print(f"      ç¸½é€²åº¦: {success_count}/{total_docs} ({success_count/total_docs*100:.1f}%)")
                
                # æ‰¹æ¬¡é–“å»¶é²
                if batch_num < len(batches):
                    delay = TOKEN_LIMITS["batch_delay"]
                    print(f"      ç­‰å¾… {delay} ç§’...")
                    time.sleep(delay)
                    
            except Exception as e:
                processing_time = time.time() - start_time
                self.batch_processor.record_batch_result(False, processing_time)
                
                error_msg = str(e)
                print(f"      æ‰¹æ¬¡ {batch_num} å¤±æ•— ({processing_time:.1f}s)")
                print(f"         éŒ¯èª¤: {error_msg}")
                
                # ğŸ”§ ç‰¹åˆ¥è™•ç†å…ƒæ•¸æ“šéŒ¯èª¤
                if "metadata" in error_msg.lower():
                    print(f"         æª¢æ¸¬åˆ°å…ƒæ•¸æ“šéŒ¯èª¤ï¼Œå˜—è©¦æ›´åš´æ ¼çš„è™•ç†...")
                    try:
                        # é‡æ–°å˜—è©¦ï¼Œä½¿ç”¨æœ€åš´æ ¼çš„å…ƒæ•¸æ“šéæ¿¾
                        ultra_safe_docs = []
                        for doc in batch_docs:
                            # åªä¿ç•™æœ€åŸºæœ¬çš„å…ƒæ•¸æ“šå­—æ®µ
                            minimal_metadata = {
                                'doc_id': str(doc.metadata.get('doc_id', 'unknown')),
                                'chunk_id': str(doc.metadata.get('chunk_id', 'unknown')),
                                'chunk_index': int(doc.metadata.get('chunk_index', 0)),
                                'text_type': str(doc.metadata.get('text_type', 'unknown')),
                                'source': str(doc.metadata.get('source', 'unknown')),
                                'filename': str(doc.metadata.get('filename', 'unknown')),
                                'token_count': int(doc.metadata.get('token_count', 0)),
                                'chunk_length': int(doc.metadata.get('chunk_length', 0)),
                                # URLè™•ç†
                                'contained_urls': str(doc.metadata.get('contained_urls', '')),
                                'url_count': int(doc.metadata.get('url_count', 0)),
                                'has_urls': bool(doc.metadata.get('has_urls', False))
                            }
                            
                            ultra_safe_doc = Document(
                                page_content=doc.page_content,
                                metadata=minimal_metadata
                            )
                            ultra_safe_docs.append(ultra_safe_doc)
                        
                        vectorstore.add_documents(ultra_safe_docs)
                        success_count += len(batch_docs)
                        print(f"         ä½¿ç”¨æœ€å°åŒ–å…ƒæ•¸æ“šé‡æ–°è™•ç†æˆåŠŸ")
                        continue
                        
                    except Exception as retry_e:
                        print(f"         é‡æ–°è™•ç†ä¹Ÿå¤±æ•—: {retry_e}")
                # å…¶ä»–éŒ¯èª¤è™•ç†
                if "timeout" in error_msg.lower():
                    print(f"         è¶…æ™‚éŒ¯èª¤ï¼Œå»¶é•·ç­‰å¾…æ™‚é–“...")
                    time.sleep(30)
                elif "rate_limit" in error_msg.lower() or "429" in error_msg:
                    print(f"         é€Ÿç‡é™åˆ¶ï¼Œå»¶é•·ç­‰å¾…...")
                    time.sleep(60)
                elif "token" in error_msg.lower() and batch_info['documents'] > 1:
                    print(f"         Tokenè¶…é™ï¼Œå˜—è©¦å–®å€‹è™•ç†...")
                    single_success = self._process_documents_individually(vectorstore, batch_docs)
                    success_count += single_success
                elif "connection" in error_msg.lower():
                    print(f"         é€£æ¥éŒ¯èª¤ï¼Œç­‰å¾…é‡è©¦...")
                    time.sleep(20)
                    try:
                        print(f"         é‡è©¦æ‰¹æ¬¡ {batch_num}...")
                        # ä½¿ç”¨å®‰å…¨çš„å…ƒæ•¸æ“šé‡è©¦
                        safe_docs = []
                        for doc in batch_docs:
                            safe_metadata = self._ensure_simple_metadata(doc.metadata)
                            safe_doc = Document(page_content=doc.page_content, metadata=safe_metadata)
                            safe_docs.append(safe_doc)
                        
                        vectorstore.add_documents(safe_docs)
                        success_count += len(batch_docs)
                        print(f"         é‡è©¦æˆåŠŸ")
                    except Exception as retry_e:
                        print(f"         é‡è©¦å¤±æ•—: {retry_e}")
                else:
                    print(f"         è·³éæ­¤æ‰¹æ¬¡")
                    
                # æ¯æ¬¡éŒ¯èª¤å¾Œæ·»åŠ é¡å¤–å»¶é²
                print(f"         éŒ¯èª¤å¾Œæš«åœ10ç§’...")
                time.sleep(10)
        
        return success_count

    def _ensure_simple_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """ç¢ºä¿å…ƒæ•¸æ“šåªåŒ…å«Chromaæ”¯æŒçš„ç°¡å–®é¡å‹ï¼šstring, int, float, bool"""
        safe_metadata = {}
        
        for key, value in metadata.items():
            if isinstance(value, (str, int, float, bool)) or value is None:
                # âœ… å·²ç¶“æ˜¯Chromaæ”¯æŒçš„ç°¡å–®é¡å‹ï¼Œç›´æ¥ä¿ç•™
                safe_metadata[key] = value
            elif isinstance(value, list):
                # âŒ åˆ—è¡¨ä¸è¢«æ”¯æŒ â†’ è½‰ç‚ºåˆ†éš”ç¬¦å­—ç¬¦ä¸²
                if key == 'contained_urls' or 'url' in key.lower():
                    safe_metadata[key] = '|'.join(str(v) for v in value) if value else ''
                    safe_metadata[f'{key}_count'] = len(value)
                else:
                    safe_metadata[key] = '|'.join(str(v) for v in value) if value else ''
                    safe_metadata[f'{key}_count'] = len(value)
            elif isinstance(value, dict):
                # âŒ å­—å…¸ä¸è¢«æ”¯æŒ â†’ è½‰ç‚ºJSONå­—ç¬¦ä¸²
                safe_metadata[key] = json.dumps(value, ensure_ascii=False)
            else:
                # âŒ å…¶ä»–é¡å‹ä¸è¢«æ”¯æŒ â†’ è½‰ç‚ºå­—ç¬¦ä¸²
                safe_metadata[key] = str(value)
        
        return safe_metadata

    def _process_documents_individually(self, vectorstore, documents: List[Document]) -> int:
        """å–®å€‹è™•ç†æ–‡æª”"""
        success_count = 0
        
        for i, doc in enumerate(documents):
            try:
                doc_tokens = doc.metadata.get('token_count', 0)
                if doc_tokens > TOKEN_LIMITS['max_tokens_per_request']:
                    print(f"         æ–‡æª” {i+1} ä»ç„¶éå¤§ ({doc_tokens:,} tokens)ï¼Œè·³é")
                    continue
                
                vectorstore.add_documents([doc])
                success_count += 1
                print(f"         å–®å€‹æ–‡æª” {i+1}/{len(documents)} å®Œæˆ")
                time.sleep(1)  # å–®å€‹è™•ç†æ™‚çŸ­æš«å»¶é²
                
            except Exception as e:
                print(f"         å–®å€‹æ–‡æª” {i+1} å¤±æ•—: {e}")
        
        return success_count

    def incremental_update(self, collection_name: str, added_files: List[Path], 
                          modified_files: List[Path], deleted_files: List[str],
                          current_files: Dict[str, FileInfo]) -> bool:
        """ğŸš€ å„ªåŒ–ç‰ˆå¢é‡æ›´æ–°"""
        with self.processing_lock:
            try:
                vectorstore = self.get_or_create_vectorstore(collection_name)
                
                # è™•ç†åˆªé™¤å’Œä¿®æ”¹
                files_to_delete = deleted_files + [str(f) for f in modified_files]
                if files_to_delete:
                    for file_path in files_to_delete:
                        try:
                            vectorstore.delete(filter={"source": file_path})
                            print(f"å·²åˆªé™¤: {Path(file_path).name}")
                        except Exception as e:
                            logger.warning(f"åˆªé™¤æ–‡æª”å¤±æ•— {file_path}: {e}")
                
                # è™•ç†æ–°å¢å’Œä¿®æ”¹çš„æ–‡ä»¶
                files_to_process = added_files + modified_files
                if not files_to_process:
                    print("   ç„¡æ–°æ–‡ä»¶éœ€è¦è™•ç†")
                    return True
                
                print(f"è™•ç† {len(files_to_process)} å€‹æ–‡ä»¶...")
                print(f"   è™•ç†å¤§æ–‡ä»¶å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å¾…...")
                
                # ä¸¦ç™¼è¼‰å…¥æ–‡æª”
                all_documents = []
                if PERFORMANCE_CONFIG.get("parallel_processing", True):
                    all_documents = self._parallel_load_documents(files_to_process, collection_name)
                else:
                    all_documents = self._sequential_load_documents(files_to_process, collection_name)
                
                if not all_documents:
                    print("   æ²’æœ‰æœ‰æ•ˆæ–‡æª”éœ€è¦å‘é‡åŒ–")
                    return True
                
                # çµ±è¨ˆå’Œæˆæœ¬ä¼°ç®—
                total_tokens = sum(doc.metadata.get('token_count', 0) for doc in all_documents)
                estimated_cost = self.batch_processor.token_estimator.estimate_embedding_cost(total_tokens)
                
                print(f"\nå‘é‡åŒ–çµ±è¨ˆ:")
                print(f"   ç¸½åˆ†å¡Šæ•¸: {len(all_documents)}")
                print(f"   ç¸½tokens: {total_tokens:,}")
                print(f"   ä¼°ç®—æˆæœ¬: ${estimated_cost:.4f}")
                
                # å‰µå»ºæ™ºèƒ½æ‰¹æ¬¡ä¸¦è™•ç†
                batches = self.batch_processor.create_smart_batches(all_documents)
                success_count = self._process_batches(vectorstore, batches)
                
                print(f"\nå‘é‡åŒ–å®Œæˆï¼")
                print(f"   æˆåŠŸ: {success_count}/{len(all_documents)} å€‹åˆ†å¡Š")
                print(f"   æˆåŠŸç‡: {(success_count/len(all_documents)*100):.1f}%")
                
                # æ›´æ–°æ–‡ä»¶è¨˜éŒ„
                self.file_records[collection_name] = current_files
                self._save_file_records()
                
                # è¨˜æ†¶é«”æ¸…ç†
                if success_count % PERFORMANCE_CONFIG.get("gc_frequency", 50) == 0:
                    gc.collect()
                
                return success_count > 0
                
            except Exception as e:
                logger.error(f"å¢é‡æ›´æ–°å¤±æ•— {collection_name}: {e}")
                print(f"âŒ ç³»çµ±éŒ¯èª¤: {e}")
                return False

    def get_collection_name(self, dir_path: Path) -> str:
        """ç²å–é›†åˆåç¨±"""
        try:
            relative_path = dir_path.relative_to(self.data_dir)
            if relative_path.parts:
                return f"collection_{relative_path.parts[0]}"
        except ValueError:
            pass
        return "collection_other"

    def sync_collections(self) -> int:
        """ç´”PostgreSQLæ–¹æ¡ˆï¼šä¸å†æƒææœ¬åœ°ç›®éŒ„"""
        print("ç´”PostgreSQLæ–¹æ¡ˆï¼šè·³éæœ¬åœ°ç›®éŒ„æƒæ")
        print("æ‰€æœ‰æ•¸æ“šéƒ½åœ¨PostgreSQLä¸­ï¼Œç„¡éœ€åŒæ­¥")
        return 0

    def scan_directory_changes(self, dir_path: Path, collection_name: str) -> Tuple[List[Path], List[Path], List[str], Dict[str, FileInfo]]:
        """æƒæç›®éŒ„è®Šæ›´ - ä¿®æ­£ç‰ˆï¼šæ­£ç¢ºè™•ç†ä¸Šå‚³æª”æ¡ˆ"""
        current_files = {}
        
        print(f"æƒæç›®éŒ„: {dir_path}")
        
        # éæ­¸æƒæç›®éŒ„
        file_count = 0
        for file_path in dir_path.rglob('*'):
            if (
                file_path.is_file() and 
                file_path.suffix.lower() in SUPPORTED_EXTENSIONS and
                not file_path.name.startswith('.') and
                file_path.stat().st_size > 0
            ):  # è·³éç©ºæ–‡ä»¶
                
                file_info = self.get_file_info(file_path)
                if file_info:
                    # ä¿®æ­£ï¼šä½¿ç”¨æ¨™æº–åŒ–çš„çµ•å°è·¯å¾‘ä½œç‚ºéµå€¼
                    try:
                        # ä½¿ç”¨absolute()é¿å…ç¬¦è™Ÿé€£çµå•é¡Œ
                        absolute_path = str(file_path.absolute())
                        current_files[absolute_path] = file_info
                        file_count += 1
                    except Exception as e:
                        logger.warning(f"è·¯å¾‘æ¨™æº–åŒ–å¤±æ•— {file_path}: {e}")
                        # å›é€€åˆ°åŸå§‹è·¯å¾‘
                        current_files[str(file_path)] = file_info
                        file_count += 1
        
        print(f"æ‰¾åˆ° {file_count} å€‹æœ‰æ•ˆæª”æ¡ˆ")
        
        old_files = self.file_records.get(collection_name, {})
        print(f"èˆŠè¨˜éŒ„ä¸­æœ‰ {len(old_files)} å€‹æª”æ¡ˆ")
        
        # ä¿®æ­£ï¼šæ­£è¦åŒ–èˆŠè¨˜éŒ„çš„è·¯å¾‘éµå€¼
        normalized_old_files = {}
        normalization_errors = 0
        
        for old_path, old_info in old_files.items():
            try:
                old_path_obj = Path(old_path)
                
                if old_path_obj.is_absolute():
                    # å·²ç¶“æ˜¯çµ•å°è·¯å¾‘
                    normalized_key = str(old_path_obj.absolute())
                else:
                    # ç›¸å°è·¯å¾‘è½‰çµ•å°è·¯å¾‘
                    try:
                        abs_path = (dir_path / old_path).absolute()
                        normalized_key = str(abs_path)
                    except Exception:
                        # å¦‚æœç„¡æ³•è½‰æ›ï¼Œä¿æŒåŸæ¨£
                        normalized_key = old_path
                        
                normalized_old_files[normalized_key] = old_info
                
            except Exception as e:
                logger.warning(f"èˆŠè·¯å¾‘æ­£è¦åŒ–å¤±æ•— {old_path}: {e}")
                # ä¿æŒåŸè·¯å¾‘
                normalized_old_files[old_path] = old_info
                normalization_errors += 1
        
        if normalization_errors > 0:
            print(f"{normalization_errors} å€‹èˆŠè·¯å¾‘æ­£è¦åŒ–å¤±æ•—")
        
        # ä¿®æ­£ï¼šæ™ºèƒ½è®Šæ›´æª¢æ¸¬
        added_files = []
        modified_files = []
        
        print("æª¢æ¸¬è®Šæ›´...")
        
        for file_path, file_info in current_files.items():
            current_file_name = Path(file_path).name
            current_hash = file_info.hash
            
            # é¦–å…ˆå˜—è©¦ç²¾ç¢ºè·¯å¾‘åŒ¹é…
            if file_path in normalized_old_files:
                old_info = normalized_old_files[file_path]
                if old_info.hash != current_hash:
                    modified_files.append(Path(file_path))
                    print(f"ä¿®æ”¹æª”æ¡ˆ: {current_file_name}")
            else:
                # æ™ºèƒ½æª”æ¡ˆåŒ¹é…ï¼šæª¢æŸ¥æ˜¯å¦æ˜¯åŒä¸€æª”æ¡ˆçš„ä¸åŒè·¯å¾‘è¡¨ç¤º
                file_found = False
                
                for old_path, old_info in normalized_old_files.items():
                    old_file_name = Path(old_path).name
                    
                    # æª”æ¡ˆåç›¸åŒä¸”å“ˆå¸Œç›¸åŒ = åŒä¸€æª”æ¡ˆ
                    if (
                        current_file_name == old_file_name and 
                        current_hash == old_info.hash
                    ):
                        file_found = True
                        print(f"è·¯å¾‘è®Šæ›´ä½†å…§å®¹ç›¸åŒ: {current_file_name}")
                        break
                        
                    # æª”æ¡ˆåç›¸åŒä½†å“ˆå¸Œä¸åŒ = æª”æ¡ˆè¢«ä¿®æ”¹
                    elif (
                        current_file_name == old_file_name and 
                        current_hash != old_info.hash
                    ):
                        modified_files.append(Path(file_path))
                        file_found = True
                        print(f"ä¿®æ”¹æª”æ¡ˆ (è·¯å¾‘è®Šæ›´): {current_file_name}")
                        break
                
                if not file_found:
                    added_files.append(Path(file_path))
                    print(f"æ–°æª”æ¡ˆ: {current_file_name}")
        
        # ä¿®æ­£ï¼šæ™ºèƒ½åˆªé™¤æª¢æ¸¬
        deleted_files = []
        
        for old_path in normalized_old_files.keys():
            old_file_name = Path(old_path).name
            
            if old_path not in current_files:
                # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦çœŸçš„ä¸å­˜åœ¨ï¼ˆå¯èƒ½åªæ˜¯è·¯å¾‘è¡¨ç¤ºä¸åŒï¼‰
                file_still_exists = False
                
                for current_path in current_files.keys():
                    current_file_name = Path(current_path).name
                    if current_file_name == old_file_name:
                        # é€²ä¸€æ­¥æª¢æŸ¥æ˜¯å¦æ˜¯åŒä¸€æª”æ¡ˆï¼ˆé€šéå…§å®¹å“ˆå¸Œï¼‰
                        current_hash = current_files[current_path].hash
                        old_hash = normalized_old_files[old_path].hash
                        
                        if current_hash == old_hash:
                            file_still_exists = True
                            break
                
                if not file_still_exists:
                    deleted_files.append(old_path)
                    print(f"åˆªé™¤æª”æ¡ˆ: {old_file_name}")
        
        print(f"è®Šæ›´çµ±è¨ˆ:")
        print(f"   æ–°å¢: {len(added_files)}")
        print(f"   ä¿®æ”¹: {len(modified_files)}")
        print(f"   åˆªé™¤: {len(deleted_files)}")
        
        return added_files, modified_files, deleted_files, current_files


# âœ… ç¢ºä¿æ‰€æœ‰importéƒ½èƒ½æ­£å¸¸å·¥ä½œ
__all__ = ['VectorOperationsCore']