#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
chatbot_instance.py - (APIç›¸å®¹ç‰ˆ)

ä¿®å¾©å…§å®¹ï¼š
- __init__ æ–¹æ³•ç¾åœ¨ç›´æ¥æ¥æ”¶ä¸€å€‹ config å­—å…¸ï¼Œè€Œä¸æ˜¯ bot_nameã€‚
- ç§»é™¤äº†å¾æª”æ¡ˆç³»çµ±è®€å– JSON è¨­å®šçš„ _load_config æ–¹æ³•ã€‚

"""

from dotenv import load_dotenv
load_dotenv(override=True)

import os
import json
import argparse
import sys
import re
import time
from pathlib import Path
import logging
from typing import Tuple, List, Dict, Optional

from fastapi import FastAPI, Request, Depends, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

from auth_middleware import OptionalAuth, User, JWTManager
from conversation_logger_simple import EnhancedConversationLogger as ConversationLogger


import sys
from pathlib import Path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
from config import app_config  # â­çµ±ä¸€å°å…¥

# ğŸ”§ æª¢æŸ¥APIæ¨¡å¼ä¾è³´
try:
    import httpx
    HTTPX_AVAILABLE = True
    print("âœ… httpx å¯ç”¨ï¼Œæ”¯æ´å‘é‡APIæ¨¡å¼")
except ImportError:
    HTTPX_AVAILABLE = False
    print("âš ï¸ httpx ä¸å¯ç”¨ï¼Œåƒ…æ”¯æ´ç›´æ¥å‘é‡å­˜å–æ¨¡å¼")

# ğŸ”§ æª¢æŸ¥å‘é‡ç³»çµ±
try:
    from vector_builder_langchain import OptimizedVectorSystem
    VECTOR_SYSTEM_AVAILABLE = True
    print("âœ… å‘é‡ç³»çµ±å¯ç”¨")
except ImportError:
    VECTOR_SYSTEM_AVAILABLE = False
    print("âŒ å‘é‡ç³»çµ±ä¸å¯ç”¨")

try:
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage, SystemMessage
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

ROOT_DIR = Path(__file__).parent
# ğŸ—‘ï¸ ç§»é™¤ï¼šä¸å†éœ€è¦å¾æª”æ¡ˆç³»çµ±è®€å–è¨­å®š
# BOT_CONFIGS_DIR = ROOT_DIR / "bot_configs"

class ChatbotInstance:
    # âœ¨ é—œéµæ›´å‹•ï¼š__init__ ç¾åœ¨æ¥æ”¶ä¸€å€‹å®Œæ•´çš„ config å­—å…¸
    def __init__(self, config: dict, **kwargs):
        if not config or not isinstance(config, dict):
            raise ValueError("A valid configuration dictionary must be provided.")
        
        self.config = config
        self.bot_name = self.config.get("bot_name")
        if not self.bot_name:
            raise ValueError("Config dictionary must contain a 'bot_name'.")

        self.collection_name = f"collection_{self.bot_name}"
        
        # ğŸ”§ å‘é‡APIé…ç½®
        self.vector_api_url = app_config.get_vector_api_url()
        self.use_api_mode = (
            HTTPX_AVAILABLE and 
            os.getenv("USE_VECTOR_API", "false").lower() == "true"
        )
        
        # ğŸ”§ å‘é‡ç³»çµ±åˆå§‹åŒ– - ä¿æŒåŸæœ‰é‚è¼¯
        self.vector_system = None
        if VECTOR_SYSTEM_AVAILABLE:
            try:
                self.vector_system = OptimizedVectorSystem()
                logger.info(f"âœ… å‘é‡ç³»çµ±åˆå§‹åŒ–æˆåŠŸ (æ©Ÿå™¨äºº: {self.bot_name})")
            except Exception as e:
                logger.error(f"å‘é‡ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
                self.vector_system = None
        
        # ğŸ”§ æ¨¡å¼é¸æ“‡é‚è¼¯
        if self.use_api_mode and HTTPX_AVAILABLE:
            logger.info(f"ğŸ”— ä½¿ç”¨å‘é‡APIæ¨¡å¼: {self.vector_api_url}")
            self.search_mode = "api"
        elif self.vector_system:
            logger.info("ğŸ”§ ä½¿ç”¨ç›´æ¥å‘é‡å­˜å–æ¨¡å¼")
            self.search_mode = "direct"
        else:
            logger.warning("âš ï¸ å‘é‡æœå°‹åŠŸèƒ½ä¸å¯ç”¨")
            self.search_mode = "disabled"
        
        # æ¯å€‹æ©Ÿå™¨äººå¯¦ä¾‹ä½¿ç”¨ç¨ç«‹çš„å°è©±è¨˜éŒ„è³‡æ–™åº«
        db_config = self._get_db_config()
        self.logger = ConversationLogger(db_config=db_config)
        
        self.app = FastAPI(title=f"{self.bot_name} Chatbot")
        try:
            from starlette.middleware.proxy_headers import ProxyHeadersMiddleware
            self.app.add_middleware(ProxyHeadersMiddleware, trusted_hosts="*")
            logger.info("âœ… ProxyHeadersMiddleware å·²å•Ÿç”¨")
        except ImportError:
            logger.info("â„¹ï¸ ProxyHeadersMiddleware ä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°é–‹ç™¼æ¨¡å¼")
        except Exception as e:
            logger.warning(f"âš ï¸ ProxyHeadersMiddleware å•Ÿç”¨å¤±æ•—: {e}")


        # CORS æ”¯æ´
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
        
        # æª¢æŸ¥ä¸¦å»ºç«‹ç¯„æœ¬ç›®éŒ„
        self.template_dir = ROOT_DIR / "chatbot_templates" / "modern"
        self.static_dir = self.template_dir / "static"
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        self.template_dir.mkdir(parents=True, exist_ok=True)
        self.static_dir.mkdir(parents=True, exist_ok=True)
        
        # æª¢æŸ¥å¿…è¦æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        chat_html_path = self.template_dir / "chat.html"
        if not chat_html_path.exists():
            logger.warning(f"âš ï¸ ç¯„æœ¬æª”æ¡ˆä¸å­˜åœ¨: {chat_html_path}")
            logger.warning("è«‹ç¢ºä¿ chat.html ä½æ–¼ chatbot_templates/modern/ ç›®éŒ„ä¸­")
        
        self.modern_templates = Jinja2Templates(directory=str(self.template_dir))
        
        # éœæ…‹æª”æ¡ˆæ›è¼‰æª¢æŸ¥
        if self.static_dir.exists():
            self.app.mount("/modern/static", StaticFiles(directory=str(self.static_dir)), name="modern_static")
            logger.info(f"âœ… éœæ…‹æª”æ¡ˆæ›è¼‰æˆåŠŸ: {self.static_dir}")
        else:
            logger.warning(f"âš ï¸ éœæ…‹æª”æ¡ˆç›®éŒ„ä¸å­˜åœ¨: {self.static_dir}")
        
        self.session_counters: Dict[str, int] = {}
        
        # æ–°å¢çµ±è¨ˆè³‡è¨Š
        self.total_conversations = 0
        self.successful_conversations = 0
        self.failed_conversations = 0
        # è¨˜éŒ„æ¯å€‹ session å·²é¡¯ç¤ºçš„é€£ç·š
        self.session_shown_links = {}
        
        self.setup_routes()
        logger.info(f"âœ… æ©Ÿå™¨äººå¯¦ä¾‹ '{self.bot_name}' åˆå§‹åŒ–å®Œæˆï¼Œå°è©±è¨˜éŒ„è³‡æ–™åº«é…ç½®é¡å‹ï¼š{db_config.get('type')}")

    # ğŸ—‘ï¸ ç§»é™¤ï¼šä¸å†éœ€è¦å¾æª”æ¡ˆç³»çµ±è®€å–è¨­å®š
    # def _load_config(self) -> dict:
    #     ...

    def _get_db_config(self) -> dict:
        """
        ç²å–è³‡æ–™åº«è¨­å®šã€‚
        æ­¤ç‰ˆæœ¬å¼·åˆ¶è¦æ±‚ä½¿ç”¨ PostgreSQLï¼Œä¸¦é€é DATABASE_URL é€²è¡Œè¨­å®šã€‚
        """
        database_url = os.getenv("DATABASE_URL")

        if not database_url:
            error_msg = "ç’°å¢ƒè®Šæ•¸ DATABASE_URL æœªè¨­å®šã€‚å°è©±ç´€éŒ„åŠŸèƒ½éœ€è¦ PostgreSQLè³‡æ–™åº«ã€‚"
            logger.error(f"âŒ æ©Ÿå™¨äºº ' {self.bot_name}' - {error_msg}")
            raise ValueError(error_msg)

        logger.info(f"âœ… æ©Ÿå™¨äºº ' {self.bot_name}' å°‡ä½¿ç”¨ PostgreSQL (é€é DATABASE_URL)è¨˜éŒ„å°è©±ã€‚")
        return {
            "type": "postgresql",
            "connection_string": database_url
        }
  
    def create_anonymous_user(self, request: Request) -> User:
        """å»ºç«‹åŒ¿åä½¿ç”¨è€…ç‰©ä»¶"""
        client_ip = request.client.host if request.client else "unknown"
        return User(
            id=0, 
            username=f"anonymous_{client_ip}", 
            role="anonymous", 
            email="anonymous@example.com", 
            is_active=True,
            password_hash=""
        )

    def get_user_identifier(self, user: Optional[User], session_id: str) -> str:
        """ç²å–ä½¿ç”¨è€…å”¯ä¸€è­˜åˆ¥ç¢¼ï¼Œç”¨æ–¼å°è©±è¨˜éŒ„"""
        if user and user.id:
            return f"auth_user_{user.id}_{user.username}"
        else:
            return f"anonymous_session_{session_id}"

    def _get_document_content(self, doc) -> str:
        """çµ±ä¸€ç²å–æ–‡ä»¶å…§å®¹çš„æ–¹æ³•"""
        # å˜—è©¦å¸¸è¦‹çš„å…§å®¹å±¬æ€§åç¨±
        for attr in ['page_content', 'content', 'text']:
            if hasattr(doc, attr):
                content = getattr(doc, attr)
                if content:
                    return str(content)
        
        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
        if isinstance(doc, dict):
            for key in ['page_content', 'content', 'text']:
                if key in doc and doc[key]:
                    return str(doc[key])
        
        # å¦‚æœéƒ½æ²’æœ‰ï¼Œè¿”å›å­—ä¸²è¡¨ç¤º
        return str(doc)

    # ğŸ”§ æ–°å¢ï¼šé€éAPIæœå°‹å‘é‡
    async def _search_vectors_via_api(self, query: str, k: int = 3):
        """é€éAPIæœå°‹å‘é‡"""
        if not HTTPX_AVAILABLE:
            logger.warning("httpx ä¸å¯ç”¨ï¼Œç„¡æ³•ä½¿ç”¨APIæ¨¡å¼")
            return []
            
        try:
            timeout = httpx.Timeout(30.0, connect=5.0)
            async with httpx.AsyncClient(timeout=timeout) as client:
                response = await client.post(
                    f"{self.vector_api_url}/api/search",
                    json={
                        "query": query,
                        "collection_name": self.collection_name,
                        "k": k
                    }
                )
                
                if response.status_code == 200:
                    response_data = response.json() # Safely get the list
                    api_results = response_data.get("results", []) 
                    
                    # è½‰æ›ç‚ºç›¸å®¹åŸæœ‰ç¨‹å¼ç¢¼çš„æ–‡ä»¶æ ¼å¼
                    documents = []
                    if api_results: # Check if there are any results
                        for result in api_results:
                            # å»ºç«‹ç›¸å®¹çš„æ–‡ä»¶ç‰©ä»¶
                            doc = type('Document', (), {
                                'page_content': result.get('content', ''),
                                'metadata': result.get('metadata', {})
                            })()
                            documents.append(doc)
                    
                    logger.info(f"APIæœå°‹æˆåŠŸ: {len(documents)} å€‹çµæœ")
                    return documents
                    
                elif response.status_code == 503:
                    logger.warning("å‘é‡APIæœå‹™ä¸å¯ç”¨")
                    return []
                else:
                    logger.error(f"APIæœå°‹å¤±æ•—: {response.status_code} - {response.text}")
                    return []
                    
        except httpx.TimeoutException:
            logger.error("å‘é‡æœå°‹APIé€¾æ™‚")
            return []
        except httpx.ConnectError:
            logger.error("ç„¡æ³•é€£ç·šåˆ°å‘é‡APIæœå‹™")
            return []
        except Exception as e:
            logger.error(f"å‘é‡æœå°‹APIå‘¼å«ç•°å¸¸: {e}")
            return []

    # ğŸ”§ ä¿æŒåŸæœ‰çš„ç›´æ¥æœå°‹æ–¹æ³•
    def _search_vectors_direct(self, query: str, k: int = 3):
        """ç›´æ¥æœå°‹å‘é‡"""
        if not self.vector_system:
            logger.warning("å‘é‡ç³»çµ±ä¸å¯ç”¨")
            return []
        
        try:
            return self.vector_system.search(query, collection_name=self.collection_name, k=k)
        except Exception as e:
            logger.error(f"ç›´æ¥å‘é‡æœå°‹å¤±æ•—: {e}")
            return []

    # ğŸ”§ æ™ºæ…§å‘é‡æœå°‹æ–¹æ³•
    async def _search_vectors_smart(self, query: str, k: int = 3):
        """æ™ºæ…§é¸æ“‡æœå°‹æ–¹å¼"""
        context_docs = []
        
        # ä¸»è¦æ¨¡å¼å˜—è©¦
        if self.search_mode == "api":
            context_docs = await self._search_vectors_via_api(query, k)
            search_method = "API"
        elif self.search_mode == "direct":
            context_docs = self._search_vectors_direct(query, k)
            search_method = "ç›´æ¥"
        else:
            logger.warning("å‘é‡æœå°‹åŠŸèƒ½ä¸å¯ç”¨")
            return []
        
        # å¦‚æœä¸»è¦æ¨¡å¼å¤±æ•—ï¼Œå˜—è©¦å‚™ç”¨æ¨¡å¼
        if not context_docs:
            if self.search_mode == "api" and self.vector_system:
                logger.warning("APIæ¨¡å¼ç„¡çµæœï¼Œå˜—è©¦ç›´æ¥æ¨¡å¼...")
                context_docs = self._search_vectors_direct(query, k)
                if context_docs:
                    search_method = "ç›´æ¥(å‚™ç”¨)"
                    logger.info(f"âœ… ç›´æ¥æ¨¡å¼å‚™ç”¨æˆåŠŸ: {len(context_docs)} å€‹æ–‡ä»¶")
            elif self.search_mode == "direct" and HTTPX_AVAILABLE:
                logger.warning("ç›´æ¥æ¨¡å¼ç„¡çµæœï¼Œå˜—è©¦APIæ¨¡å¼...")
                context_docs = await self._search_vectors_via_api(query, k)
                if context_docs:
                    search_method = "API(å‚™ç”¨)"
                    logger.info(f"âœ… APIæ¨¡å¼å‚™ç”¨æˆåŠŸ: {len(context_docs)} å€‹æ–‡ä»¶")
        
        if context_docs:
            logger.info(f"ğŸ” æ©Ÿå™¨äºº '{self.bot_name}' {search_method}æª¢ç´¢çµæœ: {len(context_docs)} å€‹æ–‡ä»¶")
        else:
            logger.warning(f"ğŸ” æ©Ÿå™¨äºº '{self.bot_name}' æœªæ‰¾åˆ°ç›¸é—œæ–‡ä»¶")
        
        return context_docs

    # ä¿æŒåŸæœ‰çš„å…¶ä»–æ–¹æ³•ä¸è®Š
    def get_chunk_info_safely(self, search_item, fallback_index: int) -> dict:
        """å®‰å…¨åœ°å¾æœå°‹çµæœä¸­ç²å–chunkè³‡è¨Š"""
        try:
            chunk_info = {
                'chunk_id': f'chunk_{fallback_index}',
                'chunk_index': fallback_index,
                'content': '',
                'source': 'unknown',
                'filename': 'unknown'
            }
            
            # æ–¹æ³•1: è™•ç†ä¸åŒé¡å‹çš„æœå°‹çµæœ
            if hasattr(search_item, 'page_content'):
                # LangChain Document ç‰©ä»¶
                chunk_info['content'] = search_item.page_content
                if hasattr(search_item, 'metadata') and search_item.metadata:
                    metadata = search_item.metadata
                    chunk_info['chunk_id'] = metadata.get('chunk_id', f'chunk_{fallback_index}')
                    chunk_info['chunk_index'] = metadata.get('chunk_index', fallback_index)
                    chunk_info['source'] = metadata.get('source', 'unknown')
                    chunk_info['filename'] = metadata.get('filename', metadata.get('original_filename', 'unknown'))
            
            elif isinstance(search_item, dict):
                # å­—å…¸æ ¼å¼çš„æœå°‹çµæœ
                chunk_info['content'] = search_item.get('content', search_item.get('page_content', ''))
                metadata = search_item.get('metadata', {})
                chunk_info['chunk_id'] = metadata.get('chunk_id', f'chunk_{fallback_index}')
                chunk_info['chunk_index'] = metadata.get('chunk_index', fallback_index)
                chunk_info['source'] = metadata.get('source', 'unknown')
                chunk_info['filename'] = metadata.get('filename', metadata.get('original_filename', 'unknown'))
            
            elif hasattr(search_item, 'content'):
                # è‡ªè¨‚æœå°‹çµæœç‰©ä»¶
                chunk_info['content'] = str(search_item.content)
                if hasattr(search_item, 'metadata'):
                    metadata = search_item.metadata
                    chunk_info['chunk_id'] = metadata.get('chunk_id', f'chunk_{fallback_index}')
                    chunk_info['chunk_index'] = metadata.get('chunk_index', fallback_index)
                    chunk_info['source'] = metadata.get('source', 'unknown')
                    chunk_info['filename'] = metadata.get('filename', 'unknown')
            
            else:
                # å‚™ç”¨ï¼šå°‡æ•´å€‹ç‰©ä»¶è½‰ç‚ºå­—ä¸²
                chunk_info['content'] = str(search_item)
            
            return chunk_info
            
        except Exception as e:
            logger.warning(f"ç²å–chunkè³‡è¨Šå¤±æ•—: {e}")
            return {
                'chunk_id': f'chunk_{fallback_index}',
                'chunk_index': fallback_index,
                'content': str(search_item) if search_item else '',
                'source': 'unknown',
                'filename': 'unknown'
            }

    def _find_real_chunk_index(self, content: str, fallback_index: int) -> int:
        """å°‹æ‰¾å…§å®¹åœ¨å‘é‡åº«ä¸­çš„çœŸå¯¦ç´¢å¼•"""
        try:
            if self.vector_system:
                vectorstore = self.vector_system.get_or_create_vectorstore(self.collection_name)
                all_docs = vectorstore.get()
                
                if all_docs and all_docs.get('documents'):
                    # å°‹æ‰¾å®Œå…¨ç¬¦åˆçš„å…§å®¹
                    for idx, doc_content in enumerate(all_docs['documents']):
                        if doc_content.strip() == content.strip():
                            return idx
                    
                    # å¦‚æœæ²’æœ‰å®Œå…¨ç¬¦åˆï¼Œå°‹æ‰¾éƒ¨åˆ†ç¬¦åˆ
                    for idx, doc_content in enumerate(all_docs['documents']):
                        if content[:100] in doc_content or doc_content[:100] in content:
                            return idx
            
            # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›å‚™ç”¨ç´¢å¼•
            return fallback_index
            
        except Exception as e:
            logger.debug(f"å°‹æ‰¾çœŸå¯¦chunkç´¢å¼•å¤±æ•—: {e}")
            return fallback_index

    def _get_chunk_index_from_doc(self, doc, fallback_index: int) -> int:
        """å¾æ–‡ä»¶ç‰©ä»¶ä¸­ç²å– chunk çš„å¯¦éš›ç´¢å¼• - æ”¹é€²ç‰ˆ"""
        try:
            logger.debug(f"ğŸ” å˜—è©¦ç²å–æ–‡ä»¶çš„çœŸå¯¦ç´¢å¼•ï¼Œfallback: {fallback_index}")
            
            # æ–¹æ³•1: å¾ metadata ä¸­ç²å– - æ”¹é€²ç‰ˆ
            metadata = None
            if hasattr(doc, 'metadata') and doc.metadata:
                metadata = doc.metadata
            elif isinstance(doc, dict) and 'metadata' in doc:
                metadata = doc['metadata']
            
            if metadata:
                # ğŸ†• è¨˜éŒ„å…ƒè³‡æ–™å…§å®¹ç”¨æ–¼åµéŒ¯
                logger.debug(f"ğŸ“‹ æ–‡ä»¶å…ƒè³‡æ–™: {list(metadata.keys())}")
                
                # æ“´å±•æœå°‹æ¬„ä½
                index_fields = [
                    'chunk_index', 'index', 'chunk_id', 'id', 
                    'chunk_num', 'chunk_number', 'doc_index',
                    'document_index', 'position', 'seq'
                ]
                
                for index_field in index_fields:
                    if index_field in metadata:
                        value = metadata[index_field]
                        logger.debug(f"ğŸ“ æ‰¾åˆ°ç´¢å¼•æ¬„ä½ {index_field}: {value} (type: {type(value)})")
                        
                        if isinstance(value, int) and value >= 0:
                            logger.info(f"âœ… å¾ metadata.{index_field} ç²å–ç´¢å¼•: {value}")
                            return value
                        elif isinstance(value, str):
                            # å˜—è©¦å¤šç¨®å­—ä¸²æ ¼å¼
                            if value.startswith('chunk_'):
                                try:
                                    idx = int(value.split('_')[-1])
                                    logger.info(f"âœ… å¾ metadata.{index_field} è§£æç´¢å¼•: {idx}")
                                    return idx
                                except ValueError:
                                    continue
                            elif value.isdigit():
                                idx = int(value)
                                logger.info(f"âœ… å¾ metadata.{index_field} è½‰æ›ç´¢å¼•: {idx}")
                                return idx
            
            # æ–¹æ³•2: å¾æ–‡ä»¶IDä¸­æå–ç´¢å¼• - ä¿æŒä¸è®Š
            doc_id = None
            if hasattr(doc, 'id') and doc.id:
                doc_id = str(doc.id)
            elif isinstance(doc, dict) and 'id' in doc:
                doc_id = str(doc['id'])
            
            if doc_id:
                logger.debug(f"ğŸ“„ æ–‡ä»¶ID: {doc_id}")
                # å˜—è©¦å¤šç¨®IDæ ¼å¼
                patterns = [
                    r'_(\d+)$',           # ending with _123
                    r'chunk_(\d+)',       # chunk_123
                    r'doc_(\d+)',         # doc_123
                    r'-(\d+)$',           # ending with -123
                    r'(\d+)$'             # ending with 123
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, doc_id)
                    if match:
                        idx = int(match.group(1))
                        logger.info(f"âœ… å¾æ–‡ä»¶IDè§£æç´¢å¼•: {idx} (pattern: {pattern})")
                        return idx
            
            # æ–¹æ³•3: å¾å‘é‡è³‡æ–™åº«æŸ¥è©¢å¯¦éš›ä½ç½® - æ”¹é€²ç‰ˆ
            if self.vector_system:
                try:
                    logger.debug(f"ğŸ” å˜—è©¦å¾å‘é‡è³‡æ–™åº«æŸ¥è©¢å¯¦éš›ä½ç½®...")
                    vectorstore = self.vector_system.get_or_create_vectorstore(self.collection_name)
                    all_docs_result = vectorstore.get()
                    
                    if all_docs_result and all_docs_result.get('documents'):
                        all_documents = all_docs_result['documents']
                        doc_content = self._get_document_content(doc)
                        
                        # ğŸ†• æ”¹é€²ï¼šå˜—è©¦å¤šç¨®ç¬¦åˆç­–ç•¥
                        
                        # ç­–ç•¥1: å®Œå…¨ç¬¦åˆ
                        for idx, stored_doc_content in enumerate(all_documents):
                            if stored_doc_content.strip() == doc_content.strip():
                                logger.info(f"âœ… é€éå®Œå…¨ç¬¦åˆæ‰¾åˆ°ç´¢å¼•: {idx}")
                                return idx
                        
                        # ç­–ç•¥2: å‰100å­—å…ƒç¬¦åˆ
                        doc_prefix = doc_content[:100].strip()
                        if len(doc_prefix) > 20:  # ç¢ºä¿æœ‰è¶³å¤ çš„å…§å®¹é€²è¡Œç¬¦åˆ
                            for idx, stored_doc_content in enumerate(all_documents):
                                stored_prefix = stored_doc_content[:100].strip()
                                if doc_prefix == stored_prefix:
                                    logger.info(f"âœ… é€éå‰ç¶´ç¬¦åˆæ‰¾åˆ°ç´¢å¼•: {idx}")
                                    return idx
                        
                        # ç­–ç•¥3: é—œéµè©ç¬¦åˆï¼ˆå¦‚æœå…§å®¹è¼ƒçŸ­ï¼‰
                        if len(doc_content) < 500:
                            doc_words = set(doc_content.split())
                            if len(doc_words) > 3:  # è‡³å°‘è¦æœ‰å¹¾å€‹è©
                                best_match_idx = -1
                                best_match_score = 0
                                
                                for idx, stored_doc_content in enumerate(all_documents):
                                    stored_words = set(stored_doc_content.split())
                                    common_words = doc_words.intersection(stored_words)
                                    score = len(common_words) / len(doc_words.union(stored_words))
                                    
                                    if score > 0.8 and score > best_match_score:
                                        best_match_score = score
                                        best_match_idx = idx
                                
                                if best_match_idx >= 0:
                                    logger.info(f"âœ… é€éè©å½™ç¬¦åˆæ‰¾åˆ°ç´¢å¼•: {best_match_idx} (ç›¸ä¼¼åº¦: {best_match_score:.2f})")
                                    return best_match_idx
                    
                    logger.warning("âš ï¸ å‘é‡è³‡æ–™åº«æŸ¥è©¢ç„¡ç¬¦åˆçµæœ")
                            
                except Exception as query_error:
                    logger.warning(f"âš ï¸ æŸ¥è©¢å‘é‡è³‡æ–™åº«ç´¢å¼•å¤±æ•—: {query_error}")
            
            # æ–¹æ³•4: ä½¿ç”¨å‚™ç”¨ç´¢å¼•
            logger.warning(f"âš ï¸ æ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨ç´¢å¼•: {fallback_index}")
            return fallback_index
            
        except Exception as e:
            logger.error(f"âŒ ç²å– chunk ç´¢å¼•æ™‚ç™¼ç”Ÿç•°å¸¸: {e}")
            return fallback_index

    def setup_routes(self):
        """è¨­å®šè·¯ç”±"""
        
        @self.app.get("/", response_class=HTMLResponse)
        async def get_modern_chat_ui(request: Request):
            """ä¸»èŠå¤©ä»‹é¢"""
            try:
                display_name = self.config.get("display_name", self.bot_name)
                return self.modern_templates.TemplateResponse(
                    "chat.html", 
                    {
                        "request": request, 
                        "bot_name": self.bot_name,
                        "display_name": display_name
                    }
                )
            except Exception as e:
                logger.error(f"âŒ è¼‰å…¥èŠå¤©ä»‹é¢å¤±æ•—: {e}")
                return HTMLResponse(f"""
                <!DOCTYPE html>
                <html>
                <head><title>{self.bot_name}</title></head>
                <body>
                    <h1>èŠå¤©ä»‹é¢è¼‰å…¥å¤±æ•—</h1>
                    <p>éŒ¯èª¤ï¼š{e}</p>
                    <p>è«‹ç¢ºä¿ç¯„æœ¬æª”æ¡ˆæ˜¯å¦å­˜åœ¨æ–¼ chatbot_templates/modern/chat.html</p>
                </body>
                </html>
                """, status_code=500)

        @self.app.post("/api/chat")
        async def chat(request: Request, user: User = Depends(OptionalAuth)):
            """èŠå¤©APIç«¯é» - æ”¯æ´APIå’Œç›´æ¥å…©ç¨®æ¨¡å¼"""
            start_time = time.time()
            conversation_id = None
            chunk_references = []
            
            try:
                data = await request.json()
                query = data.get("message")
                session_id = data.get("session_id", "default_session")
                
                if not query:
                    raise HTTPException(status_code=400, detail="Message cannot be empty")
                
                # è™•ç†ä½¿ç”¨è€…è­˜åˆ¥
                if not user:
                    user = self.create_anonymous_user(request)
                
                user_identifier = self.get_user_identifier(user, session_id)
                
                # æ›´æ–°çµ±è¨ˆè³‡è¨Š
                self.total_conversations += 1
                if session_id not in self.session_counters:
                    self.session_counters[session_id] = 0
                self.session_counters[session_id] += 1
                self._current_query = query
                logger.info(f"ğŸ“© æ©Ÿå™¨äºº '{self.bot_name}' æ”¶åˆ°æŸ¥è©¢ [{user_identifier}]: {query[:50]}...")

                # ğŸ”§ ä¿®æ”¹ï¼šä½¿ç”¨æ™ºæ…§å‘é‡æœå°‹
                context_docs = await self._search_vectors_smart(query, k=3)
                
                # è©³ç´°åµéŒ¯æª¢ç´¢çµæœ
                logger.info(f"ğŸ” ç›®å‰æŸ¥è©¢: {query}")
                logger.info(f"ğŸ“„ æª¢ç´¢åˆ° {len(context_docs) if context_docs else 0} å€‹æ–‡ä»¶")

                if context_docs:
                    for i, doc in enumerate(context_docs):
                        content_preview = self._get_document_content(doc)[:100]
                        metadata = getattr(doc, 'metadata', {})
                        contained_urls = metadata.get('contained_urls', '')
                        
                        logger.info(f"ğŸ“„ æ–‡ä»¶ {i+1}:")
                        logger.info(f"  å…§å®¹é è¦½: {content_preview}")
                        logger.info(f"  åŒ…å«é€£ç·š: {contained_urls}")
                        logger.info(f"  ä¾†æºæª”æ¡ˆ: {metadata.get('filename', 'unknown')}")

                context = "\n".join([self._get_document_content(doc) for doc in context_docs]) if context_docs else ""
                
                # æº–å‚™ chunk å¼•ç”¨è³‡è¨Š
                retrieved_docs_content = []
                
                if context_docs:
                    for i, doc in enumerate(context_docs):
                        doc_content = self._get_document_content(doc)
                        retrieved_docs_content.append(doc_content)
                        
                        # ç²å–å…ƒè³‡æ–™
                        metadata = {}
                        if hasattr(doc, 'metadata') and doc.metadata:
                            metadata = doc.metadata
                        elif isinstance(doc, dict) and 'metadata' in doc:
                            metadata = doc['metadata']
                        
                        # ä¿®æ­£ï¼šç›´æ¥å¾ metadata ç²å–çœŸå¯¦çš„ chunk_id
                        chunk_id = metadata.get('chunk_id', f'fallback_{i}')

                        chunk_ref = {
                            "chunk_id": chunk_id,  # <-- ä½¿ç”¨çœŸå¯¦çš„ chunk_id
                            "index": i, # ä½¿ç”¨å¾ªç’°çš„ç´¢å¼• i ä½œç‚ºå‚™ç”¨
                            "content_preview": doc_content[:100] + "..." if len(doc_content) > 100 else doc_content,
                            "source": metadata.get('source', 'unknown'),
                            "filename": metadata.get('filename', metadata.get('original_filename', 'unknown'))
                        }
                        chunk_references.append(chunk_ref)

                logger.info(f"ğŸ” æ©Ÿå™¨äºº '{self.bot_name}' æª¢ç´¢çµæœ: {len(context_docs)} å€‹æ–‡ä»¶, chunk_refs: {len(chunk_references)}")
                
                # ç”Ÿæˆå›æ‡‰
                system_prompt = self.config.get("system_role", "ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„ AI åŠ©ç†ã€‚")
                response_text, recommended_questions = self._generate_response(
                    query, context, system_prompt, session_id
                )
                
                # è™•ç†å¼•ç”¨ä¾†æº
                logger.info(f"[Cite Sources] Checking... Enabled: {self.config.get('cite_sources_enabled', False)}, Docs found: {bool(context_docs)}")
                if self.config.get("cite_sources_enabled", False) and context_docs:
                    all_sources = self._extract_source_urls(context_docs)
                    logger.info(f"[Cite Sources] _extract_source_urls returned {len(all_sources)} potential sources.")

                    # åªè™•ç†åŒ…å«çœŸå¯¦URLçš„ä¾†æº
                    url_sources = [s for s in all_sources if s.get("url")]
                    logger.info(f"[Cite Sources] Found {len(url_sources)} items with a 'url' key.")
                    
                    if url_sources:
                        # éæ¿¾é‡è¤‡é€£ç·š
                        url_sources = self._filter_duplicate_links(url_sources, session_id)
                        logger.info(f"[Cite Sources] After filtering duplicates, {len(url_sources)} sources remain.")
                        
                        if url_sources:  # ç¢ºä¿éæ¿¾å¾Œé‚„æœ‰é€£ç·š
                            response_text += self._format_source_links(url_sources)
                            logger.info(f"ğŸ”— æ–°å¢äº† {len(url_sources)} å€‹åƒè€ƒé€£çµ")
                        else:
                            logger.info("ğŸ”„ æ‰€æœ‰é€£ç·šéƒ½æ˜¯é‡è¤‡çš„ï¼Œè·³éé¡¯ç¤º")
                    else:
                        logger.info("ğŸ“ æœªåœ¨æ–‡ä»¶å…§å®¹ä¸­æ‰¾åˆ°å¯å¼•ç”¨çš„URL")

                processing_time_ms = int((time.time() - start_time) * 1000)
                
                # è¨˜éŒ„å°è©±
                try:
                    conversation_id = self.logger.log_conversation(
                        user_id=user_identifier,
                        user_query=query,
                        ai_response=response_text,
                        collection_used=self.collection_name if context_docs else None,
                        retrieved_docs=retrieved_docs_content[:3],
                        doc_similarities=[],
                        processing_time_ms=processing_time_ms,
                        is_image_generation=False,
                        error_occurred=False,
                        error_message=None,
                        authenticated_user_id=user.id if user and hasattr(user, 'id') and user.id else None,
                        user_role=user.role if user and hasattr(user, 'role') else "anonymous",
                        chunk_references=chunk_references
                    )
                    
                    self.successful_conversations += 1
                    logger.info(f"âœ… å°è©±è¨˜éŒ„æˆåŠŸï¼šID={conversation_id}, æ©Ÿå™¨äººï¼š{self.bot_name}, chunks={len(chunk_references)}")
                    
                except Exception as log_error:
                    logger.error(f"âŒ è¨˜éŒ„å°è©±å¤±æ•—ï¼ˆæ©Ÿå™¨äººï¼š{self.bot_name}ï¼‰: {log_error}")

                logger.info(f"ğŸ“¤ æ©Ÿå™¨äºº '{self.bot_name}' API å›æ‡‰åµéŒ¯:")
                logger.info(f"  - response_text é•·åº¦: {len(response_text)}")
                logger.info(f"  - recommended_questions: {recommended_questions}")
                logger.info(f"  - æ‰¾åˆ°æ–‡ä»¶æ•¸é‡: {len(context_docs) if context_docs else 0}")
                logger.info(f"  - chunk_references: {len(chunk_references)}")
                logger.info(f"  - è™•ç†æ™‚é–“: {processing_time_ms}ms")

                return JSONResponse({
                    "response": response_text,
                    "recommended_questions": recommended_questions if recommended_questions else [],
                    "error": False,
                    "metadata": {
                        "bot_name": self.bot_name,
                        "session_id": session_id,
                        "processing_time_ms": processing_time_ms,
                        "documents_found": len(context_docs) if context_docs else 0,
                        "conversation_id": conversation_id,
                        "chunk_count": len(chunk_references),
                        "search_mode": self.search_mode
                    }
                })

            except HTTPException as http_exc:
                error_message = str(http_exc.detail)
                processing_time_ms = int((time.time() - start_time) * 1000)
                
                try:
                    user_identifier = self.get_user_identifier(user, session_id) if 'session_id' in locals() else "unknown"
                    query = data.get("message", "æœªçŸ¥æŸ¥è©¢") if 'data' in locals() else "æœªçŸ¥æŸ¥è©¢"
                    
                    conversation_id = self.logger.log_conversation(
                        user_id=user_identifier,
                        user_query=query,
                        ai_response=f"HTTPéŒ¯èª¤: {error_message}",
                        collection_used=None,
                        retrieved_docs=[],
                        doc_similarities=[],
                        processing_time_ms=processing_time_ms,
                        is_image_generation=False,
                        error_occurred=True,
                        error_message=error_message,
                        authenticated_user_id=user.id if user and hasattr(user, 'id') and user.id else None,
                        user_role=user.role if user and hasattr(user, 'role') else "anonymous"
                    )
                    
                    self.failed_conversations += 1
                    logger.error(f"âŒ HTTPéŒ¯èª¤è¨˜éŒ„ï¼š{conversation_id}, æ©Ÿå™¨äººï¼š{self.bot_name}")
                    
                except Exception as log_error:
                    logger.error(f"âŒ HTTPéŒ¯èª¤å°è©±è¨˜éŒ„å¤±æ•—ï¼ˆæ©Ÿå™¨äººï¼š{self.bot_name}ï¼‰: {log_error}")
                
                raise http_exc
                
            except Exception as e:
                error_message = str(e)
                processing_time_ms = int((time.time() - start_time) * 1000)
                
                logger.error(f"âŒ æ©Ÿå™¨äºº '{self.bot_name}' èŠå¤© API ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
                
                try:
                    user_identifier = self.get_user_identifier(user, session_id) if 'session_id' in locals() else "unknown"
                    query = data.get("message", "æœªçŸ¥æŸ¥è©¢") if 'data' in locals() else "æœªçŸ¥æŸ¥è©¢"
                    
                    conversation_id = self.logger.log_conversation(
                        user_id=user_identifier,
                        user_query=query,
                        ai_response=f"ç³»çµ±éŒ¯èª¤: {error_message}",
                        collection_used=None,
                        retrieved_docs=[],
                        doc_similarities=[],
                        processing_time_ms=processing_time_ms,
                        is_image_generation=False,
                        error_occurred=True,
                        error_message=error_message,
                        authenticated_user_id=user.id if user and hasattr(user, 'id') and user.id else None,
                        user_role=user.role if user and hasattr(user, 'role') else "anonymous"
                    )
                    
                    self.failed_conversations += 1
                    logger.error(f"âŒ ç³»çµ±éŒ¯èª¤è¨˜éŒ„ï¼š{conversation_id}, æ©Ÿå™¨äººï¼š{self.bot_name}")
                    
                except Exception as log_error:
                    logger.error(f"âŒ ç³»çµ±éŒ¯èª¤å°è©±è¨˜éŒ„å¤±æ•—ï¼ˆæ©Ÿå™¨äººï¼š{self.bot_name}ï¼‰: {log_error}")
                
                return JSONResponse({
                    "response": f"æŠ±æ­‰ï¼Œæ©Ÿå™¨äºº '{self.bot_name}' ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚",
                    "error": True,
                    "error_message": error_message,
                    "recommended_questions": [],
                    "metadata": {
                        "bot_name": self.bot_name,
                        "processing_time_ms": processing_time_ms,
                        "conversation_id": conversation_id,
                        "search_mode": self.search_mode
                    }
                }, status_code=500)

        @self.app.get("/api/stats")
        async def get_bot_stats():
            """ç²å–æ©Ÿå™¨äººçµ±è¨ˆè³‡è¨Š"""
            try:
                db_stats = self.logger.get_statistics() if hasattr(self.logger, 'get_statistics') else {}
                
                return JSONResponse({
                    "bot_name": self.bot_name,
                    "display_name": self.config.get("display_name", self.bot_name),
                    "collection_name": self.collection_name,
                    "db_type": "postgresql",
                    "search_mode": self.search_mode,
                    "vector_api_url": self.vector_api_url if self.search_mode == "api" else None,
                    "session_stats": {
                        "total_conversations": self.total_conversations,
                        "successful_conversations": self.successful_conversations,
                        "failed_conversations": self.failed_conversations,
                        "active_sessions": len(self.session_counters)
                    },
                    "db_stats": db_stats,
                    "config": {
                        "temperature": self.config.get("temperature", 0.7),
                        "max_tokens": self.config.get("max_tokens", 2000),
                        "dynamic_recommendations_enabled": self.config.get("dynamic_recommendations_enabled", False),
                        "cite_sources_enabled": self.config.get("cite_sources_enabled", False)
                    }
                })
            except Exception as e:
                logger.error(f"ç²å–æ©Ÿå™¨äººçµ±è¨ˆå¤±æ•—: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)

        @self.app.get("/health")
        async def health_check():
            """å¥åº·æª¢æŸ¥ç«¯é»"""
            return JSONResponse({
                "status": "healthy",
                "bot_name": self.bot_name,
                "display_name": self.config.get("display_name", self.bot_name),
                "search_mode": self.search_mode,
                "timestamp": time.time(),
                "conversation_count": self.total_conversations
            })

    def _generate_response(self, query: str, context: str, system_prompt: str, session_id: str) -> Tuple[str, List[str]]:
        """ç”Ÿæˆå›æ‡‰"""
        if not OPENAI_AVAILABLE:
            return "ç³»çµ± AI æ¨¡çµ„æœªè¼‰å…¥ã€‚", []

        openai_key = os.getenv("OPENAI_API_KEY")
        if not openai_key:
            return "ç³»çµ±é…ç½®éŒ¯èª¤ï¼šç¼ºå°‘ OpenAI API Keyã€‚", []

        llm = ChatOpenAI(
            model=self.config.get("model", "gpt-4o-mini"), 
            temperature=self.config.get("temperature", 0.7), 
            api_key=openai_key
        )

        # ç”Ÿæˆä¸»è¦å›ç­”
        main_answer_messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"""åƒè€ƒè³‡æ–™ï¼š
{context}

ä½¿ç”¨è€…å•é¡Œï¼š{query}""" )
        ]
        main_response = llm.invoke(main_answer_messages)
        main_answer = main_response.content.strip()

        # ç”Ÿæˆæ¨è–¦å•é¡Œ
        recommended_questions = []
        should_recommend = self.config.get("dynamic_recommendations_enabled", False)
        recommend_count = self.config.get("dynamic_recommendations_count", 0)
        conversation_count = self.session_counters.get(session_id, 1)

        logger.info(f"ğŸ” æ©Ÿå™¨äºº '{self.bot_name}' æ¨è–¦å•é¡ŒåµéŒ¯:")
        logger.info(f"  - should_recommend: {should_recommend}")
        logger.info(f"  - recommend_count: {recommend_count}")
        logger.info(f"  - conversation_count: {conversation_count}")

        if should_recommend and (recommend_count == 0 or conversation_count <= recommend_count):
            logger.info(f"âœ… æ©Ÿå™¨äºº '{self.bot_name}' é–‹å§‹ç”Ÿæˆæ¨è–¦å•é¡Œ...")
            
            recommend_prompt = f"""
**åŸå§‹å°è©±**
ä½¿ç”¨è€…å•ï¼šã€Œ{query}ã€
ä½ çš„å›ç­”ï¼šã€Œ{main_answer}ã€

---
**æŒ‡ä»¤**
æ ¹æ“šä»¥ä¸Šå°è©±ï¼Œç”Ÿæˆä¸‰å€‹ç›¸é—œçš„å»¶ä¼¸å•é¡Œã€‚

**æ ¼å¼**
- æ¯å€‹å•é¡Œä¸€è¡Œ
- ä¸è¦ç·¨è™Ÿ
- ä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—"""
            
            try:
                recommend_messages = [HumanMessage(content=recommend_prompt)]
                recommend_response = llm.invoke(recommend_messages)
                questions_text = recommend_response.content.strip()
                
                logger.info(f"ğŸ¤– æ©Ÿå™¨äºº '{self.bot_name}' LLM åŸå§‹å›æ‡‰:")
                logger.info(f"'{questions_text}'")
                
                questions_list = [q.strip() for q in questions_text.split('\n') if q.strip()]
                recommended_questions = list(dict.fromkeys(questions_list))
                
                logger.info(f"ğŸ“ æ©Ÿå™¨äºº '{self.bot_name}' è§£æå¾Œçš„æ¨è–¦å•é¡Œ: {recommended_questions}")
                
            except Exception as e:
                logger.error(f"âŒ æ©Ÿå™¨äºº '{self.bot_name}' ç”Ÿæˆæ¨è–¦å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
                recommended_questions = []
        else:
            logger.info(f"âŒ æ©Ÿå™¨äºº '{self.bot_name}' æ¨è–¦å•é¡Œæœªå•Ÿç”¨æˆ–è¶…å‡ºé™åˆ¶")

        return main_answer, recommended_questions
    
    def _extract_links_from_content(self, content: str) -> List[dict]:
        """å¾æ–‡ä»¶å…§å®¹ä¸­æå–é€£çµå’Œæ¨™é¡Œï¼ˆç©©å¥åˆ¤å®š URL/æ¨™é¡Œé †åºï¼‰- ä¿®å¾©ç‰ˆæœ¬"""
        links = []
        seen_urls = set()  # ğŸ”§ æ–°å¢ï¼šURLå»é‡é›†åˆ
        
        #æ­£è¦è¡¨ç¤ºå¼æ¨£ç‰ˆç¬¦åˆå„ç¨®é€£çµæ ¼å¼
        patterns = [
            # Markdownæ ¼å¼: [æ¨™é¡Œ](URL) -> (title, url)
            r'\[([^\]]+)\]\((https?://[^\s)]+)\)',
            # HTMLæ ¼å¼: <a href="URL">æ¨™é¡Œ</a> -> (url, title)
            r'<a[^>]+href=["\\]([^\"\\]+)["\\][^>]*>([^<]+)</a>',
            # ç´”æ–‡å­—æ ¼å¼: æ¨™é¡Œ: URL  -> (title, url)
            r'([^:\n]+):\s*(https?://[^\s]+)',
            # ç´”æ–‡å­—æ ¼å¼: æ¨™é¡Œ - URL  -> (title, url)
            r'([^-\n]+)\s*-\s*(https?://[^\s]+)'
        ]
        
        url_like = re.compile(r'^https?://', re.IGNORECASE)

        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if len(match) != 2:
                    continue
                a, b = match[0].strip(), match[1].strip()

                # ä»¥æ˜¯å¦åƒ URL ä¾†æ±ºå®šæ¬„ä½å°æ‡‰ï¼Œçµ±ä¸€è¼¸å‡ºç‚º (title, url)
                if url_like.match(a) and not url_like.match(b):
                    url, title = a, b
                elif url_like.match(b) and not url_like.match(a):
                    url, title = b, a
                else:
                    # å›é€€ç­–ç•¥ï¼šæ²¿ç”¨åŸæœ¬ (title, url) å‡è¨­
                    title, url = a, b

                # ğŸ”§ æ–°å¢ï¼šæª¢æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
                if url in seen_urls:
                    continue

                # æ¸…ç†æ¨™é¡Œ
                title = title.strip().strip('"\'')
                if not title or len(title) < 3 or len(title) > 200:
                    continue

                # æ’é™¤ç„¡æ„ç¾©çš„æ¨™é¡Œ
                if re.match(r'^(é»æ“Šé€™è£¡|é–±è®€æ›´å¤š|æ›´å¤šè³‡è¨Š|é€£çµ|ç¶²å€|click here|read more|more info|link|url)$', title, re.IGNORECASE):
                    continue
                if re.match(r'^\d+$', title) or re.match(r'^[^\w\u4e00-\u9fff]+$', title):
                    continue

                links.append({"title": title, "url": url})
                seen_urls.add(url)  # ğŸ”§ æ–°å¢ï¼šè¨˜éŒ„å·²è™•ç†çš„URL

        return links
    
    def _find_title_for_url_in_content(self, content: str, target_url: str) -> str:
        """åœ¨æ–‡ä»¶å…§å®¹ä¸­å°‹æ‰¾ç‰¹å®šURLå°æ‡‰çš„æ¨™é¡Œ"""
        try:
            lines = content.split('\n')
            
            for i, line in enumerate(lines):
                if target_url in line:
                    # æŸ¥çœ‹ç›®å‰è¡Œå’Œå‰å¾Œå¹¾è¡Œå°‹æ‰¾æ¨™é¡Œ
                    context_lines = []
                    start = max(0, i - 2)
                    end = min(len(lines), i + 3)
                    context_lines = lines[start:end]
                    
                    for context_line in context_lines:
                        # è·³éåŒ…å«URLçš„è¡Œ
                        if target_url in context_line:
                            continue
                        
                        # å°‹æ‰¾æ¨™é¡Œæ¨£æ¿
                        context_line = context_line.strip()
                        if context_line and len(context_line) > 5 and len(context_line) < 200:
                            # ç§»é™¤å¸¸è¦‹çš„æ¨™è¨˜ç¬¦è™Ÿ
                            cleaned = re.sub(r'^[#*\-\â€¢\d\.\s]+', '', context_line)
                            if cleaned and len(cleaned) > 5:
                                return cleaned
            
            return ""
            
        except Exception as e:
            logger.debug(f"åœ¨å…§å®¹ä¸­å°‹æ‰¾æ¨™é¡Œå¤±æ•—: {e}")
            return ""

    def _generate_smart_title(self, metadata: dict, url: str) -> str:
        """æ™ºæ…§ç”Ÿæˆæ¨™é¡Œ"""
        try:
            # å„ªå…ˆç´š1: ä½¿ç”¨æª”æ¡ˆæ¨™é¡Œ
            title = metadata.get('title', '')
            if title and title != 'unknown' and len(title) > 3:
                return title
            
            # å„ªå…ˆç´š2: ä½¿ç”¨åŸå§‹æª”æ¡ˆåç¨±ï¼ˆå»é™¤å‰¯æª”åï¼‰
            filename = metadata.get('original_filename', metadata.get('filename', ''))
            if filename and filename != 'unknown':
                title = os.path.splitext(filename)[0]
                # ç¾åŒ–æª”æ¡ˆåç¨±
                title = title.replace('-', ' ').replace('_', ' ')
                title = ' '.join(word.capitalize() for word in title.split() if word)
                if len(title) > 3:
                    return title
            
            # å„ªå…ˆç´š3: å¾URLè·¯å¾‘æå–æ™ºæ…§æ¨™é¡Œ
            from urllib.parse import urlparse, unquote
            parsed = urlparse(url)
            
            # ç²å–è·¯å¾‘ä¸­çš„æœ‰ç”¨è³‡è¨Š
            path_parts = [part for part in parsed.path.strip('/').split('/') if part]
            if path_parts:
                # ä½¿ç”¨æœ€å¾Œä¸€å€‹æœ‰æ„ç¾©çš„è·¯å¾‘æ®µ
                last_part = path_parts[-1]
                # URLè§£ç¢¼
                last_part = unquote(last_part)
                # ç§»é™¤å¸¸è¦‹çš„å‰¯æª”å
                last_part = re.sub(r'\.(html|php|aspx|jsp|htm)$', '', last_part)
                # æ›¿æ›åˆ†éš”ç¬¦ä¸¦ç¾åŒ–
                title = last_part.replace('-', ' ').replace('_', ' ')
                title = ' '.join(word.capitalize() for word in title.split() if word)
                if len(title) > 3:
                    return title
            
            # å„ªå…ˆç´š4: ä½¿ç”¨æŸ¥è©¢åƒæ•¸ä¸­çš„è³‡è¨Š
            if parsed.query:
                query_parts = parsed.query.split('&')
                for part in query_parts:
                    if '=' in part:
                        key, value = part.split('=', 1)
                        if key in ['title', 'q', 'query', 'search'] and value:
                            decoded_value = unquote(value).replace('+', ' ')
                            if len(decoded_value) > 3:
                                return decoded_value
            
            # å„ªå…ˆç´š5: ä½¿ç”¨åŸŸåï¼ˆæœ€å¾Œçš„å‚™é¸ï¼‰
            domain = parsed.netloc
            if domain:
                # ç§»é™¤wwwå‰ç¶´
                domain = re.sub(r'^www\.', '', domain)
                return f"ä¾†æºï¼š{domain}"
            
            return url
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆæ™ºæ…§æ¨™é¡Œå¤±æ•—: {e}")
            return self._extract_domain_from_url(url)

    def _extract_source_urls(self, docs: List) -> List[dict]:
        """å¾æ–‡ä»¶å…ƒè³‡æ–™ä¸­æå–URLå’Œå®Œæ•´æ¨™é¡Œè³‡è¨Š"""
        sources = []
        seen_urls = set()

        for doc in docs:
            try:
                metadata = getattr(doc, 'metadata', {})
                
                # æ–¹æ³•1: æª¢æŸ¥æ˜¯å¦æœ‰é è™•ç†çš„æ¨™é¡Œ-URLå°æ‡‰
                title_url_mapping = metadata.get('title_url_mapping', {})
                if isinstance(title_url_mapping, str):
                    try:
                        title_url_mapping = json.loads(title_url_mapping)
                    except json.JSONDecodeError:
                        title_url_mapping = {}
                
                # å¦‚æœæœ‰å°æ‡‰ï¼Œç›´æ¥ä½¿ç”¨
                if title_url_mapping:
                    for title, url in title_url_mapping.items():
                        if url not in seen_urls:
                            sources.append({
                                "title": title,
                                "url": url
                            })
                            seen_urls.add(url)
                    continue
                
                # æ–¹æ³•2: å¾æ–‡ä»¶å…§å®¹ä¸­æå–æ¨™é¡Œå’ŒURL
                doc_content = self._get_document_content(doc)
                extracted_links = self._extract_links_from_content(doc_content)
                for link_info in extracted_links:
                    if link_info['url'] not in seen_urls:
                        sources.append(link_info)
                        seen_urls.add(link_info['url'])
                
                # ğŸ†• æ–¹æ³•2.5: å¦‚æœæ²’æœ‰æ‰¾åˆ°æ ¼å¼åŒ–é€£ç·šï¼Œå‰‡å¾å…§å®¹ä¸­æå–åŸå§‹URL
                if not extracted_links:
                    raw_urls = re.findall(r'https?://[^\s<>"\\]+', doc_content)
                    for url in raw_urls:
                        if url not in seen_urls:
                            title = self._generate_smart_title(metadata, url)
                            sources.append({"title": title, "url": url})
                            seen_urls.add(url)
                
                # æ–¹æ³•3: å¾contained_urlsç²å–URLï¼Œç„¶å¾Œå˜—è©¦ç¬¦åˆæ¨™é¡Œ
                url_string = metadata.get('contained_urls', '')
                if url_string and not extracted_links:
                    urls_in_chunk = [url.strip() for url in url_string.split('|') if url.strip()]
                    
                    for url in urls_in_chunk:
                        if url not in seen_urls:
                            # å˜—è©¦å¾æ–‡ä»¶å…§å®¹ä¸­æ‰¾åˆ°å°æ‡‰çš„æ¨™é¡Œ
                            title = self._find_title_for_url_in_content(doc_content, url)
                            if not title:
                                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨æ™ºæ…§æ¨™é¡Œæå–
                                title = self._generate_smart_title(metadata, url)
                            
                            sources.append({
                                "title": title,
                                "url": url
                            })
                            seen_urls.add(url)

            except Exception as e:
                logger.warning(f"å¾å…ƒè³‡æ–™æå–URLæ™‚å‡ºéŒ¯: {e}")

        logger.info(f"å¾å…ƒè³‡æ–™ä¸­æå–åˆ° {len(sources)} å€‹é€£ç·š")
        return sources

    def _is_valid_url(self, url: str) -> bool:
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            return (parsed.scheme in ['http', 'https'] and 
                    parsed.netloc and 
                    '.' in parsed.netloc and
                    len(url) > 10)
        except:
            return False


    def _extract_domain_from_url(self, url: str) -> str:
        """å¾URLä¸­æå–åŸŸåä½œç‚ºæ¨™é¡Œ"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc
            return domain if domain else url
        except:
            return url

    def _filter_duplicate_links(self, sources: List[dict], session_id: str) -> List[dict]:
        """éæ¿¾é‡è¤‡é€£ç·š"""
        if session_id not in self.session_shown_links:
            self.session_shown_links[session_id] = set()
        
        shown_links = self.session_shown_links[session_id]
        filtered_sources = []
        
        for source in sources:
            url = source.get('url', '')
            if url and url not in shown_links:
                filtered_sources.append(source)
                shown_links.add(url)
                logger.info(f"âœ… æ–°é€£ç·š: {source.get('title', 'unknown')}")
            else:
                logger.info(f"ğŸ”„ è·³éé‡è¤‡é€£ç·š: {source.get('title', 'unknown')}")
        
        return filtered_sources

    def _format_source_links(self, sources: List[dict]) -> str:
        """æ ¼å¼åŒ–åƒè€ƒé€£çµ - ç¢ºä¿æ­£ç¢ºæ›è¡Œ"""
        if not sources:
            return ""
        
        # âœ… ä¿®æ­£ï¼šåœ¨é–‹é ­å¢åŠ ä¸€å€‹é¡å¤–çš„æ›è¡Œç¬¦ï¼Œä¾†ç”¢ç”Ÿæ›´å¤šé–“è·
        source_links = "\n\n\nğŸ’¡ ä½ å¯èƒ½æƒ³çŸ¥é“\n\n"  # ç°¡åŒ–ç‰ˆæœ¬
        
        formatted_items = []
        for source in sources:
            title = source["title"]
            url = source["url"]
            formatted_items.append(f"- [{title}]({url})")
        
        source_links += "\n".join(formatted_items)
        return source_links
    
    def _clean_and_validate_title(self, title: str) -> str:
        """æ¸…ç†"""
        if not title:
            return ""
        
        # æ¸…ç†æ¨™é¡Œ
        title = title.strip()
        title = re.sub(r'\s+', ' ', title)  # åˆä½µå¤šå€‹ç©ºæ ¼
        title = re.sub(r'^[^\w\u4e00-\u9fff]+', '', title)  # ç§»é™¤é–‹é ­çš„ç¬¦è™Ÿ
        title = re.sub(r'[^\w\u4e00-\u9fff]+$', '', title)  # ç§»é™¤çµå°¾çš„ç¬¦è™Ÿ
        
        # é©—è­‰æ¨™é¡Œé•·åº¦
        if len(title) < 3 or len(title) > 200:
            return ""
        
        # æ’é™¤ç„¡æ„ç¾©çš„æ¨™é¡Œ
        meaningless_patterns = [
            r'^(é»æ“Šé€™è£¡|é–±è®€æ›´å¤š|æ›´å¤šè³‡è¨Š|é€£ç·š|ç¶²å€|click here|read more|more info|link|url)$',
            r'^\d+$',  # ç´”æ•¸å­—
            r'^[^\w\u4e00-\u9fff]+$',  # åªæœ‰ç¬¦è™Ÿ
        ]
        
        for pattern in meaningless_patterns:
            if re.match(pattern, title, re.IGNORECASE):
                return ""
        
        return title


def main():
    # âœ¨ é—œéµæ›´å‹•ï¼šç¨ç«‹å•Ÿå‹•æ™‚éœ€è¦å¾è³‡æ–™åº«åŠ è¼‰è¨­å®š
    parser = argparse.ArgumentParser(description="å•Ÿå‹•ä¸€å€‹ç¨ç«‹çš„èŠå¤©æ©Ÿå™¨äººå¯¦ä¾‹ã€‚")
    parser.add_argument("--bot-name", type=str, required=True, help="è¦å•Ÿå‹•çš„æ©Ÿå™¨äººåç¨±")
    args = parser.parse_args()

    try:
        # ç¨ç«‹å•Ÿå‹•æ™‚ï¼Œéœ€è¦ä¸€å€‹æ–¹æ³•ä¾†å¾DBç²å–è¨­å®š
        # é€™éœ€è¦ BotConfigManager çš„ä¸€å€‹å¯¦ä¾‹
        from bot_config_manager import BotConfigManager
        database_url = os.getenv("DATABASE_URL")
        if not database_url:
            raise ValueError("DATABASE_URL environment variable not set.")
        
        config_manager = BotConfigManager(database_url)
        config = config_manager.get_bot_config(args.bot_name)
        
        if not config:
            raise ValueError(f"Bot '{args.bot_name}' not found in the database.")

        instance = ChatbotInstance(config=config)
        port = instance.config.get("port")
        if not port:
            raise ValueError(f"Port not specified in the config for bot '{args.bot_name}'.")
        
        logger.info(f"ğŸ¤– æ©Ÿå™¨äºº '{instance.bot_name}' æ­£åœ¨ http://localhost:{port} ä¸Šå•Ÿå‹•")
        logger.info(f"ğŸ“Š å°è©±è¨˜éŒ„è³‡æ–™åº«å·²åœ¨å¯¦ä¾‹ä¸­é…ç½®ã€‚")
        logger.info(f"ğŸ“š çŸ¥è­˜åº«é›†åˆï¼š{instance.collection_name}")
        logger.info(f"ğŸ” æœå°‹æ¨¡å¼ï¼š{instance.search_mode}")
        if instance.search_mode == "api":
            logger.info(f"ğŸ”— å‘é‡APIåœ°å€ï¼š{instance.vector_api_url}")
        logger.info(f"ğŸ”§ åµéŒ¯æ¨¡å¼å·²å•Ÿç”¨ï¼Œå°‡è¼¸å‡ºè©³ç´°çš„å°è©±è¨˜éŒ„å’Œæ¨è–¦å•é¡Œç”Ÿæˆæ—¥èªŒ")
        
        uvicorn.run(instance.app, host="0.0.0.0", port=int(port))

    except Exception as e:
        logger.error(f"âŒ å•Ÿå‹•æ©Ÿå™¨äºº '{args.bot_name}' å¤±æ•—: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
