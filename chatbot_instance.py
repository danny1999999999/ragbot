#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
chatbot_instance.py -  (APIå…¼å®¹ç‰ˆ)

ä¿®å¤å†…å®¹ï¼š

"""

from dotenv import load_dotenv
load_dotenv(override=True)

import os
import json
import argparse
import sys
import re
import time
from pathlib import Path
import logging
from typing import Tuple, List, Dict, Optional

from fastapi import FastAPI, Request, Depends, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

from auth_middleware import OptionalAuth, User, JWTManager
from conversation_logger_simple import EnhancedConversationLogger as ConversationLogger


import sys
from pathlib import Path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
from config import app_config  # â­ çµ±ä¸€å°å…¥

# ğŸ”§ æ£€æŸ¥APIæ¨¡å¼ä¾èµ–
try:
    import httpx
    HTTPX_AVAILABLE = True
    print("âœ… httpx å¯ç”¨ï¼Œæ”¯æŒå‘é‡APIæ¨¡å¼")
except ImportError:
    HTTPX_AVAILABLE = False
    print("âš ï¸ httpx ä¸å¯ç”¨ï¼Œä»…æ”¯æŒç›´æ¥å‘é‡è®¿é—®æ¨¡å¼")

# ğŸ”§ æ£€æŸ¥å‘é‡ç³»ç»Ÿ
try:
    from vector_builder_langchain import OptimizedVectorSystem
    VECTOR_SYSTEM_AVAILABLE = True
    print("âœ… å‘é‡ç³»ç»Ÿå¯ç”¨")
except ImportError:
    VECTOR_SYSTEM_AVAILABLE = False
    print("âŒ å‘é‡ç³»ç»Ÿä¸å¯ç”¨")

try:
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage, SystemMessage
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

ROOT_DIR = Path(__file__).parent
BOT_CONFIGS_DIR = ROOT_DIR / "bot_configs"

class ChatbotInstance:
    def __init__(self, bot_name: str, **kwargs):
        self.bot_name = bot_name
        self.config = self._load_config()
        self.collection_name = f"collection_{self.bot_name}"
        
        # ğŸ”§ å‘é‡APIé…ç½®
        self.vector_api_url = app_config.get_vector_api_url()
        self.use_api_mode = (
            HTTPX_AVAILABLE and 
            os.getenv("USE_VECTOR_API", "false").lower() == "true"
        )
        
        # ğŸ”§ å‘é‡ç³»ç»Ÿåˆå§‹åŒ– - ä¿æŒåŸæœ‰é€»è¾‘
        self.vector_system = None
        if VECTOR_SYSTEM_AVAILABLE:
            try:
                self.vector_system = OptimizedVectorSystem()
                logger.info(f"âœ… å‘é‡ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ (æœºå™¨äºº: {bot_name})")
            except Exception as e:
                logger.error(f"å‘é‡ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
                self.vector_system = None
        
        # ğŸ”§ æ¨¡å¼é€‰æ‹©é€»è¾‘
        if self.use_api_mode and HTTPX_AVAILABLE:
            logger.info(f"ğŸ”— ä½¿ç”¨å‘é‡APIæ¨¡å¼: {self.vector_api_url}")
            self.search_mode = "api"
        elif self.vector_system:
            logger.info("ğŸ”§ ä½¿ç”¨ç›´æ¥å‘é‡è®¿é—®æ¨¡å¼")
            self.search_mode = "direct"
        else:
            logger.warning("âš ï¸ å‘é‡æœç´¢åŠŸèƒ½ä¸å¯ç”¨")
            self.search_mode = "disabled"
        
        # æ¯ä¸ªæœºå™¨äººå®ä¾‹ä½¿ç”¨ç¨ç«‹çš„å°è©±è¨˜éŒ„è³‡æ–™åº«
        db_config = self._get_db_config()
        self.logger = ConversationLogger(db_config=db_config)
        
        self.app = FastAPI(title=f"{self.bot_name} Chatbot")
        try:
            from starlette.middleware.proxy_headers import ProxyHeadersMiddleware
            self.app.add_middleware(ProxyHeadersMiddleware, trusted_hosts="*")
            logger.info("âœ… ProxyHeadersMiddleware å·²å•Ÿç”¨")
        except ImportError:
            logger.info("â„¹ï¸ ProxyHeadersMiddleware ä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°é–‹ç™¼æ¨¡å¼")
        except Exception as e:
            logger.warning(f"âš ï¸ ProxyHeadersMiddleware å•Ÿç”¨å¤±æ•—: {e}")


        # CORS æ”¯æŒ
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
        
        # æ£€æŸ¥å¹¶åˆ›å»ºæ¨¡æ¿ç›®å½•
        self.template_dir = ROOT_DIR / "chatbot_templates" / "modern"
        self.static_dir = self.template_dir / "static"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.template_dir.mkdir(parents=True, exist_ok=True)
        self.static_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        chat_html_path = self.template_dir / "chat.html"
        if not chat_html_path.exists():
            logger.warning(f"âš ï¸ æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {chat_html_path}")
            logger.warning("è¯·ç¡®ä¿ chat.html ä½äº chatbot_templates/modern/ ç›®å½•ä¸­")
        
        self.modern_templates = Jinja2Templates(directory=str(self.template_dir))
        
        # é™æ€æ–‡ä»¶æŒ‚è½½æ£€æŸ¥
        if self.static_dir.exists():
            self.app.mount("/modern/static", StaticFiles(directory=str(self.static_dir)), name="modern_static")
            logger.info(f"âœ… é™æ€æ–‡ä»¶æŒ‚è½½æˆåŠŸ: {self.static_dir}")
        else:
            logger.warning(f"âš ï¸ é™æ€æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨: {self.static_dir}")
        
        self.session_counters: Dict[str, int] = {}
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        self.total_conversations = 0
        self.successful_conversations = 0
        self.failed_conversations = 0
        # è®°å½•æ¯ä¸ª session å·²æ˜¾ç¤ºçš„è¿æ¥
        self.session_shown_links = {}
        
        self.setup_routes()
        logger.info(f"âœ… æœºå™¨äººå®ä¾‹ '{self.bot_name}' åˆå§‹åŒ–å®Œæˆï¼Œå¯¹è¯è®°å½•æ•°æ®åº“é…ç½®ç±»å‹ï¼š{db_config.get('type')}")

    def _load_config(self) -> dict:
        """è½½å…¥æœºå™¨äººé…ç½®"""
        config_path = BOT_CONFIGS_DIR / f"{self.bot_name}.json"
        if not config_path.exists():
            raise FileNotFoundError(f"æœºå™¨äººè®¾å®šæ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8-sig') as f:
            config = json.load(f)
        
        # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰ display_nameï¼Œè‡ªåŠ¨æ·»åŠ 
        if "display_name" not in config:
            config["display_name"] = config.get("bot_name", self.bot_name)
            try:
                with open(config_path, 'w', encoding='utf-8') as f:
                    json.dump(config, f, ensure_ascii=False, indent=4)
                logger.info(f"ä¸ºæœºå™¨äºº {self.bot_name} è‡ªåŠ¨æ·»åŠ æ˜¾ç¤ºåç§°: {config['display_name']}")
            except Exception as e:
                logger.warning(f"è‡ªåŠ¨ä¿å­˜æ˜¾ç¤ºåç§°å¤±è´¥: {e}")
        
        return config

    def _get_db_config(self) -> dict:
        """Gets the database configuration, forcing PostgreSQL on Railway."""
        database_url = os.getenv("DATABASE_URL")

        if not database_url:
            error_msg = "ç’°å¢ƒè®Šæ•¸ DATABASE_URL æœªè¨­å®šã€‚å°è©±ç´€éŒ„åŠŸèƒ½éœ€è¦ PostgreSQLè³‡æ–™åº«ã€‚"
            logger.error(f"âŒ æ©Ÿå™¨äºº ' {self.bot_name}' - {error_msg}")
            raise ValueError(error_msg)
        logger.info(f"âœ… æ©Ÿå™¨äºº ' {self.bot_name}' å°‡ä½¿ç”¨ PostgreSQL (é€é DATABASE_URL)è¨˜éŒ„å°è©±ã€‚")
        return {
             "type": "postgresql"
             "connection_string": database_url
        } 
  
    def create_anonymous_user(self, request: Request) -> User:
        """åˆ›å»ºåŒ¿åç”¨æˆ·å¯¹è±¡"""
        client_ip = request.client.host if request.client else "unknown"
        return User(
            id=0, 
            username=f"anonymous_{client_ip}", 
            role="anonymous", 
            email="anonymous@example.com", 
            is_active=True,
            password_hash=""
        )

    def get_user_identifier(self, user: Optional[User], session_id: str) -> str:
        """è·å–ç”¨æˆ·å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºå¯¹è¯è®°å½•"""
        if user and user.id:
            return f"auth_user_{user.id}_{user.username}"
        else:
            return f"anonymous_session_{session_id}"

    def _get_document_content(self, doc) -> str:
        """ç»Ÿä¸€è·å–æ–‡æ¡£å†…å®¹çš„æ–¹æ³•"""
        # å°è¯•å¸¸è§çš„å†…å®¹å±æ€§åç§°
        for attr in ['page_content', 'content', 'text']:
            if hasattr(doc, attr):
                content = getattr(doc, attr)
                if content:
                    return str(content)
        
        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
        if isinstance(doc, dict):
            for key in ['page_content', 'content', 'text']:
                if key in doc and doc[key]:
                    return str(doc[key])
        
        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›å­—ç¬¦ä¸²è¡¨ç¤º
        return str(doc)

    # ğŸ”§ æ–°å¢ï¼šé€šè¿‡APIæœç´¢å‘é‡
    async def _search_vectors_via_api(self, query: str, k: int = 3):
        """é€šè¿‡APIæœç´¢å‘é‡"""
        if not HTTPX_AVAILABLE:
            logger.warning("httpx ä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨APIæ¨¡å¼")
            return []
            
        try:
            timeout = httpx.Timeout(30.0, connect=5.0)
            async with httpx.AsyncClient(timeout=timeout) as client:
                response = await client.post(
                    f"{self.vector_api_url}/search",
                    json={
                        "query": query,
                        "collection_name": self.collection_name,
                        "k": k
                    }
                )
                
                if response.status_code == 200:
                    api_results = response.json()
                    
                    # è½¬æ¢ä¸ºå…¼å®¹åŸæœ‰ä»£ç çš„æ–‡æ¡£æ ¼å¼
                    documents = []
                    for result in api_results:
                        # åˆ›å»ºå…¼å®¹çš„æ–‡æ¡£å¯¹è±¡
                        doc = type('Document', (), {
                            'page_content': result.get('content', ''),
                            'metadata': result.get('metadata', {})
                        })()
                        documents.append(doc)
                    
                    logger.info(f"APIæœç´¢æˆåŠŸ: {len(documents)} ä¸ªç»“æœ")
                    return documents
                    
                elif response.status_code == 503:
                    logger.warning("å‘é‡APIæœåŠ¡ä¸å¯ç”¨")
                    return []
                else:
                    logger.error(f"APIæœç´¢å¤±è´¥: {response.status_code} - {response.text}")
                    return []
                    
        except httpx.TimeoutException:
            logger.error("å‘é‡æœç´¢APIè¶…æ—¶")
            return []
        except httpx.ConnectError:
            logger.error("æ— æ³•è¿æ¥åˆ°å‘é‡APIæœåŠ¡")
            return []
        except Exception as e:
            logger.error(f"å‘é‡æœç´¢APIè°ƒç”¨å¼‚å¸¸: {e}")
            return []

    # ğŸ”§ ä¿æŒåŸæœ‰çš„ç›´æ¥æœç´¢æ–¹æ³•
    def _search_vectors_direct(self, query: str, k: int = 3):
        """ç›´æ¥æœç´¢å‘é‡"""
        if not self.vector_system:
            logger.warning("å‘é‡ç³»ç»Ÿä¸å¯ç”¨")
            return []
        
        try:
            return self.vector_system.search(query, collection_name=self.collection_name, k=k)
        except Exception as e:
            logger.error(f"ç›´æ¥å‘é‡æœç´¢å¤±è´¥: {e}")
            return []

    # ğŸ”§ æ™ºèƒ½å‘é‡æœç´¢æ–¹æ³•
    async def _search_vectors_smart(self, query: str, k: int = 3):
        """æ™ºèƒ½é€‰æ‹©æœç´¢æ–¹å¼"""
        context_docs = []
        
        # ä¸»è¦æ¨¡å¼å°è¯•
        if self.search_mode == "api":
            context_docs = await self._search_vectors_via_api(query, k)
            search_method = "API"
        elif self.search_mode == "direct":
            context_docs = self._search_vectors_direct(query, k)
            search_method = "ç›´æ¥"
        else:
            logger.warning("å‘é‡æœç´¢åŠŸèƒ½ä¸å¯ç”¨")
            return []
        
        # å¦‚æœä¸»è¦æ¨¡å¼å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨¡å¼
        if not context_docs:
            if self.search_mode == "api" and self.vector_system:
                logger.warning("APIæ¨¡å¼æ— ç»“æœï¼Œå°è¯•ç›´æ¥æ¨¡å¼...")
                context_docs = self._search_vectors_direct(query, k)
                if context_docs:
                    search_method = "ç›´æ¥(å¤‡ç”¨)"
                    logger.info(f"âœ… ç›´æ¥æ¨¡å¼å¤‡ç”¨æˆåŠŸ: {len(context_docs)} ä¸ªæ–‡æ¡£")
            elif self.search_mode == "direct" and HTTPX_AVAILABLE:
                logger.warning("ç›´æ¥æ¨¡å¼æ— ç»“æœï¼Œå°è¯•APIæ¨¡å¼...")
                context_docs = await self._search_vectors_via_api(query, k)
                if context_docs:
                    search_method = "API(å¤‡ç”¨)"
                    logger.info(f"âœ… APIæ¨¡å¼å¤‡ç”¨æˆåŠŸ: {len(context_docs)} ä¸ªæ–‡æ¡£")
        
        if context_docs:
            logger.info(f"ğŸ” æœºå™¨äºº '{self.bot_name}' {search_method}æ£€ç´¢ç»“æœ: {len(context_docs)} ä¸ªæ–‡æ¡£")
        else:
            logger.warning(f"ğŸ” æœºå™¨äºº '{self.bot_name}' æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        
        return context_docs

    # ä¿æŒåŸæœ‰çš„å…¶ä»–æ–¹æ³•ä¸å˜
    def get_chunk_info_safely(self, search_item, fallback_index: int) -> dict:
        """å®‰å…¨åœ°ä»æœç´¢ç»“æœä¸­è·å–chunkä¿¡æ¯"""
        try:
            chunk_info = {
                'chunk_id': f'chunk_{fallback_index}',
                'chunk_index': fallback_index,
                'content': '',
                'source': 'unknown',
                'filename': 'unknown'
            }
            
            # æ–¹æ³•1: å¤„ç†ä¸åŒç±»å‹çš„æœç´¢ç»“æœ
            if hasattr(search_item, 'page_content'):
                # LangChain Document å¯¹è±¡
                chunk_info['content'] = search_item.page_content
                if hasattr(search_item, 'metadata') and search_item.metadata:
                    metadata = search_item.metadata
                    chunk_info['chunk_id'] = metadata.get('chunk_id', f'chunk_{fallback_index}')
                    chunk_info['chunk_index'] = metadata.get('chunk_index', fallback_index)
                    chunk_info['source'] = metadata.get('source', 'unknown')
                    chunk_info['filename'] = metadata.get('filename', metadata.get('original_filename', 'unknown'))
            
            elif isinstance(search_item, dict):
                # å­—å…¸æ ¼å¼çš„æœç´¢ç»“æœ
                chunk_info['content'] = search_item.get('content', search_item.get('page_content', ''))
                metadata = search_item.get('metadata', {})
                chunk_info['chunk_id'] = metadata.get('chunk_id', f'chunk_{fallback_index}')
                chunk_info['chunk_index'] = metadata.get('chunk_index', fallback_index)
                chunk_info['source'] = metadata.get('source', 'unknown')
                chunk_info['filename'] = metadata.get('filename', metadata.get('original_filename', 'unknown'))
            
            elif hasattr(search_item, 'content'):
                # è‡ªå®šä¹‰æœç´¢ç»“æœå¯¹è±¡
                chunk_info['content'] = str(search_item.content)
                if hasattr(search_item, 'metadata'):
                    metadata = search_item.metadata
                    chunk_info['chunk_id'] = metadata.get('chunk_id', f'chunk_{fallback_index}')
                    chunk_info['chunk_index'] = metadata.get('chunk_index', fallback_index)
                    chunk_info['source'] = metadata.get('source', 'unknown')
                    chunk_info['filename'] = metadata.get('filename', 'unknown')
            
            else:
                # å¤‡ç”¨ï¼šå°†æ•´ä¸ªå¯¹è±¡è½¬ä¸ºå­—ç¬¦ä¸²
                chunk_info['content'] = str(search_item)
            
            return chunk_info
            
        except Exception as e:
            logger.warning(f"è·å–chunkä¿¡æ¯å¤±è´¥: {e}")
            return {
                'chunk_id': f'chunk_{fallback_index}',
                'chunk_index': fallback_index,
                'content': str(search_item) if search_item else '',
                'source': 'unknown',
                'filename': 'unknown'
            }

    def _find_real_chunk_index(self, content: str, fallback_index: int) -> int:
        """æŸ¥æ‰¾å†…å®¹åœ¨å‘é‡åº“ä¸­çš„çœŸå®ç´¢å¼•"""
        try:
            if self.vector_system:
                vectorstore = self.vector_system.get_or_create_vectorstore(self.collection_name)
                all_docs = vectorstore.get()
                
                if all_docs and all_docs.get('documents'):
                    # æŸ¥æ‰¾å®Œå…¨åŒ¹é…çš„å†…å®¹
                    for idx, doc_content in enumerate(all_docs['documents']):
                        if doc_content.strip() == content.strip():
                            return idx
                    
                    # å¦‚æœæ²¡æœ‰å®Œå…¨åŒ¹é…ï¼ŒæŸ¥æ‰¾éƒ¨åˆ†åŒ¹é…
                    for idx, doc_content in enumerate(all_docs['documents']):
                        if content[:100] in doc_content or doc_content[:100] in content:
                            return idx
            
            # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›å¤‡ç”¨ç´¢å¼•
            return fallback_index
            
        except Exception as e:
            logger.debug(f"æŸ¥æ‰¾çœŸå®chunkç´¢å¼•å¤±è´¥: {e}")
            return fallback_index

    def _get_chunk_index_from_doc(self, doc, fallback_index: int) -> int:
        """å¾æ–‡æª”å°è±¡ä¸­ç²å– chunk çš„å¯¦éš›ç´¢å¼• - æ”¹é€²ç‰ˆ"""
        try:
            logger.debug(f"ğŸ” å˜—è©¦ç²å–æ–‡æª”çš„çœŸå¯¦ç´¢å¼•ï¼Œfallback: {fallback_index}")
            
            # æ–¹æ³•1: å¾ metadata ä¸­ç²å– - æ”¹é€²ç‰ˆ
            metadata = None
            if hasattr(doc, 'metadata') and doc.metadata:
                metadata = doc.metadata
            elif isinstance(doc, dict) and 'metadata' in doc:
                metadata = doc['metadata']
            
            if metadata:
                # ğŸ†• è¨˜éŒ„å…ƒæ•¸æ“šå…§å®¹ç”¨æ–¼èª¿è©¦
                logger.debug(f"ğŸ“‹ æ–‡æª”å…ƒæ•¸æ“š: {list(metadata.keys())}")
                
                # æ“´å±•æœç´¢å­—æ®µ
                index_fields = [
                    'chunk_index', 'index', 'chunk_id', 'id', 
                    'chunk_num', 'chunk_number', 'doc_index',
                    'document_index', 'position', 'seq'
                ]
                
                for index_field in index_fields:
                    if index_field in metadata:
                        value = metadata[index_field]
                        logger.debug(f"ğŸ“ æ‰¾åˆ°ç´¢å¼•å­—æ®µ {index_field}: {value} (type: {type(value)})")
                        
                        if isinstance(value, int) and value >= 0:
                            logger.info(f"âœ… å¾ metadata.{index_field} ç²å–ç´¢å¼•: {value}")
                            return value
                        elif isinstance(value, str):
                            # å˜—è©¦å¤šç¨®å­—ç¬¦ä¸²æ ¼å¼
                            if value.startswith('chunk_'):
                                try:
                                    idx = int(value.split('_')[-1])
                                    logger.info(f"âœ… å¾ metadata.{index_field} è§£æç´¢å¼•: {idx}")
                                    return idx
                                except ValueError:
                                    continue
                            elif value.isdigit():
                                idx = int(value)
                                logger.info(f"âœ… å¾ metadata.{index_field} è½‰æ›ç´¢å¼•: {idx}")
                                return idx
            
            # æ–¹æ³•2: å¾æ–‡æª”IDä¸­æå–ç´¢å¼• - ä¿æŒä¸è®Š
            doc_id = None
            if hasattr(doc, 'id') and doc.id:
                doc_id = str(doc.id)
            elif isinstance(doc, dict) and 'id' in doc:
                doc_id = str(doc['id'])
            
            if doc_id:
                logger.debug(f"ğŸ“„ æ–‡æª”ID: {doc_id}")
                # å˜—è©¦å¤šç¨®IDæ ¼å¼
                patterns = [
                    r'_(\d+)$',           # ending with _123
                    r'chunk_(\d+)',       # chunk_123
                    r'doc_(\d+)',         # doc_123
                    r'-(\d+)$',           # ending with -123
                    r'(\d+)$'             # ending with 123
                ]
                
                for pattern in patterns:
                    match = re.search(pattern, doc_id)
                    if match:
                        idx = int(match.group(1))
                        logger.info(f"âœ… å¾æ–‡æª”IDè§£æç´¢å¼•: {idx} (pattern: {pattern})")
                        return idx
            
            # æ–¹æ³•3: å¾å‘é‡æ•¸æ“šåº«æŸ¥è©¢å¯¦éš›ä½ç½® - æ”¹é€²ç‰ˆ
            if self.vector_system:
                try:
                    logger.debug(f"ğŸ” å˜—è©¦å¾å‘é‡æ•¸æ“šåº«æŸ¥è©¢å¯¦éš›ä½ç½®...")
                    vectorstore = self.vector_system.get_or_create_vectorstore(self.collection_name)
                    all_docs_result = vectorstore.get()
                    
                    if all_docs_result and all_docs_result.get('documents'):
                        all_documents = all_docs_result['documents']
                        doc_content = self._get_document_content(doc)
                        
                        # ğŸ†• æ”¹é€²ï¼šå˜—è©¦å¤šç¨®åŒ¹é…ç­–ç•¥
                        
                        # ç­–ç•¥1: å®Œå…¨åŒ¹é…
                        for idx, stored_doc_content in enumerate(all_documents):
                            if stored_doc_content.strip() == doc_content.strip():
                                logger.info(f"âœ… é€šéå®Œå…¨åŒ¹é…æ‰¾åˆ°ç´¢å¼•: {idx}")
                                return idx
                        
                        # ç­–ç•¥2: å‰100å­—ç¬¦åŒ¹é…
                        doc_prefix = doc_content[:100].strip()
                        if len(doc_prefix) > 20:  # ç¢ºä¿æœ‰è¶³å¤ çš„å…§å®¹é€²è¡ŒåŒ¹é…
                            for idx, stored_doc_content in enumerate(all_documents):
                                stored_prefix = stored_doc_content[:100].strip()
                                if doc_prefix == stored_prefix:
                                    logger.info(f"âœ… é€šéå‰ç¶´åŒ¹é…æ‰¾åˆ°ç´¢å¼•: {idx}")
                                    return idx
                        
                        # ç­–ç•¥3: é—œéµè©åŒ¹é…ï¼ˆå¦‚æœå…§å®¹è¼ƒçŸ­ï¼‰
                        if len(doc_content) < 500:
                            doc_words = set(doc_content.split())
                            if len(doc_words) > 3:  # è‡³å°‘è¦æœ‰å¹¾å€‹è©
                                best_match_idx = -1
                                best_match_score = 0
                                
                                for idx, stored_doc_content in enumerate(all_documents):
                                    stored_words = set(stored_doc_content.split())
                                    common_words = doc_words.intersection(stored_words)
                                    score = len(common_words) / len(doc_words.union(stored_words))
                                    
                                    if score > 0.8 and score > best_match_score:
                                        best_match_score = score
                                        best_match_idx = idx
                                
                                if best_match_idx >= 0:
                                    logger.info(f"âœ… é€šéè©åŒ¯åŒ¹é…æ‰¾åˆ°ç´¢å¼•: {best_match_idx} (ç›¸ä¼¼åº¦: {best_match_score:.2f})")
                                    return best_match_idx
                    
                    logger.warning(f"âš ï¸ å‘é‡æ•¸æ“šåº«æŸ¥è©¢ç„¡åŒ¹é…çµæœ")
                            
                except Exception as query_error:
                    logger.warning(f"âš ï¸ æŸ¥è©¢å‘é‡æ•¸æ“šåº«ç´¢å¼•å¤±æ•—: {query_error}")
            
            # æ–¹æ³•4: ä½¿ç”¨å‚™ç”¨ç´¢å¼•
            logger.warning(f"âš ï¸ æ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨ç´¢å¼•: {fallback_index}")
            return fallback_index
            
        except Exception as e:
            logger.error(f"âŒ ç²å– chunk ç´¢å¼•æ™‚ç™¼ç”Ÿç•°å¸¸: {e}")
            return fallback_index

    def setup_routes(self):
        """è®¾ç½®è·¯ç”±"""
        
        @self.app.get("/", response_class=HTMLResponse)
        async def get_modern_chat_ui(request: Request):
            """ä¸»èŠå¤©ç•Œé¢"""
            try:
                display_name = self.config.get("display_name", self.bot_name)
                return self.modern_templates.TemplateResponse(
                    "chat.html", 
                    {
                        "request": request, 
                        "bot_name": self.bot_name,
                        "display_name": display_name
                    }
                )
            except Exception as e:
                logger.error(f"âŒ è½½å…¥èŠå¤©ç•Œé¢å¤±è´¥: {e}")
                return HTMLResponse(f"""
                <!DOCTYPE html>
                <html>
                <head><title>{self.bot_name}</title></head>
                <body>
                    <h1>èŠå¤©ç•Œé¢è½½å…¥å¤±è´¥</h1>
                    <p>é”™è¯¯ï¼š{e}</p>
                    <p>è¯·æ£€æŸ¥æ¨¡æ¿æ–‡ä»¶æ˜¯å¦å­˜åœ¨äº chatbot_templates/modern/chat.html</p>
                </body>
                </html>
                """, status_code=500)

        @self.app.post("/api/chat")
        async def chat(request: Request, user: User = Depends(OptionalAuth)):
            """èŠå¤©APIç«¯ç‚¹ - æ”¯æŒAPIå’Œç›´æ¥ä¸¤ç§æ¨¡å¼"""
            start_time = time.time()
            conversation_id = None
            chunk_references = []
            
            try:
                data = await request.json()
                query = data.get("message")
                session_id = data.get("session_id", "default_session")
                
                if not query:
                    raise HTTPException(status_code=400, detail="Message cannot be empty")
                
                # å¤„ç†ç”¨æˆ·è¯†åˆ«
                if not user:
                    user = self.create_anonymous_user(request)
                
                user_identifier = self.get_user_identifier(user, session_id)
                
                # æ›´æ–°ç»Ÿè®¡
                self.total_conversations += 1
                if session_id not in self.session_counters:
                    self.session_counters[session_id] = 0
                self.session_counters[session_id] += 1
                self._current_query = query
                logger.info(f"ğŸ“© æœºå™¨äºº '{self.bot_name}' æ”¶åˆ°æŸ¥è¯¢ [{user_identifier}]: {query[:50]}...")

                # ğŸ”§ ä¿®æ”¹ï¼šä½¿ç”¨æ™ºèƒ½å‘é‡æœç´¢
                context_docs = await self._search_vectors_smart(query, k=3)
                
                # è¯¦ç»†è°ƒè¯•æ£€ç´¢ç»“æœ
                logger.info(f"ğŸ” å½“å‰æŸ¥è¯¢: {query}")
                logger.info(f"ğŸ“„ æ£€ç´¢åˆ° {len(context_docs) if context_docs else 0} ä¸ªæ–‡æ¡£")

                if context_docs:
                    for i, doc in enumerate(context_docs):
                        content_preview = self._get_document_content(doc)[:100]
                        metadata = getattr(doc, 'metadata', {})
                        contained_urls = metadata.get('contained_urls', '')
                        
                        logger.info(f"ğŸ“„ æ–‡æ¡£ {i+1}:")
                        logger.info(f"  å†…å®¹é¢„è§ˆ: {content_preview}")
                        logger.info(f"  åŒ…å«è¿æ¥: {contained_urls}")
                        logger.info(f"  æ¥æºæ–‡ä»¶: {metadata.get('filename', 'unknown')}")

                context = "\n".join([self._get_document_content(doc) for doc in context_docs]) if context_docs else ""
                
                # å‡†å¤‡ chunk å¼•ç”¨ä¿¡æ¯
                retrieved_docs_content = []
                
                if context_docs:
                    for i, doc in enumerate(context_docs):
                        doc_content = self._get_document_content(doc)
                        retrieved_docs_content.append(doc_content)
                        
                        chunk_index = self._get_chunk_index_from_doc(doc, i)
                        
                        # è·å–å…ƒæ•°æ®
                        metadata = {}
                        if hasattr(doc, 'metadata') and doc.metadata:
                            metadata = doc.metadata
                        elif isinstance(doc, dict) and 'metadata' in doc:
                            metadata = doc['metadata']
                        
                        chunk_ref = {
                            "index": chunk_index,
                            "content_preview": doc_content[:100] + "..." if len(doc_content) > 100 else doc_content,
                            "source": metadata.get('source', 'unknown'),
                            "filename": metadata.get('filename', metadata.get('original_filename', 'unknown'))
                        }
                        chunk_references.append(chunk_ref)

                logger.info(f"ğŸ” æœºå™¨äºº '{self.bot_name}' æ£€ç´¢ç»“æœ: {len(context_docs)} ä¸ªæ–‡æ¡£, chunk_refs: {len(chunk_references)}")
                
                # ç”Ÿæˆå›åº”
                system_prompt = self.config.get("system_role", "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„ AI åŠ©ç†ã€‚")
                response_text, recommended_questions = self._generate_response(
                    query, context, system_prompt, session_id
                )
                
                # å¤„ç†å¼•ç”¨æ¥æº
                if self.config.get("cite_sources_enabled", False) and context_docs:
                    all_sources = self._extract_source_urls(context_docs)
                    # åªå¤„ç†åŒ…å«çœŸå®URLçš„æ¥æº
                    url_sources = [s for s in all_sources if s.get("url")]
                    
                    if url_sources:
                        # è¿‡æ»¤é‡å¤è¿æ¥
                        url_sources = self._filter_duplicate_links(url_sources, session_id)
                        
                        if url_sources:  # ç¡®ä¿è¿‡æ»¤åè¿˜æœ‰è¿æ¥
                            response_text += self._format_source_links(url_sources)
                            logger.info(f"ğŸ”— æ·»åŠ äº† {len(url_sources)} ä¸ªå‚è€ƒè¿æ¥")
                        else:
                            logger.info("ğŸ”„ æ‰€æœ‰è¿æ¥éƒ½æ˜¯é‡å¤çš„ï¼Œè·³è¿‡æ˜¾ç¤º")
                    else:
                        logger.info("ğŸ“ æœªåœ¨æ–‡æ¡£å†…å®¹ä¸­æ‰¾åˆ°å¯å¼•ç”¨çš„URL")

                processing_time_ms = int((time.time() - start_time) * 1000)
                
                # è®°å½•å¯¹è¯
                try:
                    conversation_id = self.logger.log_conversation(
                        user_id=user_identifier,
                        user_query=query,
                        ai_response=response_text,
                        collection_used=self.collection_name if context_docs else None,
                        retrieved_docs=retrieved_docs_content[:3],
                        doc_similarities=[],
                        processing_time_ms=processing_time_ms,
                        is_image_generation=False,
                        error_occurred=False,
                        error_message=None,
                        authenticated_user_id=user.id if user and hasattr(user, 'id') and user.id else None,
                        user_role=user.role if user and hasattr(user, 'role') else "anonymous",
                        chunk_references=chunk_references
                    )
                    
                    self.successful_conversations += 1
                    logger.info(f"âœ… å¯¹è¯è®°å½•æˆåŠŸï¼šID={conversation_id}, æœºå™¨äººï¼š{self.bot_name}, chunks={len(chunk_references)}")
                    
                except Exception as log_error:
                    logger.error(f"âŒ è®°å½•å¯¹è¯å¤±è´¥ï¼ˆæœºå™¨äººï¼š{self.bot_name}ï¼‰: {log_error}")

                logger.info(f"ğŸ“¤ æœºå™¨äºº '{self.bot_name}' API å›åº”è°ƒè¯•:")
                logger.info(f"  - response_text é•¿åº¦: {len(response_text)}")
                logger.info(f"  - recommended_questions: {recommended_questions}")
                logger.info(f"  - æ‰¾åˆ°æ–‡æ¡£æ•°é‡: {len(context_docs) if context_docs else 0}")
                logger.info(f"  - chunk_references: {len(chunk_references)}")
                logger.info(f"  - å¤„ç†æ—¶é—´: {processing_time_ms}ms")

                return JSONResponse({
                    "response": response_text,
                    "recommended_questions": recommended_questions if recommended_questions else [],
                    "error": False,
                    "metadata": {
                        "bot_name": self.bot_name,
                        "session_id": session_id,
                        "processing_time_ms": processing_time_ms,
                        "documents_found": len(context_docs) if context_docs else 0,
                        "conversation_id": conversation_id,
                        "chunk_count": len(chunk_references),
                        "search_mode": self.search_mode
                    }
                })

            except HTTPException as http_exc:
                error_message = str(http_exc.detail)
                processing_time_ms = int((time.time() - start_time) * 1000)
                
                try:
                    user_identifier = self.get_user_identifier(user, session_id) if 'session_id' in locals() else "unknown"
                    query = data.get("message", "æœªçŸ¥æŸ¥è¯¢") if 'data' in locals() else "æœªçŸ¥æŸ¥è¯¢"
                    
                    conversation_id = self.logger.log_conversation(
                        user_id=user_identifier,
                        user_query=query,
                        ai_response=f"HTTPé”™è¯¯: {error_message}",
                        collection_used=None,
                        retrieved_docs=[],
                        doc_similarities=[],
                        processing_time_ms=processing_time_ms,
                        is_image_generation=False,
                        error_occurred=True,
                        error_message=error_message,
                        authenticated_user_id=user.id if user and hasattr(user, 'id') and user.id else None,
                        user_role=user.role if user and hasattr(user, 'role') else "anonymous"
                    )
                    
                    self.failed_conversations += 1
                    logger.error(f"âŒ HTTPé”™è¯¯è®°å½•ï¼š{conversation_id}, æœºå™¨äººï¼š{self.bot_name}")
                    
                except Exception as log_error:
                    logger.error(f"âŒ HTTPé”™è¯¯å¯¹è¯è®°å½•å¤±è´¥ï¼ˆæœºå™¨äººï¼š{self.bot_name}ï¼‰: {log_error}")
                
                raise http_exc
                
            except Exception as e:
                error_message = str(e)
                processing_time_ms = int((time.time() - start_time) * 1000)
                
                logger.error(f"âŒ æœºå™¨äºº '{self.bot_name}' èŠå¤© API å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
                
                try:
                    user_identifier = self.get_user_identifier(user, session_id) if 'session_id' in locals() else "unknown"
                    query = data.get("message", "æœªçŸ¥æŸ¥è¯¢") if 'data' in locals() else "æœªçŸ¥æŸ¥è¯¢"
                    
                    conversation_id = self.logger.log_conversation(
                        user_id=user_identifier,
                        user_query=query,
                        ai_response=f"ç³»ç»Ÿé”™è¯¯: {error_message}",
                        collection_used=None,
                        retrieved_docs=[],
                        doc_similarities=[],
                        processing_time_ms=processing_time_ms,
                        is_image_generation=False,
                        error_occurred=True,
                        error_message=error_message,
                        authenticated_user_id=user.id if user and hasattr(user, 'id') and user.id else None,
                        user_role=user.role if user and hasattr(user, 'role') else "anonymous"
                    )
                    
                    self.failed_conversations += 1
                    logger.error(f"âŒ ç³»ç»Ÿé”™è¯¯è®°å½•ï¼š{conversation_id}, æœºå™¨äººï¼š{self.bot_name}")
                    
                except Exception as log_error:
                    logger.error(f"âŒ ç³»ç»Ÿé”™è¯¯å¯¹è¯è®°å½•å¤±è´¥ï¼ˆæœºå™¨äººï¼š{self.bot_name}ï¼‰: {log_error}")
                
                return JSONResponse({
                    "response": f"æŠ±æ­‰ï¼Œæœºå™¨äºº '{self.bot_name}' å‘ç”Ÿå†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚",
                    "error": True,
                    "error_message": error_message,
                    "recommended_questions": [],
                    "metadata": {
                        "bot_name": self.bot_name,
                        "processing_time_ms": processing_time_ms,
                        "conversation_id": conversation_id,
                        "search_mode": self.search_mode
                    }
                }, status_code=500)

        @self.app.get("/api/stats")
        async def get_bot_stats():
            """è·å–æœºå™¨äººç»Ÿè®¡ä¿¡æ¯"""
            try:
                db_stats = self.logger.get_statistics() if hasattr(self.logger, 'get_statistics') else {}
                
                return JSONResponse({
                    "bot_name": self.bot_name,
                    "display_name": self.config.get("display_name", self.bot_name),
                    "collection_name": self.collection_name,
                    "conversation_db_path": self.conversation_db_path,
                    "search_mode": self.search_mode,
                    "vector_api_url": self.vector_api_url if self.search_mode == "api" else None,
                    "session_stats": {
                        "total_conversations": self.total_conversations,
                        "successful_conversations": self.successful_conversations,
                        "failed_conversations": self.failed_conversations,
                        "active_sessions": len(self.session_counters)
                    },
                    "db_stats": db_stats,
                    "config": {
                        "temperature": self.config.get("temperature", 0.7),
                        "max_tokens": self.config.get("max_tokens", 2000),
                        "dynamic_recommendations_enabled": self.config.get("dynamic_recommendations_enabled", False),
                        "cite_sources_enabled": self.config.get("cite_sources_enabled", False)
                    }
                })
            except Exception as e:
                logger.error(f"è·å–æœºå™¨äººç»Ÿè®¡å¤±è´¥: {e}")
                return JSONResponse({"error": str(e)}, status_code=500)

        @self.app.get("/health")
        async def health_check():
            """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
            return JSONResponse({
                "status": "healthy",
                "bot_name": self.bot_name,
                "display_name": self.config.get("display_name", self.bot_name),
                "search_mode": self.search_mode,
                "timestamp": time.time(),
                "conversation_count": self.total_conversations
            })

    def _generate_response(self, query: str, context: str, system_prompt: str, session_id: str) -> Tuple[str, List[str]]:
        """ç”Ÿæˆå›åº”"""
        if not OPENAI_AVAILABLE:
            return "ç³»ç»Ÿ AI æ¨¡å—æœªè½½å…¥ã€‚", []

        openai_key = os.getenv("OPENAI_API_KEY")
        if not openai_key:
            return "ç³»ç»Ÿé…ç½®é”™è¯¯ï¼šç¼ºå°‘ OpenAI API Keyã€‚", []

        llm = ChatOpenAI(
            model=self.config.get("model", "gpt-4o-mini"), 
            temperature=self.config.get("temperature", 0.7), 
            api_key=openai_key
        )

        # ç”Ÿæˆä¸»è¦å›ç­”
        main_answer_messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"""å‚è€ƒèµ„æ–™ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}""")
        ]
        main_response = llm.invoke(main_answer_messages)
        main_answer = main_response.content.strip()

        # ç”Ÿæˆæ¨èé—®é¢˜
        recommended_questions = []
        should_recommend = self.config.get("dynamic_recommendations_enabled", False)
        recommend_count = self.config.get("dynamic_recommendations_count", 0)
        conversation_count = self.session_counters.get(session_id, 1)

        logger.info(f"ğŸ” æœºå™¨äºº '{self.bot_name}' æ¨èé—®é¢˜è°ƒè¯•:")
        logger.info(f"  - should_recommend: {should_recommend}")
        logger.info(f"  - recommend_count: {recommend_count}")
        logger.info(f"  - conversation_count: {conversation_count}")

        if should_recommend and (recommend_count == 0 or conversation_count <= recommend_count):
            logger.info(f"âœ… æœºå™¨äºº '{self.bot_name}' å¼€å§‹ç”Ÿæˆæ¨èé—®é¢˜...")
            
            recommend_prompt = f"""**åŸå§‹å¯¹è¯**
ç”¨æˆ·é—®ï¼šã€Œ{query}ã€
ä½ çš„å›ç­”ï¼šã€Œ{main_answer}ã€

---
**æŒ‡ä»¤**
æ ¹æ®ä»¥ä¸Šå¯¹è¯ï¼Œç”Ÿæˆä¸‰ä¸ªç›¸å…³çš„å»¶ä¼¸é—®é¢˜ã€‚

**æ ¼å¼**
- æ¯ä¸ªé—®é¢˜ä¸€è¡Œ
- ä¸è¦ç¼–å·
- ä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—"""
            
            try:
                recommend_messages = [HumanMessage(content=recommend_prompt)]
                recommend_response = llm.invoke(recommend_messages)
                questions_text = recommend_response.content.strip()
                
                logger.info(f"ğŸ¤– æœºå™¨äºº '{self.bot_name}' LLM åŸå§‹å›åº”:")
                logger.info(f"'{questions_text}'")
                
                recommended_questions = [q.strip() for q in questions_text.split('\n') if q.strip()]
                
                logger.info(f"ğŸ“ æœºå™¨äºº '{self.bot_name}' è§£æåçš„æ¨èé—®é¢˜: {recommended_questions}")
                
            except Exception as e:
                logger.error(f"âŒ æœºå™¨äºº '{self.bot_name}' ç”Ÿæˆæ¨èé—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                recommended_questions = []
        else:
            logger.info(f"âŒ æœºå™¨äºº '{self.bot_name}' æ¨èé—®é¢˜æœªå¯ç”¨æˆ–è¶…å‡ºé™åˆ¶")

        return main_answer, recommended_questions
    
    def _extract_links_from_content(self, content: str) -> List[dict]:
        """ä»æ–‡ä»¶å†…å®¹ä¸­æå–è¿æ¥å’Œæ ‡é¢˜ï¼ˆç¨³å¥åˆ¤å®š URL/æ ‡é¢˜é¡ºåºï¼‰"""
        links = []
        
        # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åŒ¹é…å„ç§è¿æ¥æ ¼å¼
        patterns = [
            # Markdownæ ¼å¼: [æ ‡é¢˜](URL) -> (title, url)
            r'\[([^\]]+)\]\((https?://[^\s)]+)\)',
            # HTMLæ ¼å¼: <a href="URL">æ ‡é¢˜</a> -> (url, title)
            r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>([^<]+)</a>',
            # çº¯æ–‡å­—æ ¼å¼: æ ‡é¢˜: URL  -> (title, url)
            r'([^:\n]+):\s*(https?://[^\s]+)',
            # çº¯æ–‡å­—æ ¼å¼: æ ‡é¢˜ - URL  -> (title, url)
            r'([^-\n]+)\s*-\s*(https?://[^\s]+)'
        ]
        
        url_like = re.compile(r'^https?://', re.IGNORECASE)

        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if len(match) != 2:
                    continue
                a, b = match[0].strip(), match[1].strip()

                # ä»¥æ˜¯å¦åƒ URL ä¾†æ±ºå®šæ¬„ä½å°æ‡‰ï¼Œçµ±ä¸€è¼¸å‡ºç‚º (title, url)
                if url_like.match(a) and not url_like.match(b):
                    url, title = a, b
                elif url_like.match(b) and not url_like.match(a):
                    url, title = b, a
                else:
                    # å›é€€ç­–ç•¥ï¼šæ²¿ç”¨åŸæœ¬ (title, url) å‡è¨­
                    title, url = a, b

                # æ¸…ç†æ ‡é¢˜
                title = title.strip().strip('"\'')
                if not title or len(title) < 3 or len(title) > 200:
                    continue

                # æ’é™¤ç„¡æ„ç¾©æ¨™é¡Œ
                if re.match(r'^(ç‚¹å‡»è¿™é‡Œ|é˜…è¯»æ›´å¤š|æ›´å¤šä¿¡æ¯|è¿æ¥|ç½‘å€|click here|read more|more info|link|url)$', title, re.IGNORECASE):
                    continue
                if re.match(r'^\d+$', title) or re.match(r'^[^\w\u4e00-\u9fff]+$', title):
                    continue

                links.append({"title": title, "url": url})

        return links
    
    def _find_title_for_url_in_content(self, content: str, target_url: str) -> str:
        """åœ¨æ–‡ä»¶å†…å®¹ä¸­æŸ¥æ‰¾ç‰¹å®šURLå¯¹åº”çš„æ ‡é¢˜"""
        try:
            lines = content.split('\n')
            
            for i, line in enumerate(lines):
                if target_url in line:
                    # æŸ¥çœ‹å½“å‰è¡Œå’Œå‰åå‡ è¡Œå¯»æ‰¾æ ‡é¢˜
                    context_lines = []
                    start = max(0, i - 2)
                    end = min(len(lines), i + 3)
                    context_lines = lines[start:end]
                    
                    for context_line in context_lines:
                        # è·³è¿‡åŒ…å«URLçš„è¡Œ
                        if target_url in context_line:
                            continue
                        
                        # æŸ¥æ‰¾æ ‡é¢˜æ¨¡å¼
                        context_line = context_line.strip()
                        if context_line and len(context_line) > 5 and len(context_line) < 200:
                            # ç§»é™¤å¸¸è§çš„æ ‡è®°ç¬¦å·
                            cleaned = re.sub(r'^[#*\-â€¢\d\.\s]+', '', context_line)
                            if cleaned and len(cleaned) > 5:
                                return cleaned
            
            return ""
            
        except Exception as e:
            logger.debug(f"åœ¨å†…å®¹ä¸­æŸ¥æ‰¾æ ‡é¢˜å¤±è´¥: {e}")
            return ""

    def _generate_smart_title(self, metadata: dict, url: str) -> str:
        """æ™ºæ…§ç”Ÿæˆæ ‡é¢˜"""
        try:
            # ä¼˜å…ˆçº§1: ä½¿ç”¨æ–‡ä»¶æ ‡é¢˜
            title = metadata.get('title', '')
            if title and title != 'unknown' and len(title) > 3:
                return title
            
            # ä¼˜å…ˆçº§2: ä½¿ç”¨åŸå§‹æ–‡ä»¶åç§°ï¼ˆå»é™¤å‰¯æ¡£åï¼‰
            filename = metadata.get('original_filename', metadata.get('filename', ''))
            if filename and filename != 'unknown':
                title = os.path.splitext(filename)[0]
                # ç¾åŒ–æ–‡ä»¶åç§°
                title = title.replace('-', ' ').replace('_', ' ')
                title = ' '.join(word.capitalize() for word in title.split() if word)
                if len(title) > 3:
                    return title
            
            # ä¼˜å…ˆçº§3: ä»URLè·¯å¾„æå–æ™ºæ…§æ ‡é¢˜
            from urllib.parse import urlparse, unquote
            parsed = urlparse(url)
            
            # è·å–è·¯å¾„ä¸­çš„æœ‰ç”¨ä¿¡æ¯
            path_parts = [part for part in parsed.path.strip('/').split('/') if part]
            if path_parts:
                # ä½¿ç”¨æœ€åä¸€ä¸ªæœ‰æ„ä¹‰çš„è·¯å¾„æ®µ
                last_part = path_parts[-1]
                # URLè§£ç 
                last_part = unquote(last_part)
                # ç§»é™¤å¸¸è§çš„å‰¯æ¡£å
                last_part = re.sub(r'\.(html|php|aspx|jsp|htm)$', '', last_part)
                # æ›¿æ¢åˆ†éš”ç¬¦å¹¶ç¾åŒ–
                title = last_part.replace('-', ' ').replace('_', ' ')
                title = ' '.join(word.capitalize() for word in title.split() if word)
                if len(title) > 3:
                    return title
            
            # ä¼˜å…ˆçº§4: ä½¿ç”¨æŸ¥è¯¢å‚æ•°ä¸­çš„ä¿¡æ¯
            if parsed.query:
                query_parts = parsed.query.split('&')
                for part in query_parts:
                    if '=' in part:
                        key, value = part.split('=', 1)
                        if key in ['title', 'q', 'query', 'search'] and value:
                            decoded_value = unquote(value).replace('+', ' ')
                            if len(decoded_value) > 3:
                                return decoded_value
            
            # ä¼˜å…ˆçº§5: ä½¿ç”¨åŸŸåï¼ˆæœ€åçš„å¤‡é€‰ï¼‰
            domain = parsed.netloc
            if domain:
                # ç§»é™¤wwwå‰ç¼€
                domain = re.sub(r'^www\.', '', domain)
                return f"æ¥æºï¼š{domain}"
            
            return url
            
        except Exception as e:
            logger.warning(f"ç”Ÿæˆæ™ºæ…§æ ‡é¢˜å¤±è´¥: {e}")
            return self._extract_domain_from_url(url)

    def _extract_source_urls(self, docs: List) -> List[dict]:
        """ä»æ–‡ä»¶å…ƒæ•°æ®ä¸­æå–URLå’Œå®Œæ•´æ ‡é¢˜ä¿¡æ¯"""
        sources = []
        seen_urls = set()

        for doc in docs:
            try:
                metadata = getattr(doc, 'metadata', {})
                
                # æ–¹æ³•1: æ£€æŸ¥æ˜¯å¦æœ‰é¢„å¤„ç†çš„æ ‡é¢˜-URLæ˜ å°„
                title_url_mapping = metadata.get('title_url_mapping', {})
                if isinstance(title_url_mapping, str):
                    try:
                        title_url_mapping = json.loads(title_url_mapping)
                    except json.JSONDecodeError:
                        title_url_mapping = {}
                
                # å¦‚æœæœ‰æ˜ å°„ï¼Œç›´æ¥ä½¿ç”¨
                if title_url_mapping:
                    for title, url in title_url_mapping.items():
                        if url not in seen_urls:
                            sources.append({
                                "title": title,
                                "url": url
                            })
                            seen_urls.add(url)
                    continue
                
                # æ–¹æ³•2: ä»æ–‡ä»¶å†…å®¹ä¸­æå–æ ‡é¢˜å’ŒURL
                doc_content = self._get_document_content(doc)
                extracted_links = self._extract_links_from_content(doc_content)
                for link_info in extracted_links:
                    if link_info['url'] not in seen_urls:
                        sources.append(link_info)
                        seen_urls.add(link_info['url'])
                
                # æ–¹æ³•3: ä»contained_urlsè·å–URLï¼Œç„¶åå°è¯•åŒ¹é…æ ‡é¢˜
                url_string = metadata.get('contained_urls', '')
                if url_string and not extracted_links:
                    urls_in_chunk = [url.strip() for url in url_string.split('|') if url.strip()]
                    
                    for url in urls_in_chunk:
                        if url not in seen_urls:
                            # å°è¯•ä»æ–‡ä»¶å†…å®¹ä¸­æ‰¾åˆ°å¯¹åº”çš„æ ‡é¢˜
                            title = self._find_title_for_url_in_content(doc_content, url)
                            if not title:
                                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨æ™ºæ…§æ ‡é¢˜æå–
                                title = self._generate_smart_title(metadata, url)
                            
                            sources.append({
                                "title": title,
                                "url": url
                            })
                            seen_urls.add(url)

            except Exception as e:
                logger.warning(f"ä»å…ƒæ•°æ®æå–URLæ—¶å‡ºé”™: {e}")

        logger.info(f"ä»å…ƒæ•°æ®ä¸­æå–åˆ° {len(sources)} ä¸ªè¿æ¥")
        return sources

    def _extract_domain_from_url(self, url: str) -> str:
        """ä»URLä¸­æå–åŸŸåä½œä¸ºæ ‡é¢˜"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc
            return domain if domain else url
        except:
            return url

    def _filter_duplicate_links(self, sources: List[dict], session_id: str) -> List[dict]:
        """è¿‡æ»¤é‡å¤è¿æ¥"""
        if session_id not in self.session_shown_links:
            self.session_shown_links[session_id] = set()
        
        shown_links = self.session_shown_links[session_id]
        filtered_sources = []
        
        for source in sources:
            url = source.get('url', '')
            if url and url not in shown_links:
                filtered_sources.append(source)
                shown_links.add(url)
                logger.info(f"âœ… æ–°è¿æ¥: {source.get('title', 'unknown')}")
            else:
                logger.info(f"ğŸ”„ è·³è¿‡é‡å¤è¿æ¥: {source.get('title', 'unknown')}")
        
        return filtered_sources

    def _format_source_links(self, sources: List[dict]) -> str:
        """æ ¼å¼åŒ–åƒè€ƒé€£çµ - ç¢ºä¿æ­£ç¢ºæ›è¡Œ"""
        if not sources:
            return ""
        
        # ğŸ”§ ä¿®æ­£ï¼šç¢ºä¿æ©Ÿå™¨äººå›ç­”å’Œæ¨è–¦å€å¡Šä¹‹é–“æœ‰ç©ºè¡Œ
        source_links = "\n\nğŸ’¡ ä½ å¯èƒ½æƒ³çŸ¥é“\n"  # å…©å€‹\nç¢ºä¿ç©ºè¡Œï¼Œæœ€å¾Œä¸€å€‹\nè®“æ¨™é¡Œå–®ç¨ä¸€è¡Œ
        
        formatted_items = []
        for source in sources:
            title = source["title"]
            url = source["url"]
            formatted_items.append(f"- [{title}]({url})")
        
        source_links += "\n".join(formatted_items)
        return source_links
    
    def _clean_and_validate_title(self, title: str) -> str:
        """æ¸…ç†"""
        if not title:
            return ""
        
        # æ¸…ç†æ ‡é¢˜
        title = title.strip()
        title = re.sub(r'\s+', ' ', title)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
        title = re.sub(r'^[^\w\u4e00-\u9fff]+', '', title)  # ç§»é™¤å¼€å¤´çš„ç¬¦å·
        title = re.sub(r'[^\w\u4e00-\u9fff]+$', '', title)  # ç§»é™¤ç»“å°¾çš„ç¬¦å·
        
        # éªŒè¯æ ‡é¢˜é•¿åº¦å’Œå†…å®¹
        if len(title) < 3 or len(title) > 200:
            return ""
        
        # æ’é™¤æ— æ„ä¹‰çš„æ ‡é¢˜
        meaningless_patterns = [
            r'^(|æ›´å¤šä¿¡æ¯|é€£æ¥|ç¶²å€|click here|read more|more info|link|url)$',
            r'^\d+$',  # çº¯æ•°å­—
            r'^[^\w\u4e00-\u9fff]+$',  # åªæœ‰ç¬¦å·
        ]
        
        for pattern in meaningless_patterns:
            if re.match(pattern, title, re.IGNORECASE):
                return ""
        
        return title


def main():
    parser = argparse.ArgumentParser(description="å¯åŠ¨ä¸€ä¸ªç‹¬ç«‹çš„èŠå¤©æœºå™¨äººå®ä¾‹ã€‚")
    parser.add_argument("--bot-name", type=str, required=True, help="è¦å¯åŠ¨çš„æœºå™¨äººåç§°")
    args = parser.parse_args()

    try:
        instance = ChatbotInstance(args.bot_name)
        port = instance.config.get("port")
        if not port:
            raise ValueError(f"è®¾å®šæ–‡ä»¶ '{args.bot_name}.json' ä¸­æœªæŒ‡å®šç«¯å£")
        
        logger.info(f"ğŸ¤– æœºå™¨äºº '{instance.bot_name}' æ­£åœ¨ http://localhost:{port} ä¸Šå¯åŠ¨")
        logger.info(f"ğŸ“Š å¯¹è¯è®°å½•æ•°æ®åº“å·²åœ¨å®ä¾‹ä¸­é…ç½®ã€‚")
        logger.info(f"ğŸ“š çŸ¥è¯†åº“é›†åˆï¼š{instance.collection_name}")
        logger.info(f"ğŸ” æœç´¢æ¨¡å¼ï¼š{instance.search_mode}")
        if instance.search_mode == "api":
            logger.info(f"ğŸ”— å‘é‡APIåœ°å€ï¼š{instance.vector_api_url}")
        logger.info(f"ğŸ”§ è°ƒè¯•æ¨¡å¼å·²å¯ç”¨ï¼Œå°†è¾“å‡ºè¯¦ç»†çš„å¯¹è¯è®°å½•å’Œæ¨èé—®é¢˜ç”Ÿæˆæ—¥å¿—")
        
        uvicorn.run(instance.app, host="0.0.0.0", port=int(port))

    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨æœºå™¨äºº '{args.bot_name}' å¤±è´¥: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()