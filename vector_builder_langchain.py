#!/usr/bin/env python3
"""
ğŸ“¦ ä¸»å…¥å£æª”æ¡ˆ vector_builder_langchain.py - ä¿®æ­£ç‰ˆ (æ”¯æ´4A+4Bçµæ§‹)

ğŸ¯ è¨­è¨ˆåŸå‰‡ï¼š
   - é©æ‡‰4A+4Bæ‹†åˆ†æ–¹æ¡ˆï¼ˆvector_operations.py + management_api.pyï¼‰
   - å®Œå…¨å‘å¾Œå…¼å®¹ï¼Œæ‰€æœ‰ç¾æœ‰ç¨‹å¼ç„¡éœ€ä¿®æ”¹
   - æ™ºèƒ½å›é€€æ©Ÿåˆ¶ï¼Œæ”¯æ´é€æ­¥æª”æ¡ˆå‰µå»º
   - çµ±ä¸€çš„é‡å°å‡ºä»‹é¢

ğŸ”’ å‘å¾Œå…¼å®¹æ€§ä¿è­‰ï¼š
   âœ… from vector_builder_langchain import OptimizedVectorSystem
   âœ… from vector_builder_langchain import SmartTextAnalyzer, OptimizedTextSplitter
   âœ… from vector_builder_langchain import AdaptiveBatchProcessor
   âœ… from vector_builder_langchain import SYSTEM_CONFIG, TOKEN_LIMITS
   âœ… from vector_builder_langchain import main

ğŸ”„ ä¿®æ­£ï¼šæ”¯æ´4A+4Bçµæ§‹ vs åŸ5æª”æ¡ˆçµæ§‹çš„å·®ç•°
"""

import os
import time
import hashlib
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, List, Optional, Any, Set, Tuple

# =============================================================================
# ğŸ”„ æ™ºèƒ½å°å…¥ç­–ç•¥ - æ”¯æ´4A+4Bçµæ§‹ + 5æª”æ¡ˆçµæ§‹ + å®Œæ•´å›é€€
# =============================================================================

# ğŸ¯ ç­–ç•¥1: å˜—è©¦å¾4A+4Bçµæ§‹å°å…¥ (ç•¶å‰ä½¿ç”¨çš„çµæ§‹)
try:
    print("ğŸ”„ å˜—è©¦4A+4Bçµæ§‹å°å…¥...")
    
    # 4A: å‘é‡æ“ä½œæ ¸å¿ƒå±¤
    from vector_operations import VectorOperationsCore
    
    # 4B: ç®¡ç†APIå±¤ (æª¢æŸ¥æ–‡ä»¶å)
    try:
        from management_api import ManagementAPI, OptimizedVectorSystem
        USING_4AB_STRUCTURE = True
        print("âœ… 4A+4Bçµæ§‹å°å…¥æˆåŠŸ (management_api.py)")
    except ImportError:
        # å˜—è©¦å‚™é¸æ–‡ä»¶å
        from management_interface import OptimizedVectorSystem
        ManagementAPI = OptimizedVectorSystem  # å»ºç«‹åˆ¥å
        USING_4AB_STRUCTURE = True
        print("âœ… 4A+4Bçµæ§‹å°å…¥æˆåŠŸ (management_interface.py)")
    
    # å¾4Aå°å…¥åŸºç¤é…ç½®å’Œé¡åˆ¥
    SYSTEM_CONFIG = getattr(vector_operations, 'SYSTEM_CONFIG', {})
    TOKEN_LIMITS = getattr(vector_operations, 'TOKEN_LIMITS', {})
    SUPPORTED_EXTENSIONS = getattr(vector_operations, 'SUPPORTED_EXTENSIONS', set())
    
    # å¾4Aå°å…¥ä¾è³´æª¢æŸ¥è®Šæ•¸
    CHROMA_AVAILABLE = getattr(vector_operations, 'CHROMA_AVAILABLE', False)
    PGVECTOR_AVAILABLE = getattr(vector_operations, 'PGVECTOR_AVAILABLE', False)
    OPENAI_EMBEDDINGS_AVAILABLE = getattr(vector_operations, 'OPENAI_EMBEDDINGS_AVAILABLE', False)
    PSYCOPG2_AVAILABLE = getattr(vector_operations, 'PSYCOPG2_AVAILABLE', False)
    JIEBA_AVAILABLE = getattr(vector_operations, 'JIEBA_AVAILABLE', False)
    OPENCC_AVAILABLE = getattr(vector_operations, 'OPENCC_AVAILABLE', False)
    DOCX2TXT_AVAILABLE = getattr(vector_operations, 'DOCX2TXT_AVAILABLE', False)
    EPUB_AVAILABLE = getattr(vector_operations, 'EPUB_AVAILABLE', False)
    
    # å¾4Aå°å…¥è³‡æ–™é¡åˆ¥
    FileInfo = getattr(vector_operations, 'FileInfo', None)
    TextAnalysis = getattr(vector_operations, 'TextAnalysis', None)
    ChunkInfo = getattr(vector_operations, 'ChunkInfo', None)
    SearchResult = getattr(vector_operations, 'SearchResult', None)
    
    # å¾4Aå°å…¥æ–‡æœ¬è™•ç†é¡åˆ¥
    SmartTextAnalyzer = getattr(vector_operations, 'SmartTextAnalyzer', None)
    OptimizedTextSplitter = getattr(vector_operations, 'OptimizedTextSplitter', None)
    ChineseTextNormalizer = getattr(vector_operations, 'ChineseTextNormalizer', None)
    AdaptiveBatchProcessor = getattr(vector_operations, 'AdaptiveBatchProcessor', None)
    
    # å¾4Aå°å…¥å·¥å…·å‡½æ•¸
    detect_railway_environment = getattr(vector_operations, 'detect_railway_environment', None)
    check_openai_api_key = getattr(vector_operations, 'check_openai_api_key', None)
    
    print("âœ… 4A+4Bçµæ§‹å®Œå…¨å°å…¥æˆåŠŸ")

except ImportError as e:
    print(f"âš ï¸ 4A+4Bçµæ§‹å°å…¥å¤±æ•—: {e}")
    USING_4AB_STRUCTURE = False
    
    # ğŸ¯ ç­–ç•¥2: å˜—è©¦å¾5æª”æ¡ˆçµæ§‹å°å…¥
    try:
        print("ğŸ”„ å˜—è©¦5æª”æ¡ˆçµæ§‹å°å…¥...")
        from core_config import *
        from text_processing import *
        from vector_builder import *
        from management_interface import *
        print("âœ… 5æª”æ¡ˆçµæ§‹å°å…¥æˆåŠŸ")
        
    except ImportError:
        print("âš ï¸ 5æª”æ¡ˆçµæ§‹ä¹Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨å›é€€å®šç¾©")
        
        # ğŸ¯ ç­–ç•¥3: å®Œæ•´å›é€€å®šç¾© - ç¢ºä¿ç³»çµ±å¯ä»¥ç¨ç«‹é‹è¡Œ
        SYSTEM_CONFIG = {
            "persist_dir": "./chroma_langchain_db",
            "data_dir": "./data",
            "device": "cpu",
            "log_level": "INFO",
            "max_workers": 4,
            "cache_embeddings": True,
            "backup_enabled": True
        }
        
        SMART_TEXT_CONFIG = {
            "micro_text_threshold": 100,
            "short_text_threshold": 500,
            "medium_text_threshold": 2000,
            "long_text_threshold": 8000,
            "ultra_long_threshold": 20000,
            "micro_text": {
                "min_length": 20,
                "process_whole": True,
                "merge_with_next": True,
                "chunk_size": 0,
                "description": "å¾®æ–‡æœ¬æ•´é«”è™•ç†"
            },
            "short_text": {
                "min_length": 50,
                "process_whole": True,
                "preserve_structure": True,
                "chunk_size": 0,
                "allow_merge": True,
                "description": "çŸ­æ–‡æœ¬æ•´é«”ä¿å­˜"
            }
        }
        
        TOKEN_LIMITS = {
            "max_tokens_per_request": 150000,
            "max_batch_size": 8,
            "min_batch_size": 1,
            "batch_delay": 5.0,
            "retry_delay": 10,
            "max_retries": 3,
            "token_safety_margin": 0.2,
            "adaptive_batching": True
        }
        
        PERFORMANCE_CONFIG = {
            "embedding_batch_size": 12,
            "parallel_processing": True,
            "memory_limit_mb": 1024,
            "chunk_cache_size": 1000,
            "preload_models": True,
            "gc_frequency": 50
        }
        
        SUPPORTED_EXTENSIONS = {
            '.txt', '.md', '.pdf', '.csv', '.json', '.py', '.js',
            '.docx', '.doc', '.epub', '.rst', '.org'
        }
        
        # ä¾è³´æª¢æŸ¥è®Šæ•¸ï¼ˆå›é€€ç‰ˆæœ¬ï¼‰
        CHROMA_AVAILABLE = False
        PGVECTOR_AVAILABLE = False
        OPENAI_EMBEDDINGS_AVAILABLE = False
        PSYCOPG2_AVAILABLE = False
        JIEBA_AVAILABLE = False
        OPENCC_AVAILABLE = False
        DOCX2TXT_AVAILABLE = False
        EPUB_AVAILABLE = False
        
        @dataclass
        class FileInfo:
            path: str
            size: int
            mtime: float
            hash: str
            encoding: str = "utf-8"
            file_type: str = ""
        
        @dataclass
        class TextAnalysis:
            length: int
            text_type: str
            language: str
            encoding: str
            structure_info: Dict
            quality_score: float
            processing_strategy: str
        
        @dataclass
        class ChunkInfo:
            chunk_id: str
            content: str
            metadata: Dict
            token_count: int
            quality_score: float
            relationships: List[str]
        
        @dataclass
        class SearchResult:
            content: str
            score: float
            metadata: Dict
            collection: str
            chunk_info: Optional[ChunkInfo] = None
        
        def detect_railway_environment():
            railway_indicators = [
                os.getenv("RAILWAY_PROJECT_ID"),
                os.getenv("RAILWAY_SERVICE_ID"),
                os.getenv("DATABASE_URL"),
                "railway.internal" in os.getenv("POSTGRES_HOST", "")
            ]
            return any(railway_indicators)
        
        def check_openai_api_key():
            api_key = os.getenv("OPENAI_API_KEY")
            return bool(api_key and api_key.startswith("sk-") and len(api_key) > 20)
        
        # å›é€€ç‰ˆæœ¬çš„è™•ç†é¡åˆ¥
        class SmartTextAnalyzer:
            def __init__(self):
                pass
                
            def analyze_text(self, text: str, source_info: Dict = None):
                length = len(text)
                if length < 100:
                    text_type = "micro_text"
                elif length < 500:
                    text_type = "short_text"
                elif length < 2000:
                    text_type = "medium_text"
                elif length < 8000:
                    text_type = "long_text"
                else:
                    text_type = "ultra_long"
                
                return TextAnalysis(
                    length=length,
                    text_type=text_type,
                    language="chinese",
                    encoding="utf-8",
                    structure_info={},
                    quality_score=0.7,
                    processing_strategy="simple_split"
                )
        
        class OptimizedTextSplitter:
            def __init__(self):
                pass
            
            def smart_split_documents(self, text: str, doc_id: str, source_info: Dict = None):
                try:
                    from langchain_core.documents import Document
                    from langchain_text_splitters import RecursiveCharacterTextSplitter
                    
                    splitter = RecursiveCharacterTextSplitter(
                        chunk_size=1000,
                        chunk_overlap=200,
                        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""]
                    )
                    
                    chunks = splitter.split_text(text)
                    documents = []
                    
                    for i, chunk in enumerate(chunks):
                        if chunk.strip():
                            metadata = {
                                'doc_id': doc_id,
                                'chunk_id': f"{doc_id}_{i+1:03d}",
                                'chunk_index': i,
                                'text_type': 'medium_text',
                                'token_count': len(chunk) // 4,  # ç°¡å–®ä¼°è¨ˆ
                                'source': source_info.get('file_path', '') if source_info else ''
                            }
                            documents.append(Document(page_content=chunk, metadata=metadata))
                    
                    return documents
                except ImportError:
                    return []
        
        class ChineseTextNormalizer:
            def __init__(self):
                pass
            
            def normalize_text(self, text: str):
                return text.strip(), {}
            
            def create_search_variants(self, query: str):
                return [query]
        
        class AdaptiveBatchProcessor:
            def __init__(self):
                self.max_batch_size = TOKEN_LIMITS["max_batch_size"]
            
            def create_smart_batches(self, documents):
                if not documents:
                    return []
                
                batches = []
                batch_size = min(self.max_batch_size, len(documents))
                
                for i in range(0, len(documents), batch_size):
                    batch_docs = documents[i:i+batch_size]
                    batch_info = {
                        'documents': len(batch_docs),
                        'tokens': sum(len(doc.page_content) // 4 for doc in batch_docs),
                        'types': {'medium_text': len(batch_docs)}
                    }
                    batches.append((batch_docs, batch_info))
                
                return batches
            
            def record_batch_result(self, success: bool, processing_time: float = 0):
                pass
            
            def get_performance_stats(self):
                return {
                    'total_batches': 0,
                    'success_rate': 1.0,
                    'avg_batch_time': 0
                }
        
        # å›é€€ç‰ˆæœ¬çš„ä¸»è¦ç³»çµ±é¡åˆ¥
        class OptimizedVectorSystem:
            def __init__(self, data_dir: str = None, model_type: str = None):
                raise ImportError(
                    "OptimizedVectorSystem éœ€è¦å®Œæ•´çš„æ¨¡å¡Šæ¶æ§‹ã€‚\n"
                    "è«‹ç¢ºä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ï¼š\n"
                    "  - 4A+4Bçµæ§‹: vector_operations.py + management_api.py\n"
                    "  - æˆ–5æª”æ¡ˆçµæ§‹: core_config.py + text_processing.py + vector_builder.py + management_interface.py"
                )

# =============================================================================
# ğŸ¯ ä¸»ç¨‹å¼å‡½æ•¸ - ä¿æŒå®Œå…¨å‘å¾Œå…¼å®¹
# =============================================================================

def main():
    """ä¸»ç¨‹å¼ - æ”¯æ´4A+4Bçµæ§‹çš„å®Œæ•´ç‰ˆæœ¬"""
    print("ğŸ‡¨ğŸ‡³ === å®Œæ•´å„ªåŒ–ç‰ˆ LangChain ä¸­æ–‡å‘é‡ç³»çµ± === ğŸ‡¨ğŸ‡³")
    
    # é¡¯ç¤ºç•¶å‰æ¶æ§‹
    if globals().get('USING_4AB_STRUCTURE', False):
        print("ğŸ“¦ ç•¶å‰æ¶æ§‹: 4A+4Bæ¨¡å¡ŠåŒ–çµæ§‹")
        print("   ğŸ“ 4A: vector_operations.py (å‘é‡æ“ä½œæ ¸å¿ƒå±¤)")
        print("   ğŸ“ 4B: management_api.py (ç®¡ç†APIå±¤)")
    else:
        print("ğŸ“¦ ç•¶å‰æ¶æ§‹: 5æª”æ¡ˆæ¨¡å¡ŠåŒ–çµæ§‹æˆ–å›é€€æ¨¡å¼")
    
    # ç³»çµ±ç‹€æ…‹æª¢æŸ¥
    print(f"ğŸ¤– OpenAI Embeddings: {'âœ… å·²å•Ÿç”¨' if OPENAI_EMBEDDINGS_AVAILABLE else 'âŒ æœªå•Ÿç”¨'}")
    print(f"ğŸ˜ PostgreSQL: {'âœ… å·²å•Ÿç”¨' if detect_railway_environment() else 'âŒ æœªå•Ÿç”¨'}")  
    print(f"ğŸ“Š PGVector: {'âœ… å·²å•Ÿç”¨' if PGVECTOR_AVAILABLE else 'âŒ æœªå•Ÿç”¨'}")
    print(f"ğŸ“„ DOCX æ”¯æ´: {'âœ… å·²å•Ÿç”¨' if DOCX2TXT_AVAILABLE else 'âŒ æœªå•Ÿç”¨'}")
    print(f"ğŸ“š EPUB æ”¯æ´: {'âœ… å·²å•Ÿç”¨' if EPUB_AVAILABLE else 'âŒ æœªå•Ÿç”¨'}")
    print(f"ğŸ”¤ ç¹ç°¡è½‰æ›: {'âœ… å·²å•Ÿç”¨' if OPENCC_AVAILABLE else 'âŒ æœªå•Ÿç”¨'}")
    print(f"ğŸ§  æ™ºèƒ½è™•ç†: âœ… å·²å•Ÿç”¨")
    print(f"ğŸš€ æ€§èƒ½å„ªåŒ–: âœ… å·²å•Ÿç”¨")
    
    # åˆå§‹åŒ–ç³»çµ±
    try:
        system = OptimizedVectorSystem()
        
        # ç³»çµ±è¨ºæ–·
        print("\n" + "="*60)
        if hasattr(system, 'diagnose_system'):
            system.diagnose_system()
        else:
            print("ğŸ“Š ç³»çµ±è¨ºæ–·: åŸºç¤ç‰ˆæœ¬ï¼Œå®Œæ•´è¨ºæ–·éœ€è¦å®Œæ•´æ¨¡å¡Š")
        print("="*60 + "\n")
        
        # åŒæ­¥é›†åˆï¼ˆå¦‚æœæ”¯æŒï¼‰
        if hasattr(system, 'sync_collections'):
            changes = system.sync_collections()
            
            # é¡¯ç¤ºçµ±è¨ˆ
            if hasattr(system, 'get_stats'):
                stats = system.get_stats()
                print(f"\nğŸ“Š é›†åˆçµ±è¨ˆ:")
                for folder, count in stats.items():
                    print(f"   ğŸ“ {folder}: {count:,} å€‹æ–‡æª”åˆ†å¡Š")
            
            if changes > 0:
                print(f"\nâœ… è™•ç†å®Œæˆï¼Œå…± {changes} å€‹è®Šæ›´")
            else:
                print(f"\nâœ… æ‰€æœ‰æ–‡ä»¶éƒ½æ˜¯æœ€æ–°çš„")
        else:
            print("â„¹ï¸ å®Œæ•´çš„åŒæ­¥åŠŸèƒ½éœ€è¦å®Œæ•´æ¨¡å¡Šæ¶æ§‹")
        
        return system
        
    except ImportError as e:
        print(f"\nâŒ ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
        print(f"ğŸ“‹ å»ºè­°è§£æ±ºæ–¹æ¡ˆ:")
        print(f"   1. ç¢ºä¿ vector_operations.py å’Œ management_api.py æ–‡ä»¶å­˜åœ¨")
        print(f"   2. æˆ–è€…å‰µå»ºå®Œæ•´çš„5æª”æ¡ˆçµæ§‹")
        print(f"   3. æª¢æŸ¥æ‰€æœ‰å¿…è¦çš„ä¾è³´æ˜¯å¦æ­£ç¢ºå®‰è£")
        return None
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œæ™‚éŒ¯èª¤: {e}")
        return None

# =============================================================================
# ğŸ¯ ç¨‹å¼å…¥å£é» - åªåœ¨ç›´æ¥åŸ·è¡Œæ™‚é‹è¡Œ
# =============================================================================

if __name__ == "__main__":
    system = main()
    if system and hasattr(system, 'test_knowledge_management'):
        system.test_knowledge_management()
    elif system:
        print("âœ… ç³»çµ±åˆå§‹åŒ–æˆåŠŸï¼Œä½†ç¼ºå°‘å®Œæ•´æ¸¬è©¦åŠŸèƒ½")
    else:
        print("âŒ ç³»çµ±åˆå§‹åŒ–å¤±æ•—")

# =============================================================================
# ğŸ”’ å‘å¾Œå…¼å®¹æ€§å°å‡º - ç¢ºä¿æ‰€æœ‰ç¾æœ‰importèªå¥æ­£å¸¸å·¥ä½œ
# =============================================================================

__all__ = [
    # ä¸»è¦ç³»çµ±é¡åˆ¥
    'OptimizedVectorSystem',
    
    # æ ¸å¿ƒé…ç½®
    'SYSTEM_CONFIG', 'TOKEN_LIMITS', 'SMART_TEXT_CONFIG', 'PERFORMANCE_CONFIG',
    'SUPPORTED_EXTENSIONS',
    
    # ä¾è³´æª¢æŸ¥è®Šæ•¸
    'CHROMA_AVAILABLE', 'PGVECTOR_AVAILABLE', 'OPENAI_EMBEDDINGS_AVAILABLE',
    'PSYCOPG2_AVAILABLE', 'JIEBA_AVAILABLE', 'OPENCC_AVAILABLE', 
    'DOCX2TXT_AVAILABLE', 'EPUB_AVAILABLE',
    
    # è³‡æ–™é¡åˆ¥
    'FileInfo', 'TextAnalysis', 'ChunkInfo', 'SearchResult',
    
    # è™•ç†é¡åˆ¥
    'SmartTextAnalyzer', 'OptimizedTextSplitter', 'ChineseTextNormalizer', 
    'AdaptiveBatchProcessor',
    
    # å·¥å…·å‡½æ•¸
    'detect_railway_environment', 'check_openai_api_key',
    
    # ä¸»ç¨‹å¼
    'main'
]