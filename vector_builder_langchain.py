#!/usr/bin/env python3
"""
å®Œæ•´å„ªåŒ–ç‰ˆ LangChain å‘é‡ç³»çµ± - æ™ºèƒ½é•·çŸ­æ–‡æœ¬è™•ç†
- ğŸ§  æ™ºèƒ½æ–‡æœ¬é•·åº¦åˆ†é¡å’Œè™•ç†ç­–ç•¥
- ğŸ“ ç²¾ç¢ºçš„ Token ä¼°ç®—å’Œæ‰¹æ¬¡è™•ç†
- ğŸ“„ ä¿æŒæ–‡æª”çµæ§‹å®Œæ•´æ€§
- ğŸ”§ å„ªåŒ–çš„åˆ†å‰²é‚è¼¯
- ğŸš€ æ€§èƒ½å’ŒéŒ¯èª¤è™•ç†å„ªåŒ–
- Python 3.11.7 ç’°å¢ƒ
"""

import json
import shutil
import time
import hashlib
import re
import logging
import os
import gc
import threading
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set, Any, Union
from dataclasses import dataclass, asdict
import random
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import tempfile
# ğŸ”§ å¼·åˆ¶è¼‰å…¥ç’°å¢ƒè®Šæ•¸
from dotenv import load_dotenv
from database_adapter import DatabaseFactory, PostgreSQLAdapter
load_dotenv('.env', override=True)
logger = logging.getLogger(__name__)

import urllib.parse
from urllib.parse import quote_plus

import sys
from pathlib import Path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
from config import app_config  # â­ çµ±ä¸€å°å…¥



# ğŸ†• 1. æ·»åŠ  Railway ç’°å¢ƒæª¢æ¸¬å‡½æ•¸ï¼ˆæ”¾åœ¨æ–‡ä»¶é ‚éƒ¨ï¼Œå°å…¥èªå¥ä¹‹å¾Œï¼‰
def detect_railway_environment():
    """æª¢æ¸¬æ˜¯å¦åœ¨ Railway ç’°å¢ƒä¸­é‹è¡Œ"""
    railway_indicators = [
        os.getenv("RAILWAY_PROJECT_ID"),
        os.getenv("RAILWAY_SERVICE_ID"), 
        os.getenv("DATABASE_URL"),
        "railway.internal" in os.getenv("POSTGRES_HOST", "")
    ]
    
    is_railway = any(railway_indicators)
    if is_railway:
        print("ğŸš‚ æª¢æ¸¬åˆ° Railway éƒ¨ç½²ç’°å¢ƒ")
        # é¡¯ç¤º Railway ç‰¹å®šä¿¡æ¯
        project_id = os.getenv("RAILWAY_PROJECT_ID", "unknown")
        service_id = os.getenv("RAILWAY_SERVICE_ID", "unknown")  
        print(f"   é …ç›®ID: {project_id[:8]}***")
        print(f"   æœå‹™ID: {service_id[:8]}***")
    
    return is_railway


# ğŸ”§ å„ªåŒ–ç‰ˆç³»çµ±é…ç½®
SYSTEM_CONFIG = {
    "persist_dir": "./chroma_langchain_db",
    "data_dir": "./data", 
    "device": "cpu",  
    "log_level": "INFO",
    "max_workers": 4,  # ä¸¦ç™¼è™•ç†æ•¸
    "cache_embeddings": True,  # å¿«å–åµŒå…¥å‘é‡
    "backup_enabled": True  # è‡ªå‹•å‚™ä»½
}

# ğŸ§  æ™ºèƒ½æ–‡æœ¬åˆ†é¡é…ç½®
SMART_TEXT_CONFIG = {
    # æ–‡æœ¬é•·åº¦é–¾å€¼ï¼ˆå­—ç¬¦æ•¸ï¼‰
    "micro_text_threshold": 100,      # å¾®æ–‡æœ¬ï¼šæ¨™é¡Œã€æ‘˜è¦ç­‰
    "short_text_threshold": 500,      # çŸ­æ–‡æœ¬ï¼šçŸ­æ–‡ç« ã€æ®µè½
    "medium_text_threshold": 2000,    # ä¸­ç­‰æ–‡æœ¬ï¼šä¸­ç¯‡æ–‡ç« 
    "long_text_threshold": 8000,      # é•·æ–‡æœ¬ï¼šé•·ç¯‡æ–‡ç« 
    "ultra_long_threshold": 20000,    # è¶…é•·æ–‡æœ¬ï¼šæ›¸ç±ç« ç¯€
    
    # å„é¡æ–‡æœ¬çš„è™•ç†ç­–ç•¥
    "micro_text": {
        "min_length": 20,
        "process_whole": True,
        "merge_with_next": True,  # å˜—è©¦èˆ‡ä¸‹ä¸€å€‹ç‰‡æ®µåˆä½µ
        "chunk_size": 0,
        "description": "å¾®æ–‡æœ¬æ•´é«”è™•ç†"
    },
    
    "short_text": {
        "min_length": 50,
        "process_whole": True,
        "preserve_structure": True,
        "chunk_size": 0,
        "allow_merge": True,  # å…è¨±åˆä½µç›¸é„°çŸ­æ–‡æœ¬
        "description": "çŸ­æ–‡æœ¬æ•´é«”ä¿å­˜"
    },
    
    "medium_text": {
        "chunk_size": 800,
        "chunk_overlap": 120,
        "preserve_paragraphs": True,
        "smart_boundaries": True,  # æ™ºèƒ½é‚Šç•Œæª¢æ¸¬
        "min_chunk_ratio": 0.3,   # æœ€å°ç‰‡æ®µæ¯”ä¾‹
        "description": "ä¸­ç­‰æ–‡æœ¬æ®µè½æ„ŸçŸ¥åˆ†å‰²"
    },
    
    "long_text": {
        "chunk_size": 500,
        "chunk_overlap": 80,
        "hierarchical_split": True,
        "section_aware": True,    # ç« ç¯€æ„ŸçŸ¥
        "quality_check": True,    # è³ªé‡æª¢æŸ¥
        "description": "é•·æ–‡æœ¬ç²¾ç´°åŒ–åˆ†å‰²"
    },
    
    "ultra_long": {
        "chunk_size": 600,
        "chunk_overlap": 100,
        "multi_level_split": True,  # å¤šå±¤ç´šåˆ†å‰²
        "chapter_detection": True,  # ç« ç¯€æª¢æ¸¬
        "summary_chunks": True,     # ç”Ÿæˆæ‘˜è¦ç‰‡æ®µ
        "description": "è¶…é•·æ–‡æœ¬éšå±¤å¼è™•ç†"
    },
    
    "mega_text": {
        "chunk_size": 500,
        "chunk_overlap": 80,
        "multi_level_split": True,  # å¤šå±¤ç´šåˆ†å‰²
        "chapter_detection": True,  # ç« ç¯€æª¢æ¸¬
        "aggressive_split": True,   # ç©æ¥µåˆ†å‰²
        "quality_filter": True,     # è³ªé‡éæ¿¾
        "description": "è¶…å¤§æ–‡æœ¬ç©æ¥µåˆ†å‰²è™•ç†"
    }
}

# ğŸ”§ å„ªåŒ–ç‰ˆ Token é™åˆ¶é…ç½®
TOKEN_LIMITS = {
    "max_tokens_per_request": 150000,  # æ›´ä¿å®ˆçš„é™åˆ¶
    "max_batch_size": 8,               # æ¸›å°æ‰¹æ¬¡å¤§å°
    "min_batch_size": 1,
    "batch_delay": 5.0,                # å¢åŠ æ‰¹æ¬¡é–“å»¶é²
    "retry_delay": 10,
    "max_retries": 3,
    "token_safety_margin": 0.2,        # 20% å®‰å…¨é‚Šéš›
    "adaptive_batching": True           # è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°
}

# ğŸ”§ æ€§èƒ½å„ªåŒ–é…ç½®
PERFORMANCE_CONFIG = {
    "embedding_batch_size": 12,
    "parallel_processing": True,
    "memory_limit_mb": 1024,      # è¨˜æ†¶é«”é™åˆ¶
    "chunk_cache_size": 1000,     # åˆ†å¡Šå¿«å–å¤§å°
    "preload_models": True,       # é è¼‰å…¥æ¨¡å‹
    "gc_frequency": 50            # åƒåœ¾å›æ”¶é »ç‡
}

# æ”¯æ´çš„æ–‡ä»¶æ ¼å¼æ“´å±•
SUPPORTED_EXTENSIONS = {
    '.txt', '.md', '.pdf', '.csv', '.json', '.py', '.js', 
    '.docx', '.doc', '.epub', '.rst', '.org'
}

from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from langchain_community.vectorstores import Chroma

try:
    from langchain_community.vectorstores import Chroma
    CHROMA_AVAILABLE = True
except ImportError:
    CHROMA_AVAILABLE = False
    # å‰µå»ºä¸€å€‹è™›æ“¬çš„ Chroma é¡å‹ç”¨æ–¼é¡å‹è¨»è§£
    class Chroma:
        pass



# ğŸ”§ æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
def check_openai_api_key():
    """æª¢æŸ¥ OpenAI API Key"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸  æœªè¨­ç½® OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
        print("   è«‹åœ¨ .env æ–‡ä»¶ä¸­è¨­ç½®: OPENAI_API_KEY=sk-your-api-key")
        return False
    
    if api_key.startswith("sk-") and len(api_key) > 20:
        print("âœ… OpenAI API Key æ ¼å¼æ­£ç¢º")
        print(f"   API Key: {api_key[:8]}***{api_key[-4:]}")
        return True
    else:
        print("âš ï¸  OpenAI API Key æ ¼å¼å¯èƒ½ä¸æ­£ç¢º")
        return False

# LangChain æ ¸å¿ƒçµ„ä»¶å°å…¥
try:
    from langchain_postgres import PGVector
    PGVECTOR_AVAILABLE = True
    print("âœ… PGVector å¯ç”¨")
except ImportError:
    try:
        from langchain_community.vectorstores import PGVector
        PGVECTOR_AVAILABLE = True
        print("âœ… PGVector (community) å¯ç”¨")
    except ImportError:
        PGVECTOR_AVAILABLE = False
        print("âŒ PGVector ä¸å¯ç”¨")
        # å¯ä»¥å›é€€åˆ° Chroma
        from langchain_community.vectorstores import Chroma

# OpenAI embeddings å°å…¥
try:
    from langchain_openai import OpenAIEmbeddings
    OPENAI_EMBEDDINGS_AVAILABLE = True
    print("âœ… OpenAI Embeddings å¯ç”¨")
    
    if check_openai_api_key():
        print("âœ… OpenAI ç’°å¢ƒæª¢æŸ¥é€šé")
    else:
        print("âš ï¸ OpenAI ç’°å¢ƒé…ç½®å¯èƒ½æœ‰å•é¡Œ")
        
except ImportError as e:
    OPENAI_EMBEDDINGS_AVAILABLE = False
    print(f"âš ï¸ OpenAI Embeddings ä¸å¯ç”¨: {e}")

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    TextLoader, PyPDFLoader, Docx2txtLoader, 
    CSVLoader, JSONLoader
)

# ğŸ”§ PostgreSQL ä¾è³´æª¢æŸ¥
try:
    import psycopg2
    PSYCOPG2_AVAILABLE = True
except ImportError:
    PSYCOPG2_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: psycopg2 æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install psycopg2-binary")

"""
# ğŸ˜ PostgreSQL é…ç½®
POSTGRES_CONFIG = {
    "host": os.getenv("POSTGRES_HOST", "localhost"),
    "port": int(os.getenv("POSTGRES_PORT", "5432")),
    "database": os.getenv("POSTGRES_DATABASE", "chatbot_system"),
    "user": os.getenv("POSTGRES_USER", "chatbot_user"),
    "password": os.getenv("POSTGRES_PASSWORD", "chatbot123"),
    "schema": os.getenv("POSTGRES_SCHEMA", "public"),
}"""

# ğŸ“ é€æ­¥ä¿®æ”¹æ‚¨çš„ vector_builder_langchain.py æ–‡ä»¶

# ====================
# æ­¥é©Ÿ 1: åœ¨æ–‡ä»¶é ‚éƒ¨æ·»åŠ å°å…¥èªå¥
# ====================
# åœ¨æ‚¨çš„æ–‡ä»¶é ‚éƒ¨ï¼Œæ‰¾åˆ°å…¶ä»–å°å…¥èªå¥çš„åœ°æ–¹ï¼Œæ·»åŠ é€™å…©è¡Œï¼š

import urllib.parse  # âœ… Python å…§å»ºï¼Œä¸éœ€è¦å®‰è£
from urllib.parse import quote_plus  # âœ… Python å…§å»ºï¼Œä¸éœ€è¦å®‰è£

# ====================
# æ­¥é©Ÿ 2: æ‰¾åˆ°ç¾æœ‰çš„ PostgreSQL é…ç½®éƒ¨åˆ†ä¸¦è¨»é‡‹æ‰
# ====================
# åœ¨æ‚¨çš„æ–‡ä»¶ä¸­æ‰¾åˆ°é€™å€‹éƒ¨åˆ†ï¼ˆå¤§ç´„åœ¨ç¬¬ xxx è¡Œï¼‰ï¼š

"""
# ğŸ”§ è¨»é‡‹æ‰é€™å€‹èˆŠçš„é…ç½®
POSTGRES_CONFIG = {
    "host": os.getenv("POSTGRES_HOST", "localhost"),
    "port": int(os.getenv("POSTGRES_PORT", "5432")),
    "database": os.getenv("POSTGRES_DATABASE", "chatbot_system"),
    "user": os.getenv("POSTGRES_USER", "chatbot_user"),
    "password": os.getenv("POSTGRES_PASSWORD", "chatbot123"),
    "schema": os.getenv("POSTGRES_SCHEMA", "public"),
}
"""





# 3ï¸âƒ£ ä¿®æ”¹ OptimizedVectorSystem é¡çš„ load_document æ–¹æ³•
# ğŸ“ åœ¨ load_document æ–¹æ³•ä¸­æ·»åŠ  EPUB è™•ç†é‚è¼¯

def load_document(self, file_path: Path) -> List[Document]:
    """è¼‰å…¥ä¸¦æ™ºèƒ½è™•ç†æ–‡æª” - å®Œæ•´ç‰ˆåŒ…å« EPUB"""
    try:
        extension = file_path.suffix.lower()
        
        # æ ¹æ“šæª”æ¡ˆé¡å‹è¼‰å…¥
        if extension == '.pdf':
            loader = PyPDFLoader(str(file_path))
            documents = loader.load()
            full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            
        elif extension in ['.docx', '.doc'] and DOCX2TXT_AVAILABLE:
            if DOCX_METHOD == "docx2txt":
                import docx2txt
                full_text = docx2txt.process(str(file_path))
            else:
                loader = Docx2txtLoader(str(file_path))
                documents = loader.load()
                full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
                
        elif extension == '.epub' and EPUB_AVAILABLE:
            # ğŸ†• EPUB è™•ç†é‚è¼¯
            epub_processor = EpubProcessor()
            full_text = epub_processor.extract_epub_content(file_path)
            
        elif extension == '.csv':
            loader = CSVLoader(str(file_path), encoding='utf-8')
            documents = loader.load()
            full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            
        elif extension == '.json':
            loader = JSONLoader(str(file_path), jq_schema='.', text_content=False)
            documents = loader.load()
            full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            
        else:
            # æ–‡æœ¬æ–‡ä»¶è™•ç†
            encodings = ['utf-8', 'gbk', 'gb2312', 'big5']
            full_text = None
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        full_text = f.read()
                    break
                except UnicodeDecodeError:
                    continue
            
            if full_text is None:
                raise ValueError(f"ç„¡æ³•è§£ç¢¼æ–‡ä»¶: {file_path}")
        
        # æª¢æŸ¥å…§å®¹
        if not full_text or not full_text.strip():
            print(f"âš ï¸ æ–‡ä»¶å…§å®¹ç‚ºç©º: {file_path.name}")
            return []
        
        # ç”Ÿæˆæ–‡æª”ID
        doc_id = self._generate_doc_id(file_path)
        
        # ä½¿ç”¨å„ªåŒ–çš„æ–‡æœ¬åˆ†å‰²å™¨
        source_info = {
            'file_path': str(file_path),
            'file_type': extension,
            'file_size': file_path.stat().st_size if file_path.exists() else 0
        }
        
        documents = self.text_splitter.smart_split_documents(full_text, doc_id, source_info)
        
        # æ·»åŠ çµ±ä¸€å…ƒæ•¸æ“š
        for doc in documents:
            doc.metadata.update({
                'source': str(file_path),
                'filename': file_path.name,
                'extension': extension,
                'file_size': source_info['file_size'],
                'load_timestamp': time.time()
            })
        
        print(f"ğŸ“„ æ–‡æª”è¼‰å…¥å®Œæˆ: {file_path.name} ({len(documents)} å€‹åˆ†å¡Š)")
        
        return documents
        
    except Exception as e:
        logger.error(f"è¼‰å…¥æ–‡æª”å¤±æ•— {file_path}: {e}")
        print(f"âŒ æ–‡æª”è¼‰å…¥å¤±æ•—: {file_path.name} - {e}")
        return []



# 5ï¸âƒ£ æ·»åŠ  EPUB åŠŸèƒ½æ¸¬è©¦
def test_epub_functionality():
    """æ¸¬è©¦ EPUB åŠŸèƒ½"""
    print("ğŸ§ª === EPUB åŠŸèƒ½æ¸¬è©¦ ===")
    
    # æª¢æŸ¥ä¾è³´
    print(f"ğŸ“š EPUB åº«å¯ç”¨: {'âœ…' if EPUB_AVAILABLE else 'âŒ'}")
    
    if not EPUB_AVAILABLE:
        print("âŒ EPUB åŠŸèƒ½ä¸å¯ç”¨ï¼Œè«‹å®‰è£ä¾è³´:")
        print("   pip install ebooklib beautifulsoup4 lxml")
        return False
    
    # æŸ¥æ‰¾æ¸¬è©¦æ–‡ä»¶
    test_epub_files = []
    data_dir = Path("data")
    
    if data_dir.exists():
        test_epub_files = list(data_dir.rglob("*.epub"))
    
    if test_epub_files:
        print(f"ğŸ“– æ‰¾åˆ° {len(test_epub_files)} å€‹ EPUB æ–‡ä»¶:")
        for epub_file in test_epub_files[:3]:  # åªé¡¯ç¤ºå‰3å€‹
            print(f"   ğŸ“š {epub_file.name}")
        
        # æ¸¬è©¦è™•ç†ç¬¬ä¸€å€‹æ–‡ä»¶
        try:
            processor = EpubProcessor()
            content = processor.extract_epub_content(test_epub_files[0])
            print(f"âœ… EPUB è™•ç†æ¸¬è©¦æˆåŠŸ: {len(content):,} å­—ç¬¦")
            return True
        except Exception as e:
            print(f"âŒ EPUB è™•ç†æ¸¬è©¦å¤±æ•—: {e}")
            return False
    else:
        print("ğŸ“­ æœªæ‰¾åˆ° EPUB æ¸¬è©¦æ–‡ä»¶")
        print("   è«‹å°‡ .epub æ–‡ä»¶æ”¾å…¥ data/ ç›®éŒ„é€²è¡Œæ¸¬è©¦")
        return True  # æ²’æœ‰æ¸¬è©¦æ–‡ä»¶ä¸ç®—å¤±æ•—

# 6ï¸âƒ£ åœ¨ main å‡½æ•¸ä¸­æ·»åŠ  EPUB æ¸¬è©¦
def main():
    """ä¸»ç¨‹å¼ - åŒ…å« EPUB æ¸¬è©¦"""
    
    # EPUB åŠŸèƒ½æ¸¬è©¦
    epub_test_result = test_epub_functionality()
    
    # åˆå§‹åŒ–ç³»çµ±
    try:
        system = OptimizedVectorSystem()
        
        # ç³»çµ±è¨ºæ–·
        print("\n" + "="*60)
        system.diagnose_system()
        print("="*60 + "\n")
        
        # åŒæ­¥é›†åˆï¼ˆç¾åœ¨æ”¯æ´ EPUBï¼‰
        changes = system.sync_collections()
        
        # å…¶é¤˜ä»£ç¢¼ä¿æŒä¸è®Š...
        
    except Exception as e:
        print(f"âŒ ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
        return None

# =============================================================================
# ğŸš¨ é‡è¦ï¼šä¿®å¾©æ­¥é©Ÿ
# =============================================================================

print("âœ… EPUB è®€å–åŠŸèƒ½å·²å®Œæ•´ä¿®å¾©ï¼")
print("ğŸ“š æ”¯æ´åŠŸèƒ½:")
print("   â€¢ è‡ªå‹•æå–ç« ç¯€å…§å®¹")
print("   â€¢ æ™ºèƒ½ç« ç¯€æ¨™é¡Œè­˜åˆ¥") 
print("   â€¢ ä¸­æ–‡æ–‡æœ¬æ¨™æº–åŒ–")
print("   â€¢ æ›¸ç±å…ƒè³‡æ–™æå–")
print("   â€¢ HTML æ¨™ç±¤æ¸…ç†")
print("   â€¢ å¤šç« ç¯€æ™ºèƒ½åˆ†å‰²")


# ä¸­æ–‡åˆ†è©å’Œè½‰æ›
try:
    import jieba
    jieba.initialize()
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: jieba æœªå®‰è£ï¼Œä¸­æ–‡åˆ†è©åŠŸèƒ½å°‡è¢«ç¦ç”¨")

try:
    import opencc
    OPENCC_AVAILABLE = True
except ImportError:
    OPENCC_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: opencc æœªå®‰è£ï¼Œç¹ç°¡è½‰æ›åŠŸèƒ½å°‡è¢«ç¦ç”¨")

# æ–‡æª”è™•ç†æ”¯æ´
try:
    import docx2txt
    DOCX2TXT_AVAILABLE = True
    DOCX_METHOD = "docx2txt"
    print("âœ… DOCX æ”¯æ´å·²å•Ÿç”¨ (docx2txt)")
except ImportError:
    try:
        from docx import Document as DocxDocument
        DOCX2TXT_AVAILABLE = True
        DOCX_METHOD = "python-docx"
        print("âœ… DOCX æ”¯æ´å·²å•Ÿç”¨ (python-docx)")
    except ImportError:
        DOCX2TXT_AVAILABLE = False
        DOCX_METHOD = None
        print("âš ï¸ è­¦å‘Š: docx2txt æˆ– python-docx æœªå®‰è£")

try:
    import ebooklib
    from ebooklib import epub
    from bs4 import BeautifulSoup
    EPUB_AVAILABLE = True
    print("âœ… EPUB æ”¯æ´å·²å•Ÿç”¨")
except ImportError:
    EPUB_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: ebooklib æˆ– beautifulsoup4 æœªå®‰è£")

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=getattr(logging, SYSTEM_CONFIG["log_level"]))
logger = logging.getLogger(__name__)

@dataclass
class FileInfo:
    """æ–‡ä»¶è³‡è¨Š"""
    path: str
    size: int
    mtime: float
    hash: str
    encoding: str = "utf-8"
    file_type: str = ""

@dataclass
class TextAnalysis:
    """æ–‡æœ¬åˆ†æçµæœ"""
    length: int
    text_type: str
    language: str
    encoding: str
    structure_info: Dict
    quality_score: float
    processing_strategy: str

@dataclass
class ChunkInfo:
    """åˆ†å¡Šè³‡è¨Š"""
    chunk_id: str
    content: str
    metadata: Dict
    token_count: int
    quality_score: float
    relationships: List[str]  # èˆ‡å…¶ä»–åˆ†å¡Šçš„é—œä¿‚

@dataclass
class SearchResult:
    content: str
    score: float
    metadata: Dict
    collection: str
    chunk_info: Optional[ChunkInfo] = None

class AdvancedTokenEstimator:
    """ğŸ”§ é«˜ç´š Token ä¼°ç®—å™¨"""
    
    def __init__(self):
        # ä¸åŒèªè¨€å’Œå…§å®¹é¡å‹çš„ Token ä¿‚æ•¸
        self.token_ratios = {
            "chinese": 2.5,      # ä¸­æ–‡å­—ç¬¦/token
            "english": 4.0,      # è‹±æ–‡å­—ç¬¦/token
            "mixed": 3.0,        # ä¸­è‹±æ··åˆ
            "code": 3.5,         # ç¨‹å¼ç¢¼
            "punctuation": 1.0,  # æ¨™é»ç¬¦è™Ÿ
            "numbers": 2.0       # æ•¸å­—
        }
        self.safety_margin = TOKEN_LIMITS.get("token_safety_margin", 0.15)
    
    def analyze_text_composition(self, text: str) -> Dict[str, int]:
        """åˆ†ææ–‡æœ¬çµ„æˆ"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        numbers = len(re.findall(r'\d', text))
        punctuation = len(re.findall(r'[^\w\s\u4e00-\u9fff]', text))
        other_chars = len(text) - chinese_chars - english_chars - numbers - punctuation
        
        return {
            "chinese": chinese_chars,
            "english": english_chars,
            "numbers": numbers,
            "punctuation": punctuation,
            "other": other_chars,
            "total": len(text)
        }
    
    def estimate_tokens(self, text: str, content_type: str = "mixed") -> int:
        """ç²¾ç¢ºä¼°ç®— Token æ•¸é‡"""
        if not text:
            return 0
        
        composition = self.analyze_text_composition(text)
        
        # æ ¹æ“šæ–‡æœ¬çµ„æˆè¨ˆç®— token
        estimated_tokens = (
            composition["chinese"] / self.token_ratios["chinese"] +
            composition["english"] / self.token_ratios["english"] +
            composition["numbers"] / self.token_ratios["numbers"] +
            composition["punctuation"] / self.token_ratios["punctuation"] +
            composition["other"] / self.token_ratios["mixed"]
        )
        
        # æ ¹æ“šå…§å®¹é¡å‹èª¿æ•´
        if content_type == "code":
            estimated_tokens *= 1.2  # ç¨‹å¼ç¢¼é€šå¸¸ token å¯†åº¦æ›´é«˜
        elif content_type == "academic":
            estimated_tokens *= 1.1  # å­¸è¡“æ–‡æœ¬å°ˆæ¥­è©å½™å¤š
        
        # åŠ ä¸Šå®‰å…¨é‚Šéš›
        final_estimate = int(estimated_tokens * (1 + self.safety_margin))
        
        return max(final_estimate, 1)  # è‡³å°‘1å€‹token
    
    def estimate_embedding_cost(self, total_tokens: int, model: str = "text-embedding-3-small") -> float:
        """ä¼°ç®— Embedding æˆæœ¬"""
        cost_per_1k_tokens = {
            "text-embedding-3-small": 0.00002,
            "text-embedding-3-large": 0.00013,
            "text-embedding-ada-002": 0.0001
        }
        
        rate = cost_per_1k_tokens.get(model, 0.00002)
        return (total_tokens / 1000) * rate

class ChineseTextNormalizer:
    """å„ªåŒ–ç‰ˆä¸­æ–‡æ–‡æœ¬æ¨™æº–åŒ–è™•ç†å™¨"""
    
    def __init__(self):
        self.s2t_converter = None
        self.t2s_converter = None
        
        if OPENCC_AVAILABLE:
            try:
                self.s2t_converter = opencc.OpenCC('s2t')
                self.t2s_converter = opencc.OpenCC('t2s')
                print("ğŸ”¤ ç¹ç°¡è½‰æ›å™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.warning(f"OpenCC åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # ä¸­æ–‡æ–‡æœ¬æ­£è¦åŒ–è¦å‰‡
        self.normalization_rules = [
            (r'[ã€€\u3000]+', ' '),                    # çµ±ä¸€ç©ºæ ¼
            (r'\r\n|\r', '\n'),                       # çµ±ä¸€æ›è¡Œ
            (r'\n{3,}', '\n\n'),                      # é™åˆ¶é€£çºŒæ›è¡Œ
            (r'[\u201C\u201D\u2018\u2019\u201E\u201A\u2033\u2032]', '"'),  # çµ±ä¸€å¼•è™Ÿ
            (r'[â€”â€“âˆ’]', '-'),                          # çµ±ä¸€ç ´æŠ˜è™Ÿ
            (r'[â€¦â‹¯]', '...'),                         # çµ±ä¸€çœç•¥è™Ÿ
        ]
    
    def normalize_text(self, text: str) -> Tuple[str, Dict]:
        """æ¨™æº–åŒ–æ–‡æœ¬ä¸¦è¿”å›è™•ç†è³‡è¨Š"""
        if not text:
            return "", {}
        
        original_length = len(text)
        processed_text = text
        
        # æ‡‰ç”¨æ­£è¦åŒ–è¦å‰‡
        for pattern, replacement in self.normalization_rules:
            processed_text = re.sub(pattern, replacement, processed_text)
        
        # ç§»é™¤é¦–å°¾ç©ºç™½
        processed_text = processed_text.strip()
        
        # æª¢æ¸¬å’Œè½‰æ›ç¹ç°¡
        variant, confidence = self.detect_chinese_variant(processed_text)
        if variant == 'simplified' and self.s2t_converter:
            try:
                processed_text = self.s2t_converter.convert(processed_text)
                variant = 'traditional_converted'
            except Exception as e:
                logger.warning(f"ç¹ç°¡è½‰æ›å¤±æ•—: {e}")
        
        processing_info = {
            'original_length': original_length,
            'processed_length': len(processed_text),
            'variant': variant,
            'confidence': confidence,
            'normalized': original_length != len(processed_text)
        }
        
        return processed_text, processing_info
    
    def detect_chinese_variant(self, text: str) -> Tuple[str, float]:
        """æª¢æ¸¬ç¹é«”æˆ–ç°¡é«”ä¸­æ–‡"""
        if not text:
            return 'unknown', 0.0
        
        simplified_chars = set('å›½å‘ä¼šå­¦ä¹ è®ºé—®é¢˜ä¸šä¸“é•¿æ—¶é—´ç»æµ')
        traditional_chars = set('åœ‹ç™¼æœƒå­¸ç¿’è«–å•é¡Œæ¥­å°ˆé•·æ™‚é–“ç¶“æ¿Ÿ')
        
        simplified_count = sum(1 for char in text if char in simplified_chars)
        traditional_count = sum(1 for char in text if char in traditional_chars)
        total_chinese = len(re.findall(r'[\u4e00-\u9fff]', text))
        
        if total_chinese == 0:
            return 'unknown', 0.0
        
        simplified_ratio = simplified_count / total_chinese
        traditional_ratio = traditional_count / total_chinese
        
        if simplified_ratio > traditional_ratio:
            return 'simplified', simplified_ratio
        elif traditional_ratio > simplified_ratio:
            return 'traditional', traditional_ratio
        else:
            return 'mixed', max(simplified_ratio, traditional_ratio)
    
    def create_search_variants(self, query: str) -> List[str]:
        """å‰µå»ºç¹ç°¡æœç´¢è®Šé«”"""
        variants = [query]
        
        if not self.s2t_converter or not self.t2s_converter:
            return variants
        
        try:
            traditional = self.s2t_converter.convert(query)
            if traditional != query:
                variants.append(traditional)
            
            simplified = self.t2s_converter.convert(query)
            if simplified != query:
                variants.append(simplified)
        except Exception as e:
            logger.warning(f"å‰µå»ºæœç´¢è®Šé«”å¤±æ•—: {e}")
        
        return list(set(variants))

class EpubProcessor:
    """ğŸ“š EPUB æ–‡ä»¶è™•ç†å™¨"""
    
    def __init__(self):
        self.normalizer = ChineseTextNormalizer()
        
    def extract_epub_content(self, file_path: Path) -> str:
        """æå– EPUB å…§å®¹"""
        try:
            if not EPUB_AVAILABLE:
                raise ImportError("EPUB è™•ç†åº«æœªå®‰è£")
            
            print(f"ğŸ“š æ­£åœ¨è™•ç† EPUB æ–‡ä»¶: {file_path.name}")
            
            # è®€å– EPUB æ–‡ä»¶
            book = epub.read_epub(str(file_path))
            
            # æå–æ‰€æœ‰æ–‡æœ¬å…§å®¹
            content_parts = []
            chapter_count = 0
            
            # ç²å–æ›¸ç±ä¿¡æ¯
            title = book.get_metadata('DC', 'title')
            author = book.get_metadata('DC', 'creator')
            
            book_info = []
            if title:
                book_info.append(f"æ›¸å: {title[0][0] if title else 'æœªçŸ¥'}")
            if author:
                book_info.append(f"ä½œè€…: {author[0][0] if author else 'æœªçŸ¥'}")
            
            if book_info:
                content_parts.append("\n".join(book_info) + "\n\n")
            
            # æŒ‰é †åºè™•ç†ç« ç¯€
            spine_items = book.get_items_of_type(ebooklib.ITEM_DOCUMENT)
            
            for item in spine_items:
                try:
                    # è§£æ HTML å…§å®¹
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    
                    # ç§»é™¤è…³æœ¬å’Œæ¨£å¼
                    for script in soup(["script", "style"]):
                        script.decompose()
                    
                    # æå–æ–‡æœ¬
                    text = soup.get_text()
                    
                    if text and len(text.strip()) > 50:  # éæ¿¾å¤ªçŸ­çš„å…§å®¹
                        # æ¸…ç†æ–‡æœ¬
                        cleaned_text = self._clean_epub_text(text)
                        
                        if cleaned_text.strip():
                            chapter_count += 1
                            
                            # æ·»åŠ ç« ç¯€æ¨™è¨˜
                            chapter_title = self._extract_chapter_title(soup, cleaned_text)
                            if chapter_title:
                                content_parts.append(f"\n\n=== {chapter_title} ===\n")
                            else:
                                content_parts.append(f"\n\n=== ç¬¬ {chapter_count} ç«  ===\n")
                            
                            content_parts.append(cleaned_text)
                            
                except Exception as e:
                    print(f"   âš ï¸ ç« ç¯€è™•ç†å¤±æ•—: {e}")
                    continue
            
            if not content_parts:
                raise ValueError("EPUB æ–‡ä»¶ä¸­æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆå…§å®¹")
            
            full_content = "".join(content_parts)
            
            print(f"   âœ… EPUB è™•ç†å®Œæˆ: {chapter_count} å€‹ç« ç¯€, {len(full_content):,} å­—ç¬¦")
            
            return full_content
            
        except Exception as e:
            print(f"   âŒ EPUB è™•ç†å¤±æ•—: {e}")
            raise
    
    def _clean_epub_text(self, text: str) -> str:
        """æ¸…ç† EPUB æ–‡æœ¬"""
        if not text:
            return ""
        
        # æ¨™æº–åŒ–æ–‡æœ¬
        normalized_text, _ = self.normalizer.normalize_text(text)
        
        # EPUB ç‰¹å®šæ¸…ç†
        epub_cleaning_rules = [
            (r'\n{4,}', '\n\n'),                    # é™åˆ¶é€£çºŒæ›è¡Œ
            (r'[ \t]{3,}', ' '),                    # é™åˆ¶é€£çºŒç©ºæ ¼
            (r'^[ \t]+', '', re.MULTILINE),          # ç§»é™¤è¡Œé¦–ç©ºç™½
            (r'[ \t]+$', '', re.MULTILINE),          # ç§»é™¤è¡Œå°¾ç©ºç™½
            (r'\n[ \t]*\n', '\n\n'),                # æ¸…ç†ç©ºè¡Œ
            (r'[\x00-\x08\x0B\x0C\x0E-\x1F]', ''),  # ç§»é™¤æ§åˆ¶å­—ç¬¦
        ]
        
        cleaned_text = normalized_text
        for pattern, replacement, *flags in epub_cleaning_rules:
            flag = flags[0] if flags else 0
            cleaned_text = re.sub(pattern, replacement, cleaned_text, flags=flag)
        
        return cleaned_text.strip()
    
    def _extract_chapter_title(self, soup, text: str) -> Optional[str]:
        """å˜—è©¦æå–ç« ç¯€æ¨™é¡Œ"""
        try:
            # å¾ HTML æ¨™ç±¤ä¸­æŸ¥æ‰¾æ¨™é¡Œ
            for tag in ['h1', 'h2', 'h3', 'title']:
                title_element = soup.find(tag)
                if title_element and title_element.get_text().strip():
                    title = title_element.get_text().strip()
                    if 5 <= len(title) <= 100:  # åˆç†çš„æ¨™é¡Œé•·åº¦
                        return title
            
            # å¾æ–‡æœ¬é–‹é ­æŸ¥æ‰¾æ¨™é¡Œ
            lines = text.split('\n')[:5]  # æª¢æŸ¥å‰5è¡Œ
            for line in lines:
                line = line.strip()
                if line and 5 <= len(line) <= 100:
                    # æª¢æŸ¥æ˜¯å¦åŒ…å«ç« ç¯€é—œéµè©
                    chapter_keywords = ['ç« ', 'Chapter', 'ç¬¬', 'å·', 'Part']
                    if any(keyword in line for keyword in chapter_keywords):
                        return line
                    # æˆ–è€…æ˜¯çŸ­è¡Œä¸”çœ‹èµ·ä¾†åƒæ¨™é¡Œ
                    elif len(line) <= 50 and not line.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
                        return line
            
            return None
            
        except Exception:
            return None


class SmartTextAnalyzer:
    """ğŸ§  æ™ºèƒ½æ–‡æœ¬åˆ†æå™¨"""
    
    def __init__(self):
        self.normalizer = ChineseTextNormalizer()
        self.token_estimator = AdvancedTokenEstimator()
        
    def analyze_text(self, text: str, source_info: Dict = None) -> TextAnalysis:
        """ç¶œåˆåˆ†ææ–‡æœ¬"""
        if not text:
            return TextAnalysis(
                length=0, text_type="empty", language="unknown",
                encoding="utf-8", structure_info={}, quality_score=0.0,
                processing_strategy="skip"
            )
        
        # æ¨™æº–åŒ–æ–‡æœ¬
        normalized_text, norm_info = self.normalizer.normalize_text(text)
        length = len(normalized_text)
        
        # åˆ†é¡æ–‡æœ¬é•·åº¦
        text_type = self._classify_text_length(length)
        
        # åˆ†ææ–‡æœ¬çµæ§‹
        structure_info = self._analyze_structure(normalized_text)
        
        # æª¢æ¸¬èªè¨€
        language = self._detect_language(normalized_text)
        
        # è©•ä¼°è³ªé‡
        quality_score = self._evaluate_quality(normalized_text, structure_info)
        
        # æ±ºå®šè™•ç†ç­–ç•¥
        processing_strategy = self._determine_strategy(text_type, structure_info, quality_score)
        
        return TextAnalysis(
            length=length,
            text_type=text_type,
            language=language,
            encoding=norm_info.get('encoding', 'utf-8'),
            structure_info=structure_info,
            quality_score=quality_score,
            processing_strategy=processing_strategy
        )
    
    def _classify_text_length(self, length: int) -> str:
        """åˆ†é¡æ–‡æœ¬é•·åº¦"""
        config = SMART_TEXT_CONFIG
        
        if length < config["micro_text_threshold"]:
            return "micro_text"
        elif length < config["short_text_threshold"]:
            return "short_text"
        elif length < config["medium_text_threshold"]:
            return "medium_text"
        elif length < config["long_text_threshold"]:
            return "long_text"
        elif length < config["ultra_long_threshold"]:
            return "ultra_long"
        else:
            return "mega_text"  # è¶…å¤§æ–‡æœ¬
    
    def _analyze_structure(self, text: str) -> Dict:
        """åˆ†ææ–‡æœ¬çµæ§‹"""
        structure_info = {
            'paragraphs': len(text.split('\n\n')),
            'lines': len(text.split('\n')),
            'sentences': len(re.findall(r'[ã€‚ï¼ï¼Ÿ.!?]+', text)),
            'has_chapters': False,
            'has_sections': False,
            'has_lists': False,
            'has_tables': False,
            'chapter_count': 0,
            'section_count': 0
        }
        
        # æª¢æ¸¬ç« ç¯€çµæ§‹
        chapter_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ',
            r'Chapter\s+\d+',
            r'\d+\.\s*[^\n]{1,50}\n'
        ]
        
        for pattern in chapter_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                structure_info['has_chapters'] = True
                structure_info['chapter_count'] = len(matches)
                break
        
        # æª¢æ¸¬å°ç¯€çµæ§‹
        section_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç¯€',
            r'\d+\.\d+',
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€'
        ]
        
        for pattern in section_patterns:
            matches = re.findall(pattern, text)
            if matches:
                structure_info['has_sections'] = True
                structure_info['section_count'] = len(matches)
                break
        
        # æª¢æ¸¬åˆ—è¡¨
        if re.search(r'^\s*[â€¢\-\*]\s+', text, re.MULTILINE) or \
           re.search(r'^\s*\d+\.\s+', text, re.MULTILINE):
            structure_info['has_lists'] = True
        
        # æª¢æ¸¬è¡¨æ ¼
        if '|' in text and text.count('|') > 4:
            structure_info['has_tables'] = True
        
        return structure_info
    
    def _detect_language(self, text: str) -> str:
        """æª¢æ¸¬æ–‡æœ¬ä¸»è¦èªè¨€"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_chars = len(text)
        
        if total_chars == 0:
            return "unknown"
        
        chinese_ratio = chinese_chars / total_chars
        english_ratio = english_chars / total_chars
        
        if chinese_ratio > 0.3:
            if english_ratio > 0.2:
                return "mixed_zh_en"
            else:
                return "chinese"
        elif english_ratio > 0.5:
            return "english"
        else:
            return "mixed"
    
    def _evaluate_quality(self, text: str, structure_info: Dict) -> float:
        """è©•ä¼°æ–‡æœ¬è³ªé‡ï¼ˆ0-1åˆ†æ•¸ï¼‰"""
        score = 0.5  # åŸºç¤åˆ†æ•¸
        
        # é•·åº¦åˆç†æ€§ï¼ˆå¤ªçŸ­æˆ–å¤ªé•·éƒ½æ‰£åˆ†ï¼‰
        length = len(text)
        if 50 <= length <= 10000:
            score += 0.2
        elif length < 20:
            score -= 0.3
        
        # çµæ§‹å®Œæ•´æ€§
        if structure_info['paragraphs'] > 1:
            score += 0.1
        if structure_info['sentences'] > 2:
            score += 0.1
        
        # å…§å®¹è±å¯Œåº¦
        unique_chars = len(set(text))
        if unique_chars > 20:
            score += 0.1
        
        # é¿å…é‡è¤‡å†…å®¹
        lines = text.split('\n')
        unique_lines = len(set(lines))
        if len(lines) > 0:
            uniqueness_ratio = unique_lines / len(lines)
            score += uniqueness_ratio * 0.1
        
        return min(max(score, 0.0), 1.0)
    
    def _determine_strategy(self, text_type: str, structure_info: Dict, quality_score: float) -> str:
        """æ±ºå®šè™•ç†ç­–ç•¥"""
        if quality_score < 0.3:
            return "low_quality_skip"
        
        if text_type in ["micro_text", "short_text"]:
            return "whole_document"
        elif text_type == "medium_text":
            if structure_info['has_chapters'] or structure_info['paragraphs'] > 5:
                return "paragraph_aware"
            else:
                return "simple_split"
        elif text_type == "long_text":
            if structure_info['has_chapters']:
                return "chapter_aware"
            elif structure_info['has_sections']:
                return "section_aware"
            else:
                return "fine_split"
        elif text_type in ["ultra_long", "mega_text"]:
            return "hierarchical_split"
        else:
            # å›é€€ç­–ç•¥
            return "hierarchical_split"

class OptimizedTextSplitter:
    """ğŸ”§ å„ªåŒ–ç‰ˆæ–‡æœ¬åˆ†å‰²å™¨"""
    
    def __init__(self):
        self.analyzer = SmartTextAnalyzer()
        self.token_estimator = AdvancedTokenEstimator()
        self.config = SMART_TEXT_CONFIG
        
        # åˆ†å‰²å™¨å¯¦ä¾‹æ± 
        self._splitter_cache = {}
        
        print("ğŸ”§ å„ªåŒ–ç‰ˆæ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“ æ”¯æŒ {len(self.config)} ç¨®æ–‡æœ¬é¡å‹")
        print(f"   ğŸ§  æ™ºèƒ½ç­–ç•¥é¸æ“‡")
        print(f"   âš¡ æ€§èƒ½å„ªåŒ–")
    
    def get_splitter(self, chunk_size: int, chunk_overlap: int) -> RecursiveCharacterTextSplitter:
        """ç²å–åˆ†å‰²å™¨å¯¦ä¾‹ï¼ˆå¸¶å¿«å–ï¼‰"""
        cache_key = f"{chunk_size}_{chunk_overlap}"
        
        if cache_key not in self._splitter_cache:
            self._splitter_cache[cache_key] = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=[
                    "\n\nç¬¬", "\nç¬¬",      # ç« ç¯€æ¨™é¡Œ
                    "\n\n", "\n",          # æ®µè½
                    "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›",  # å¥å­çµæŸ
                    "ï¼Œ", "ã€",            # çŸ­èªåˆ†éš”
                    " ", ""                # æœ€å¾Œresort
                ],
                keep_separator=True,
                length_function=len
            )
        
        return self._splitter_cache[cache_key]
    
    def smart_split_documents(self, text: str, doc_id: str, source_info: Dict = None) -> List[Document]:
        """ğŸ§  æ™ºèƒ½æ–‡æª”åˆ†å‰²ä¸»å…¥å£"""
        if not text or not text.strip():
            return []
        
        # åˆ†ææ–‡æœ¬
        analysis = self.analyzer.analyze_text(text, source_info)
        
        print(f"ğŸ“Š æ–‡æœ¬åˆ†æ: {analysis.text_type} ({analysis.length:,} å­—ç¬¦)")
        print(f"   ğŸ¯ è™•ç†ç­–ç•¥: {analysis.processing_strategy}")
        print(f"   ğŸ“ˆ è³ªé‡åˆ†æ•¸: {analysis.quality_score:.2f}")
        print(f"   ğŸ—£ï¸ ä¸»è¦èªè¨€: {analysis.language}")
        
        # æ ¹æ“šç­–ç•¥é¸æ“‡è™•ç†æ–¹æ³•
        documents = []
        if analysis.processing_strategy == "low_quality_skip":
            print("   âš ï¸ æ–‡æœ¬è³ªé‡éä½ï¼Œè·³éè™•ç†")
            return []
        elif analysis.processing_strategy == "whole_document":
            documents = self._process_whole_document(text, doc_id, analysis)
        elif analysis.processing_strategy == "paragraph_aware":
            documents = self._process_paragraph_aware(text, doc_id, analysis)
        elif analysis.processing_strategy == "simple_split":
            documents = self._process_simple_split(text, doc_id, analysis)
        elif analysis.processing_strategy == "chapter_aware":
            documents = self._process_chapter_aware(text, doc_id, analysis)
        elif analysis.processing_strategy == "section_aware":
            documents = self._process_section_aware(text, doc_id, analysis)
        elif analysis.processing_strategy == "fine_split":
            documents = self._process_fine_split(text, doc_id, analysis)
        elif analysis.processing_strategy == "hierarchical_split":
            documents = self._process_hierarchical_split(text, doc_id, analysis)
        else:
            # å›é€€åˆ°ç°¡å–®åˆ†å‰²
            documents = self._process_simple_split(text, doc_id, analysis)

        # æœ€çµ‚é©—è­‰å’Œéæ¿¾ï¼Œç¢ºä¿æ²’æœ‰ç©ºå…§å®¹çš„æ–‡æª”
        final_documents = [doc for doc in documents if doc.page_content and doc.page_content.strip()]
        if len(final_documents) != len(documents):
            logger.warning(f"éæ¿¾æ‰ {len(documents) - len(final_documents)} å€‹ç©ºå…§å®¹çš„å€å¡Š")
        
        return final_documents
    
    def _process_whole_document(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """è™•ç†æ•´å€‹æ–‡æª”ï¼ˆä¸åˆ†å‰²ï¼‰"""
        config = self.config[analysis.text_type]
        
        if len(text.strip()) < config.get("min_length", 20):
            print(f"   âš ï¸ æ–‡æª”éçŸ­ï¼Œè·³éè™•ç†")
            return []
        
        print(f"   ğŸ“ {config['description']}")
        
        # è¨ˆç®— token æ•¸
        token_count = self.token_estimator.estimate_tokens(text)
        
        metadata = {
            'doc_id': doc_id,
            'chunk_id': f"{doc_id}_whole",
            'chunk_index': 0,
            'total_chunks': 1,
            'text_type': analysis.text_type,
            'processing_strategy': analysis.processing_strategy,
            'is_complete_document': True,
            'token_count': token_count,
            'quality_score': analysis.quality_score,
            'language': analysis.language,
            'structure_info': analysis.structure_info
        }
        
        return [Document(page_content=text, metadata=metadata)]
    
    def _process_paragraph_aware(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """æ®µè½æ„ŸçŸ¥è™•ç†"""
        config = self.config[analysis.text_type]
        chunk_size = config["chunk_size"]
        chunk_overlap = config["chunk_overlap"]
        
        print(f"   ğŸ“‘ {config['description']} (ç›®æ¨™å¤§å°: {chunk_size})")
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        if not paragraphs:
            return self._process_simple_split(text, doc_id, analysis)
        
        documents = []
        current_chunk = ""
        chunk_index = 0
        
        for para_idx, paragraph in enumerate(paragraphs):
            # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŠ å…¥ç•¶å‰åˆ†å¡Š
            potential_chunk = current_chunk + ("\n\n" if current_chunk else "") + paragraph
            
            if len(potential_chunk) <= chunk_size or not current_chunk:
                current_chunk = potential_chunk
            else:
                # ç•¶å‰åˆ†å¡Šå·²æ»¿ï¼Œå‰µå»ºæ–‡æª”
                if current_chunk:
                    doc = self._create_chunk_document(
                        current_chunk, doc_id, chunk_index, analysis, "paragraph_aware"
                    )
                    documents.append(doc)
                    chunk_index += 1
                
                current_chunk = paragraph
        
        # è™•ç†æœ€å¾Œä¸€å€‹åˆ†å¡Š
        if current_chunk:
            doc = self._create_chunk_document(
                current_chunk, doc_id, chunk_index, analysis, "paragraph_aware"
            )
            documents.append(doc)
        
        # è™•ç†é‡ç–Š
        if chunk_overlap > 0 and len(documents) > 1:
            documents = self._add_overlap_to_documents(documents, chunk_overlap)
        
        return documents
    
    def _process_simple_split(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """ç°¡å–®åˆ†å‰²è™•ç†"""
        config = self.config[analysis.text_type]
        chunk_size = config["chunk_size"]
        chunk_overlap = config["chunk_overlap"]
        
        print(f"   âœ‚ï¸ {config['description']} (å¤§å°: {chunk_size}, é‡ç–Š: {chunk_overlap})")
        
        splitter = self.get_splitter(chunk_size, chunk_overlap)
        chunks = splitter.split_text(text)
        
        documents = []
        for i, chunk in enumerate(chunks):
            if len(chunk.strip()) >= config.get("min_length", 20):
                doc = self._create_chunk_document(
                    chunk, doc_id, i, analysis, "simple_split"
                )
                documents.append(doc)
        
        return documents
    
    def _process_chapter_aware(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """ç« ç¯€æ„ŸçŸ¥è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   ğŸ“š {config['description']} - æª¢æ¸¬åˆ° {analysis.structure_info.get('chapter_count', 0)} å€‹ç« ç¯€")
        
        # æŒ‰ç« ç¯€åˆ†å‰²
        chapters = self._split_by_chapters(text)
        
        if len(chapters) <= 1:
            # æ²’æœ‰æª¢æ¸¬åˆ°ç« ç¯€ï¼Œå›é€€åˆ°æ®µè½æ„ŸçŸ¥
            return self._process_paragraph_aware(text, doc_id, analysis)
        
        all_documents = []
        
        for chapter_idx, chapter_text in enumerate(chapters):
            chapter_doc_id = f"{doc_id}_ch{chapter_idx+1:02d}"
            
            # æ¯å€‹ç« ç¯€å…§éƒ¨ä½¿ç”¨æ®µè½æ„ŸçŸ¥åˆ†å‰²
            chapter_analysis = analysis  # ä½¿ç”¨ç›¸åŒçš„åˆ†æçµæœ
            chapter_docs = self._process_paragraph_aware(chapter_text, chapter_doc_id, chapter_analysis)
            
            # æ·»åŠ ç« ç¯€ä¿¡æ¯åˆ°å…ƒæ•¸æ“š
            for doc in chapter_docs:
                doc.metadata.update({
                    'chapter_index': chapter_idx,
                    'chapter_total': len(chapters),
                    'parent_doc_id': doc_id,
                    'split_method': 'chapter_aware'
                })
            
            all_documents.extend(chapter_docs)
        
        return all_documents
    
    def _process_section_aware(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """å°ç¯€æ„ŸçŸ¥è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   ğŸ“‹ {config['description']} - æª¢æ¸¬åˆ° {analysis.structure_info.get('section_count', 0)} å€‹å°ç¯€")
        
        # æŒ‰å°ç¯€åˆ†å‰²
        sections = self._split_by_sections(text)
        
        if len(sections) <= 1:
            return self._process_simple_split(text, doc_id, analysis)
        
        all_documents = []
        
        for section_idx, section_text in enumerate(sections):
            section_doc_id = f"{doc_id}_sec{section_idx+1:02d}"
            
            # å°ç¯€å…§éƒ¨ç°¡å–®åˆ†å‰²
            section_docs = self._process_simple_split(section_text, section_doc_id, analysis)
            
            # æ·»åŠ å°ç¯€ä¿¡æ¯
            for doc in section_docs:
                doc.metadata.update({
                    'section_index': section_idx,
                    'section_total': len(sections),
                    'parent_doc_id': doc_id,
                    'split_method': 'section_aware'
                })
            
            all_documents.extend(section_docs)
        
        return all_documents
    
    def _process_fine_split(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """ç²¾ç´°åˆ†å‰²è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   ğŸ”¬ {config['description']}")
        
        return self._process_simple_split(text, doc_id, analysis)
    
    def _process_hierarchical_split(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """éšå±¤å¼åˆ†å‰²è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   ğŸ—ï¸ {config['description']}")
        
        # ç¬¬ä¸€å±¤ï¼šç« ç¯€åˆ†å‰²
        chapters = self._split_by_chapters(text)
        
        if len(chapters) > 1:
            return self._process_chapter_aware(text, doc_id, analysis)
        
        # ç¬¬äºŒå±¤ï¼šæ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        if len(paragraphs) > 10:  # æ®µè½è¼ƒå¤šï¼Œç”¨æ®µè½æ„ŸçŸ¥
            return self._process_paragraph_aware(text, doc_id, analysis)
        
        # ç¬¬ä¸‰å±¤ï¼šç²¾ç´°åˆ†å‰²
        return self._process_fine_split(text, doc_id, analysis)
    
    def _split_by_chapters(self, text: str) -> List[str]:
        """æŒ‰ç« ç¯€åˆ†å‰²æ–‡æœ¬"""
        chapter_patterns = [
            r'\n(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« [^\n]*)',
            r'\n(Chapter\s+\d+[^\n]*)',
            r'\n(\d+\.\s*[^\n]{5,50})\n',
            r'\n([A-Z][^\n]{10,50})\n(?=[A-Z]|$)'
        ]
        
        best_split = None
        best_count = 0
        
        for pattern in chapter_patterns:
            matches = list(re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE))
            
            if len(matches) >= 2:
                splits = []
                start = 0
                
                for match in matches:
                    if start < match.start():
                        splits.append(text[start:match.start()].strip())
                    start = match.start()
                
                splits.append(text[start:].strip())
                splits = [s for s in splits if s and len(s) > 50]
                
                if len(splits) > best_count:
                    best_split = splits
                    best_count = len(splits)
        
        return best_split if best_split else [text]
    
    def _split_by_sections(self, text: str) -> List[str]:
        """æŒ‰å°ç¯€åˆ†å‰²æ–‡æœ¬"""
        section_patterns = [
            r'\n(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç¯€[^\n]*)',
            r'\n(\d+\.\d+[^\n]*)',
            r'\n([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€[^\n]*)',
            r'\n([A-Z]\.[^\n]*)'
        ]
        
        for pattern in section_patterns:
            matches = list(re.finditer(pattern, text, re.MULTILINE))
            
            if len(matches) >= 2:
                splits = []
                start = 0
                
                for match in matches:
                    if start < match.start():
                        splits.append(text[start:match.start()].strip())
                    start = match.start()
                
                splits.append(text[start:].strip())
                return [s for s in splits if s and len(s) > 30]
        
        return [text]
    
    def _add_overlap_to_documents(self, documents: List[Document], overlap_size: int) -> List[Document]:
        """ç‚ºæ–‡æª”æ·»åŠ é‡ç–Šéƒ¨åˆ†"""
        if len(documents) <= 1:
            return documents
        
        for i in range(1, len(documents)):
            # å¾å‰ä¸€å€‹æ–‡æª”æœ«å°¾å–é‡ç–Šå…§å®¹
            prev_content = documents[i-1].page_content
            current_content = documents[i].page_content
            
            if len(prev_content) > overlap_size:
                overlap_text = prev_content[-overlap_size:]
                # å°‹æ‰¾å®Œæ•´çš„å¥å­é‚Šç•Œ
                sentence_end = overlap_text.rfind('ã€‚')
                if sentence_end == -1:
                    sentence_end = overlap_text.rfind('.')
                
                if sentence_end > overlap_size // 2:
                    overlap_text = overlap_text[sentence_end+1:]
                
                documents[i].page_content = overlap_text + "\n" + current_content
                documents[i].metadata['has_overlap'] = True
                documents[i].metadata['overlap_length'] = len(overlap_text)
        
        return documents
    
    
    def _create_chunk_document(self, content: str, doc_id: str, chunk_index: int, 
                          analysis: TextAnalysis, split_method: str) -> Document:
        """å‰µå»ºåˆ†å¡Šæ–‡æª” - çµ±ä¸€å…ƒæ•¸æ“šæ ¼å¼"""
        token_count = self.token_estimator.estimate_tokens(content)
        
        # æƒæURL
        url_regex = r'https?://[\w\-./?#&%=]+'
        found_urls = re.findall(url_regex, content)

        # åŸºæœ¬å…ƒæ•¸æ“šï¼ˆç¢ºä¿éƒ½æ˜¯ç°¡å–®é¡å‹ï¼‰
        metadata = {
            'doc_id': str(doc_id),
            'chunk_id': f"{doc_id}_{chunk_index+1:03d}",
            'chunk_index': int(chunk_index),
            'text_type': str(analysis.text_type),
            'processing_strategy': str(analysis.processing_strategy),
            'split_method': str(split_method),
            'chunk_length': int(len(content)),
            'token_count': int(token_count),
            'quality_score': float(analysis.quality_score),
            'language': str(analysis.language),
            'has_overlap': False
        }

        # ğŸ¯ æ­£ç¢ºæ–¹æ¡ˆï¼šURLåˆ—è¡¨ â†’ åˆ†éš”ç¬¦å­—ä¸² (é€™æ˜¯å”¯ä¸€å¯è¡Œçš„æ–¹æ¡ˆ)
        if found_urls:
            metadata['contained_urls'] = '|'.join(found_urls)  # âœ… å­—ç¬¦ä¸²é¡å‹
            metadata['url_count'] = len(found_urls)           # âœ… æ•´æ•¸é¡å‹
            metadata['has_urls'] = True                       # âœ… å¸ƒæ—é¡å‹
        else:
            metadata['contained_urls'] = ''                   # âœ… ç©ºå­—ç¬¦ä¸²
            metadata['url_count'] = 0                        # âœ… æ•´æ•¸ 0
            metadata['has_urls'] = False                     # âœ… å¸ƒæ— False

        # ğŸ¯ æ­£ç¢ºæ–¹æ¡ˆï¼šè¤‡é›œçµæ§‹ â†’ JSONå­—ä¸²
        if analysis.structure_info:
            metadata['structure_info'] = json.dumps(analysis.structure_info, ensure_ascii=False)  # âœ… JSONå­—ç¬¦ä¸²
            # åŒæ™‚ä¿ç•™é—œéµä¿¡æ¯ä½œç‚ºç°¡å–®å­—æ®µï¼ˆä¾¿æ–¼æŸ¥è©¢ï¼‰
            metadata['has_chapters'] = bool(analysis.structure_info.get('has_chapters', False))    # âœ… å¸ƒæ—
            metadata['has_sections'] = bool(analysis.structure_info.get('has_sections', False))    # âœ… å¸ƒæ—  
            metadata['paragraph_count'] = int(analysis.structure_info.get('paragraphs', 0))       # âœ… æ•´æ•¸
            metadata['sentence_count'] = int(analysis.structure_info.get('sentences', 0))         # âœ… æ•´æ•¸
        else:
            metadata['structure_info'] = '{}'        # âœ… ç©ºJSONå­—ç¬¦ä¸²
            metadata['has_chapters'] = False         # âœ… å¸ƒæ—
            metadata['has_sections'] = False         # âœ… å¸ƒæ—
            metadata['paragraph_count'] = 0          # âœ… æ•´æ•¸
            metadata['sentence_count'] = 0           # âœ… æ•´æ•¸
        
        return Document(page_content=content, metadata=metadata)

class AdaptiveBatchProcessor:
    """ğŸ”§ è‡ªé©æ‡‰æ‰¹æ¬¡è™•ç†å™¨"""
    
    def __init__(self):
        self.token_estimator = AdvancedTokenEstimator()
        self.max_tokens_per_batch = TOKEN_LIMITS["max_tokens_per_request"]
        self.max_batch_size = TOKEN_LIMITS["max_batch_size"]
        self.adaptive_batching = TOKEN_LIMITS.get("adaptive_batching", True)
        
        # æ€§èƒ½çµ±è¨ˆ
        self.batch_stats = {
            'total_batches': 0,
            'total_documents': 0,
            'total_tokens': 0,
            'avg_batch_time': 0,
            'success_rate': 0,
            'adaptive_adjustments': 0
        }
        
        # è‡ªé©æ‡‰åƒæ•¸
        self.current_batch_size = self.max_batch_size
        self.success_streak = 0
        self.failure_streak = 0
    
    def create_smart_batches(self, documents: List[Document]) -> List[Tuple[List[Document], Dict]]:
        """å‰µå»ºæ™ºèƒ½æ‰¹æ¬¡"""
        if not documents:
            return []
        
        print(f"ğŸ”§ å‰µå»ºæ™ºèƒ½æ‰¹æ¬¡...")
        print(f"   ğŸ“„ ç¸½æ–‡æª”æ•¸: {len(documents)}")
        print(f"   ğŸ“ Token é™åˆ¶: {self.max_tokens_per_batch:,}")
        print(f"   ğŸ“¦ æœ€å¤§æ‰¹æ¬¡å¤§å°: {self.max_batch_size}")
        print(f"   ğŸ§  è‡ªé©æ‡‰æ‰¹æ¬¡: {'âœ…' if self.adaptive_batching else 'âŒ'}")
        
        batches = []
        current_batch = []
        current_tokens = 0
        batch_info = {'documents': 0, 'tokens': 0, 'types': defaultdict(int)}
        
        # æŒ‰ token æ•¸æ’åºæ–‡æª”ï¼ˆå¤§çš„åœ¨å‰é¢ï¼Œæ›´å®¹æ˜“è™•ç†ï¼‰
        sorted_docs = sorted(
            documents, 
            key=lambda doc: doc.metadata.get('token_count', 
                self.token_estimator.estimate_tokens(doc.page_content)),
            reverse=True
        )
        
        for doc_idx, doc in enumerate(sorted_docs):
            doc_tokens = doc.metadata.get('token_count') or \
                        self.token_estimator.estimate_tokens(doc.page_content)
            
            # æª¢æŸ¥å–®å€‹æ–‡æª”æ˜¯å¦éå¤§
            if doc_tokens > self.max_tokens_per_batch:
                print(f"   âš ï¸ æ–‡æª” {doc_idx+1} éå¤§ ({doc_tokens:,} tokens)ï¼Œéœ€è¦åˆ†å‰²")
                
                # å®Œæˆç•¶å‰æ‰¹æ¬¡
                if current_batch:
                    batches.append((current_batch, dict(batch_info)))
                    current_batch = []
                    current_tokens = 0
                    batch_info = {'documents': 0, 'tokens': 0, 'types': defaultdict(int)}
                
                # åˆ†å‰²å¤§æ–‡æª”
                split_docs = self._split_large_document(doc)
                for split_doc in split_docs:
                    split_tokens = self.token_estimator.estimate_tokens(split_doc.page_content)
                    split_doc.metadata['token_count'] = split_tokens
                    batches.append(([split_doc], {
                        'documents': 1, 
                        'tokens': split_tokens, 
                        'types': {split_doc.metadata.get('text_type', 'unknown'): 1},
                        'is_split': True
                    }))
                continue
            
            # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŠ å…¥ç•¶å‰æ‰¹æ¬¡
            would_exceed_tokens = current_tokens + doc_tokens > self.max_tokens_per_batch
            would_exceed_size = len(current_batch) >= self._get_adaptive_batch_size()
            
            if would_exceed_tokens or would_exceed_size:
                # å®Œæˆç•¶å‰æ‰¹æ¬¡
                if current_batch:
                    batches.append((current_batch, dict(batch_info)))
                
                # é–‹å§‹æ–°æ‰¹æ¬¡
                current_batch = [doc]
                current_tokens = doc_tokens
                batch_info = {
                    'documents': 1, 
                    'tokens': doc_tokens, 
                    'types': defaultdict(int)
                }
                batch_info['types'][doc.metadata.get('text_type', 'unknown')] += 1
            else:
                # åŠ å…¥ç•¶å‰æ‰¹æ¬¡
                current_batch.append(doc)
                current_tokens += doc_tokens
                batch_info['documents'] += 1
                batch_info['tokens'] += doc_tokens
                batch_info['types'][doc.metadata.get('text_type', 'unknown')] += 1
        
        # è™•ç†æœ€å¾Œä¸€å€‹æ‰¹æ¬¡
        if current_batch:
            batches.append((current_batch, dict(batch_info)))
        
        # çµ±è¨ˆä¿¡æ¯
        total_docs = sum(info['documents'] for _, info in batches)
        total_tokens = sum(info['tokens'] for _, info in batches)
        avg_tokens_per_batch = total_tokens / len(batches) if batches else 0
        
        print(f"âœ… æ™ºèƒ½æ‰¹æ¬¡å‰µå»ºå®Œæˆ:")
        print(f"   ğŸ“¦ ç¸½æ‰¹æ¬¡æ•¸: {len(batches)}")
        print(f"   ğŸ“„ ç¸½æ–‡æª”æ•¸: {total_docs}")
        print(f"   ğŸ“ ç¸½ tokens: {total_tokens:,}")
        print(f"   ğŸ“Š å¹³å‡ tokens/æ‰¹æ¬¡: {avg_tokens_per_batch:.0f}")
        print(f"   ğŸ’° ä¼°ç®—æˆæœ¬: ${self.token_estimator.estimate_embedding_cost(total_tokens):.4f}")
        
        # æ›´æ–°çµ±è¨ˆ
        self.batch_stats['total_batches'] += len(batches)
        self.batch_stats['total_documents'] += total_docs
        self.batch_stats['total_tokens'] += total_tokens
        
        return batches
    
    def _get_adaptive_batch_size(self) -> int:
        """ç²å–è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°"""
        if not self.adaptive_batching:
            return self.max_batch_size
        
        # æ ¹æ“šæˆåŠŸç‡èª¿æ•´æ‰¹æ¬¡å¤§å°
        if self.success_streak >= 3:
            # é€£çºŒæˆåŠŸï¼Œå¯ä»¥å¢åŠ æ‰¹æ¬¡å¤§å°
            self.current_batch_size = min(self.max_batch_size, self.current_batch_size + 1)
            self.batch_stats['adaptive_adjustments'] += 1
        elif self.failure_streak >= 2:
            # é€£çºŒå¤±æ•—ï¼Œæ¸›å°‘æ‰¹æ¬¡å¤§å°
            self.current_batch_size = max(1, self.current_batch_size - 2)
            self.batch_stats['adaptive_adjustments'] += 1
        
        return self.current_batch_size
    
    def _split_large_document(self, doc: Document) -> List[Document]:
        """åˆ†å‰²éå¤§çš„æ–‡æª”"""
        content = doc.page_content
        max_chars = int(self.max_tokens_per_batch * 2.5)  # ä¼°ç®—å­—ç¬¦æ•¸
        
        # å˜—è©¦æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        split_docs = []
        current_content = ""
        part_index = 0
        
        for para in paragraphs:
            if len(current_content + para) <= max_chars or not current_content:
                current_content += ("\n\n" if current_content else "") + para
            else:
                if current_content.strip():
                    split_doc = Document(
                        page_content=current_content,
                        metadata={
                            **doc.metadata,
                            'chunk_id': f"{doc.metadata.get('chunk_id', 'unknown')}_part{part_index+1}",
                            'is_split_part': True,
                            'part_index': part_index,
                            'split_reason': 'token_limit'
                        }
                    )
                    split_docs.append(split_doc)
                    part_index += 1
                current_content = para
        
        # è™•ç†æœ€å¾Œä¸€éƒ¨åˆ†
        if current_content.strip():
            split_doc = Document(
                page_content=current_content,
                metadata={
                    **doc.metadata,
                    'chunk_id': f"{doc.metadata.get('chunk_id', 'unknown')}_part{part_index+1}",
                    'is_split_part': True,
                    'part_index': part_index,
                    'split_reason': 'token_limit'
                }
            )
            split_docs.append(split_doc)
        
        return split_docs or [doc]  # å¦‚æœåˆ†å‰²å¤±æ•—ï¼Œè¿”å›åŸæ–‡æª”
    
    def record_batch_result(self, success: bool, processing_time: float = 0):
        """è¨˜éŒ„æ‰¹æ¬¡è™•ç†çµæœ"""
        if success:
            self.success_streak += 1
            self.failure_streak = 0
        else:
            self.failure_streak += 1
            self.success_streak = 0
        
        # æ›´æ–°æˆåŠŸç‡
        total_attempts = self.batch_stats['total_batches']
        if total_attempts > 0:
            self.batch_stats['success_rate'] = (total_attempts - self.failure_streak) / total_attempts
        
        # æ›´æ–°å¹³å‡è™•ç†æ™‚é–“
        if processing_time > 0:
            current_avg = self.batch_stats['avg_batch_time']
            self.batch_stats['avg_batch_time'] = (current_avg + processing_time) / 2
    
    def get_performance_stats(self) -> Dict:
        """ç²å–æ€§èƒ½çµ±è¨ˆ"""
        return dict(self.batch_stats)

class OptimizedVectorSystem:
    """ğŸš€ å®Œæ•´å„ªåŒ–ç‰ˆå‘é‡ç³»çµ±"""
    
    def __init__(self, data_dir: str = None, model_type: str = None):
        """âœ… ä¿®æ­£ç‰ˆåˆå§‹åŒ–æ–¹æ³• - æ­£ç¢ºçš„åŸ·è¡Œé †åº"""
        
        # ğŸ”§ 1. åŸºæœ¬è®Šæ•¸è¨­ç½®
        self.data_dir = Path(data_dir or SYSTEM_CONFIG["data_dir"])
        self.model_type = model_type or "openai"
        self.persist_dir = Path(SYSTEM_CONFIG["persist_dir"])  # Chromaå‚™ç”¨

        # å»ºç«‹ç›®éŒ„
        self.data_dir.mkdir(exist_ok=True)
        
        # ğŸ”§ 2. è³‡æ–™åº«é€£æ¥è¨­ç½®ï¼ˆä½†ä¸æ¸¬è©¦ï¼‰
        self.db_adapter = None
        self.connection_string = None
        self.use_postgres = False

        database_url = os.getenv("DATABASE_URL")
        if PGVECTOR_AVAILABLE and database_url:
            self.connection_string = database_url
            print("ğŸ” ç™¼ç¾ DATABASE_URLï¼Œæº–å‚™æ¸¬è©¦ PostgreSQL é€£æ¥...")
        else:
            print("âš ï¸ DATABASE_URL æœªè¨­ç½®æˆ– PGVector ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ Chroma")

        if not PGVECTOR_AVAILABLE:
            print("âš ï¸ PGVector ä¾è³´æœªå®‰è£ï¼Œä½¿ç”¨ Chroma ä½œç‚ºå‚™ç”¨")
            self.persist_dir.mkdir(exist_ok=True)

        # âœ… 3. å…ˆåˆå§‹åŒ– Embedding æ¨¡å‹ï¼ˆé—œéµï¼ï¼‰
        self._setup_embedding_model()
        print("âœ… Embedding æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        # âœ… 4. ç¾åœ¨å¯ä»¥æ¸¬è©¦ PostgreSQL é€£æ¥äº†ï¼ˆembeddings å·²å­˜åœ¨ï¼‰
        if PGVECTOR_AVAILABLE and database_url and hasattr(self, 'embeddings'):
            try:
                print("ğŸ” æ¸¬è©¦ PostgreSQL + PGVector é€£æ¥...")
                # æ¸¬è©¦é€£æ¥
                PGVector.from_existing_index(
                    collection_name="_test_connection",
                    embedding=self.embeddings,  # âœ… ç¾åœ¨å®‰å…¨äº†
                    connection_string=self.connection_string
                )
                self.use_postgres = True
                print("âœ… PostgreSQL (pgvector) é€£æ¥æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ PostgreSQL (pgvector) é€£æ¥æ¸¬è©¦å¤±æ•—: {e}")
                self.use_postgres = False
                print("ğŸ”„ å›é€€åˆ° Chroma æœ¬åœ°å­˜å„²")
                self.persist_dir.mkdir(exist_ok=True)
        
        if not self.use_postgres:
            print("ğŸ“ ä½¿ç”¨ Chroma ä½œç‚ºå‘é‡å­˜å„²")
            self.persist_dir.mkdir(exist_ok=True)
        
        # ğŸ”§ 5. åˆå§‹åŒ–æ–‡æœ¬è™•ç†çµ„ä»¶
        self._setup_text_processing()
        
        # ğŸ”§ 6. åˆå§‹åŒ–è™•ç†å™¨
        self.batch_processor = AdaptiveBatchProcessor()
        self.text_splitter = OptimizedTextSplitter()
        
        # ğŸ”§ 7. åˆå§‹åŒ–å­˜å„²å’Œè¨˜éŒ„
        self._vector_stores = {}
        self.file_records = self._load_file_records()
        self.processing_lock = threading.Lock()
        
        print(f"ğŸš€ å®Œæ•´å„ªåŒ–ç‰ˆå‘é‡ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ¤– åµŒå…¥æ¨¡å‹: {self.model_type}")
        print(f"   ğŸ“ æ•¸æ“šç›®éŒ„: {self.data_dir}")
        print(f"   ğŸ—„ï¸ å‘é‡åº«: {'PostgreSQL + PGVector' if self.use_postgres else 'Chroma (æœ¬åœ°)'}")
        print(f"   ğŸ§  æ™ºèƒ½æ–‡æœ¬è™•ç†: âœ…")
        print(f"   ğŸ”§ è‡ªé©æ‡‰æ‰¹æ¬¡: âœ…")


    def _setup_embedding_model(self):
        """è¨­å®šåµŒå…¥æ¨¡å‹"""
        try:
            if self.model_type == "openai":
                if not OPENAI_EMBEDDINGS_AVAILABLE:
                    raise ImportError("OpenAI Embeddings ä¸å¯ç”¨")
                
                print(f"ğŸ”§ åˆå§‹åŒ– OpenAI Embeddings...")
                
                api_key = os.getenv("OPENAI_API_KEY")
                base_url = os.getenv("OPENAI_API_BASE")
                
                embedding_params = {
                    "model": "text-embedding-3-small",
                    "api_key": api_key,
                    "max_retries": 3,
                    "request_timeout": 60  # å¢åŠ è¶…æ™‚æ™‚é–“åˆ°60ç§’
                }
                
                if base_url:
                    embedding_params["base_url"] = base_url
                    print(f"ğŸ”§ ä½¿ç”¨è‡ªå®šç¾© API ç«¯é»: {base_url}")
                
                self.embeddings = OpenAIEmbeddings(**embedding_params)
                print(f"âœ… OpenAI Embeddings åˆå§‹åŒ–æˆåŠŸ")
                
            else:
                # HuggingFace æ¨¡å‹
                print(f"ğŸ”§ åˆå§‹åŒ– HuggingFace Embeddings...")
                
                self.embeddings = HuggingFaceEmbeddings(
                    model_name="BAAI/bge-small-zh-v1.5",
                    model_kwargs={"device": "cpu"},
                    encode_kwargs={"batch_size": 16, "normalize_embeddings": True}
                )
                print(f"âœ… HuggingFace Embeddings åˆå§‹åŒ–æˆåŠŸ")
                
        except Exception as e:
            print(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
            
            # å›é€€æ©Ÿåˆ¶
            if self.model_type == "openai":
                print("ğŸ”„ å˜—è©¦ HuggingFace å‚™é¸...")
                try:
                    self.embeddings = HuggingFaceEmbeddings(
                        model_name="BAAI/bge-small-zh-v1.5",
                        model_kwargs={"device": "cpu"}
                    )
                    self.model_type = "huggingface"
                    print("âœ… å·²å›é€€åˆ° HuggingFace")
                except Exception as e2:
                    raise RuntimeError(f"æ‰€æœ‰åµŒå…¥æ¨¡å‹éƒ½åˆå§‹åŒ–å¤±æ•—: {e2}")
            else:
                raise

    def load_document(self, file_path: Path) -> List[Document]:
        """è¼‰å…¥ä¸¦æ™ºèƒ½è™•ç†æ–‡æª”"""
        try:
            extension = file_path.suffix.lower()
            
            # æ ¹æ“šæª”æ¡ˆé¡å‹è¼‰å…¥
            if extension == '.pdf':
                loader = PyPDFLoader(str(file_path))
                documents = loader.load()
                full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            elif extension in ['.docx', '.doc'] and DOCX2TXT_AVAILABLE:
                if DOCX_METHOD == "docx2txt":
                    import docx2txt
                    full_text = docx2txt.process(str(file_path))
                else:
                    loader = Docx2txtLoader(str(file_path))
                    documents = loader.load()
                    full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            elif extension == '.csv':
                loader = CSVLoader(str(file_path), encoding='utf-8')
                documents = loader.load()
                full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            elif extension == '.json':
                loader = JSONLoader(str(file_path), jq_schema='.', text_content=False)
                documents = loader.load()
                full_text = "\n".join([doc.page_content for doc in documents if doc.page_content])
            else:
                # å˜—è©¦è‡ªå‹•æª¢æ¸¬ç·¨ç¢¼
                encodings = ['utf-8', 'gbk', 'gb2312', 'big5']
                full_text = None
                
                for encoding in encodings:
                    try:
                        with open(file_path, 'r', encoding=encoding) as f:
                            full_text = f.read()
                        break
                    except UnicodeDecodeError:
                        continue
                
                if full_text is None:
                    raise ValueError(f"ç„¡æ³•è§£ç¢¼æ–‡ä»¶: {file_path}")
            
            if not full_text or not full_text.strip():
                logger.warning(f"æ–‡ä»¶å…§å®¹ç‚ºç©ºæˆ–ç„¡æ³•æå–: {file_path.name}")
                return []
            
            # ç”Ÿæˆæ–‡æª”ID
            doc_id = self._generate_doc_id(file_path)
            
            # ä½¿ç”¨å„ªåŒ–çš„æ–‡æœ¬åˆ†å‰²å™¨
            source_info = {
                'file_path': str(file_path),
                'file_type': extension,
                'file_size': file_path.stat().st_size if file_path.exists() else 0
            }
            
            documents = self.text_splitter.smart_split_documents(full_text, doc_id, source_info)
            
            # æ·»åŠ çµ±ä¸€å…ƒæ•¸æ“š
            for doc in documents:
                doc.metadata.update({
                    'source': str(file_path),
                    'filename': file_path.name,
                    'extension': extension,
                    'file_size': source_info['file_size'],
                    'load_timestamp': time.time()
                })
            
            print(f"ğŸ“„ æ–‡æª”è¼‰å…¥å®Œæˆ: {file_path.name} ({len(documents)} å€‹åˆ†å¡Š)")
            
            return documents
            
        except Exception as e:
            logger.error(f"è¼‰å…¥æ–‡æª”å¤±æ•— {file_path}: {e}")
            print(f"âŒ æ–‡æª”è¼‰å…¥å¤±æ•—: {file_path.name} - {e}")
            return []
    
    def _setup_text_processing(self):
        """è¨­å®šæ–‡æœ¬è™•ç†çµ„ä»¶"""
        self.normalizer = ChineseTextNormalizer()
        self.analyzer = SmartTextAnalyzer()
        print("âœ… æ–‡æœ¬è™•ç†çµ„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    def _load_file_records(self) -> Dict[str, Dict[str, FileInfo]]:
        """è¼‰å…¥æª”æ¡ˆè¨˜éŒ„ - åŠ å¼·éŒ¯èª¤è™•ç†å’Œæ¢å¾©æ©Ÿåˆ¶"""
        record_file = self.data_dir / "file_records.json"
        
        # ğŸ”§ æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not record_file.exists():
            print("ğŸ“ æª”æ¡ˆè¨˜éŒ„ä¸å­˜åœ¨ï¼Œå°‡å»ºç«‹æ–°çš„è¨˜éŒ„")
            return {}
        
        try:
            # ğŸ”§ è®€å–ä¸¦æª¢æŸ¥æª”æ¡ˆå…§å®¹
            with open(record_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            # ğŸ”§ æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºç©º
            if not content:
                print("âš ï¸ æª”æ¡ˆè¨˜éŒ„ç‚ºç©ºï¼Œå°‡å»ºç«‹æ–°çš„è¨˜éŒ„")
                return {}
            
            # ğŸ”§ æª¢æŸ¥æ˜¯å¦ä»¥ { é–‹é ­ï¼ˆåŸºæœ¬ JSON æ ¼å¼æª¢æŸ¥ï¼‰
            if not content.startswith('{'):
                print(f"âš ï¸ æª”æ¡ˆè¨˜éŒ„æ ¼å¼éŒ¯èª¤ï¼Œå…§å®¹é–‹é ­: {repr(content[:50])}")
                return self._handle_corrupted_records(record_file, content)
            
            # ğŸ”§ å˜—è©¦è§£æ JSON
            try:
                data = json.loads(content)
            except json.JSONDecodeError as json_error:
                print(f"âŒ JSON è§£æå¤±æ•—: {json_error}")
                print(f"   éŒ¯èª¤ä½ç½®: line {json_error.lineno}, column {json_error.colno}")
                print(f"   æª”æ¡ˆå‰ 100 å­—ç¬¦: {repr(content[:100])}")
                return self._handle_corrupted_records(record_file, content)
            
            # ğŸ”§ é©—è­‰è³‡æ–™æ ¼å¼
            if not isinstance(data, dict):
                print(f"âš ï¸ æª”æ¡ˆè¨˜éŒ„æ ¼å¼éŒ¯èª¤ï¼Œæ‡‰ç‚ºå­—å…¸ä½†å¾—åˆ°: {type(data)}")
                return {}
            
            # ğŸ”§ è½‰æ›ç‚º FileInfo ç‰©ä»¶
            records = {}
            for collection, files in data.items():
                records[collection] = {}
                for file_path, info in files.items():
                    try:
                        if isinstance(info, dict):
                            fileinfo_fields = {
                                'path': info.get('path', file_path),
                                'size': info.get('size', 0),
                                'mtime': info.get('mtime', time.time()),
                                'hash': info.get('hash', ''),
                                'encoding': info.get('encoding', 'utf-8'),
                                'file_type': info.get('file_type', '')
                            }
                            
                            file_info_obj = FileInfo(**fileinfo_fields)
                            
                            # ğŸ”§ æ¢å¾©é¡å¤–å±¬æ€§
                            if 'uploaded_by' in info:
                                file_info_obj.uploaded_by = info['uploaded_by']
                            if 'uploaded_at' in info:
                                file_info_obj.uploaded_at = info['uploaded_at']
                            if 'file_source' in info:
                                file_info_obj.file_source = info['file_source']
                            
                            records[collection][file_path] = file_info_obj
                        else:
                            records[collection][file_path] = info
                            
                    except Exception as e:
                        logger.warning(f"è¼‰å…¥æª”æ¡ˆè¨˜éŒ„å¤±æ•— {file_path}: {e}")
                        # ğŸ”§ å»ºç«‹é è¨­è¨˜éŒ„
                        try:
                            default_info = FileInfo(
                                path=file_path,
                                size=0,
                                mtime=time.time(),
                                hash='',
                                encoding='utf-8',
                                file_type=''
                            )
                            records[collection][file_path] = default_info
                        except Exception:
                            logger.error(f"ç„¡æ³•å»ºç«‹é è¨­ FileInfo for {file_path}")
                            continue
            
            print(f"âœ… æª”æ¡ˆè¨˜éŒ„è¼‰å…¥æˆåŠŸ: {len(records)} å€‹é›†åˆ")
            return records
            
        except Exception as e:
            logger.error(f"è¼‰å…¥æª”æ¡ˆè¨˜éŒ„å¤±æ•—: {e}")
            print(f"âŒ åš´é‡éŒ¯èª¤ï¼Œè¼‰å…¥æª”æ¡ˆè¨˜éŒ„å¤±æ•—: {e}")
            return self._handle_corrupted_records(record_file, "")
    def _handle_corrupted_records(self, record_file: Path, content: str) -> Dict:
        """è™•ç†æå£çš„æª”æ¡ˆè¨˜éŒ„"""
        try:
            # ğŸ”§ å»ºç«‹å‚™ä»½
            backup_file = record_file.with_suffix('.json.corrupted')
            backup_counter = 1
            while backup_file.exists():
                backup_file = record_file.with_suffix(f'.json.corrupted.{backup_counter}')
                backup_counter += 1
            
            if content:
                with open(backup_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                print(f"ğŸ“ æå£çš„æª”æ¡ˆå·²å‚™ä»½è‡³: {backup_file}")
            
            # ğŸ”§ å˜—è©¦å¾å¯¦éš›æª”æ¡ˆé‡å»ºè¨˜éŒ„
            print("ğŸ”„ å˜—è©¦å¾å¯¦éš›æª”æ¡ˆé‡å»ºè¨˜éŒ„...")
            return self._rebuild_file_records()
            
        except Exception as e:
            logger.error(f"è™•ç†æå£è¨˜éŒ„å¤±æ•—: {e}")
            return {}

    def _rebuild_file_records(self) -> Dict:
        """å¾å¯¦éš›æª”æ¡ˆé‡å»ºè¨˜éŒ„"""
        try:
            rebuilt_records = {}
            
            # ğŸ”§ æƒæ data ç›®éŒ„
            for collection_dir in self.data_dir.iterdir():
                if collection_dir.is_dir():
                    collection_name = f"collection_{collection_dir.name}"
                    rebuilt_records[collection_name] = {}
                    
                    print(f"ğŸ” é‡å»ºé›†åˆ: {collection_name}")
                    
                    # æƒæç›®éŒ„ä¸­çš„æª”æ¡ˆ
                    for file_path in collection_dir.rglob('*'):
                        if (file_path.is_file() and 
                            file_path.suffix.lower() in SUPPORTED_EXTENSIONS and
                            not file_path.name.startswith('.')):
                            
                            try:
                                file_info = self.get_file_info(file_path)
                                if file_info:
                                    # è¨­å®šç‚ºé‡å»ºçš„æª”æ¡ˆ
                                    file_info.file_source = "rebuilt"
                                    file_info.uploaded_by = "ç³»çµ±é‡å»º"
                                    rebuilt_records[collection_name][str(file_path)] = file_info
                                    print(f"   ğŸ“„ é‡å»º: {file_path.name}")
                            except Exception as e:
                                logger.warning(f"é‡å»ºæª”æ¡ˆè¨˜éŒ„å¤±æ•— {file_path}: {e}")
            
            # ğŸ”§ ä¿å­˜é‡å»ºçš„è¨˜éŒ„
            if rebuilt_records:
                print(f"ğŸ’¾ ä¿å­˜é‡å»ºçš„è¨˜éŒ„...")
                self.file_records = rebuilt_records
                self._save_file_records()
                print(f"âœ… è¨˜éŒ„é‡å»ºå®Œæˆ: {len(rebuilt_records)} å€‹é›†åˆ")
            
            return rebuilt_records
            
        except Exception as e:
            logger.error(f"é‡å»ºæª”æ¡ˆè¨˜éŒ„å¤±æ•—: {e}")
            return {}    

        
    def _save_file_records(self):
        """å„²å­˜æ–‡ä»¶è¨˜éŒ„ - ä¿®æ­£ï¼šä½¿ç”¨data_dirè€Œä¸æ˜¯persist_dir"""
        try:
            records_file = self.data_dir / "file_records.json"  # ğŸ”§ ä¿®æ­£ï¼šæ”¹ç‚ºdata_dir
            
            # å‰µå»ºå‚™ä»½
            if records_file.exists():
                backup_file = records_file.with_suffix('.json.backup')
                try:
                    import shutil
                    shutil.copy2(records_file, backup_file)
                except Exception as e:
                    logger.warning(f"å‰µå»ºå‚™ä»½å¤±æ•—: {e}")
            
            data = {}
            total_files = 0
            
            for collection, files in self.file_records.items():
                data[collection] = {}
                
                for file_path, info in files.items():
                    try:
                        if hasattr(info, '__dict__'):
                            record_dict = {
                                'path': getattr(info, 'path', file_path),
                                'size': getattr(info, 'size', 0),
                                'mtime': getattr(info, 'mtime', time.time()),
                                'hash': getattr(info, 'hash', ''),
                                'encoding': getattr(info, 'encoding', 'utf-8'),
                                'file_type': getattr(info, 'file_type', '')
                            }
                            
                            extra_fields = ['uploaded_by', 'uploaded_at', 'file_source']
                            for field in extra_fields:
                                if hasattr(info, field):
                                    value = getattr(info, field)
                                    if value is not None:
                                        record_dict[field] = value
                            
                            try:
                                normalized_path = str(Path(file_path).absolute())
                                data[collection][normalized_path] = record_dict
                            except Exception:
                                data[collection][file_path] = record_dict
                                
                            total_files += 1
                            
                        else:
                            try:
                                normalized_path = str(Path(file_path).absolute())
                                data[collection][normalized_path] = info
                            except Exception:
                                data[collection][file_path] = info
                            total_files += 1
                            
                    except Exception as e:
                        logger.warning(f"è™•ç†æª”æ¡ˆè¨˜éŒ„å¤±æ•— {file_path}: {e}")
                        continue
            
            # å®‰å…¨å¯«å…¥æª”æ¡ˆ
            temp_file = records_file.with_suffix('.json.tmp')
            try:
                with open(temp_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                if records_file.exists():
                    records_file.unlink()
                temp_file.rename(records_file)
                
                logger.info(f"ğŸ“ æ–‡ä»¶è¨˜éŒ„å·²ä¿å­˜åˆ°dataç›®éŒ„:")
                logger.info(f"   ğŸ“ é›†åˆæ•¸: {len(data)}")
                logger.info(f"   ğŸ“„ ç¸½æª”æ¡ˆ: {total_files}")
                
            except Exception as e:
                if temp_file.exists():
                    temp_file.unlink()
                raise e
                
        except Exception as e:
            logger.error(f"å„²å­˜æ–‡ä»¶è¨˜éŒ„å¤±æ•—: {e}")
            raise

    def get_file_source_statistics(self) -> Dict[str, Dict[str, int]]:
        """ç²å–æª”æ¡ˆä¾†æºçµ±è¨ˆ"""
        stats = {}
        
        for collection_name, files in self.file_records.items():
            display_name = collection_name.replace('collection_', '')
            stats[display_name] = {
                'total': len(files),
                'upload': 0,
                'sync': 0,
                'unknown': 0
            }
            
            for file_path, file_info in files.items():
                source = 'unknown'
                
                # æª¢æŸ¥æª”æ¡ˆä¾†æº
                if hasattr(file_info, 'file_source'):
                    source = file_info.file_source
                elif hasattr(file_info, '__dict__'):
                    # æª¢æŸ¥å­—å…¸æ ¼å¼
                    if isinstance(file_info, dict):
                        source = file_info.get('file_source', 'unknown')
                    elif hasattr(file_info, 'uploaded_by'):
                        source = 'upload'
                    else:
                        source = 'sync'  # å‡è¨­æ˜¯åŒæ­¥è€Œä¾†
                else:
                    source = 'sync'  # é è¨­ç‚ºåŒæ­¥
                
                # çµ±è¨ˆ
                if source in stats[display_name]:
                    stats[display_name][source] += 1
                else:
                    stats[display_name]['unknown'] += 1
        
        return stats
    
    def diagnose_file_records(self) -> Dict:
        """è¨ºæ–·æª”æ¡ˆè¨˜éŒ„ç‹€æ…‹"""
        diagnosis = {
            'total_collections': len(self.file_records),
            'total_files': 0,
            'source_stats': {},
            'path_issues': [],
            'missing_files': [],
            'orphaned_records': [],
            'recommendations': []
        }
        
        try:
            diagnosis['source_stats'] = self.get_file_source_statistics()
        except Exception as e:
            logger.warning(f"çµ±è¨ˆä¾†æºå¤±æ•—: {e}")
            diagnosis['source_stats'] = {}
        
        # æª¢æŸ¥æ¯å€‹é›†åˆçš„æª”æ¡ˆ
        for collection_name, files in self.file_records.items():
            diagnosis['total_files'] += len(files)
            
            for file_path, file_info in files.items():
                try:
                    file_path_obj = Path(file_path)
                    
                    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å¯¦éš›å­˜åœ¨
                    if not file_path_obj.exists():
                        diagnosis['missing_files'].append({
                            'collection': collection_name,
                            'path': file_path,
                            'name': file_path_obj.name
                        })
                    
                    # æª¢æŸ¥è·¯å¾‘æ ¼å¼å•é¡Œ
                    if '\\' in file_path and '/' in file_path:
                        diagnosis['path_issues'].append({
                            'collection': collection_name,
                            'path': file_path,
                            'issue': 'mixed_path_separators'
                        })
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚ºå­¤ç«‹è¨˜éŒ„ï¼ˆè·¯å¾‘ä¸åœ¨é æœŸçš„ data/ ç›®éŒ„ä¸‹ï¼‰
                    expected_prefix = str(self.data_dir)
                    if not file_path.startswith(expected_prefix):
                        diagnosis['orphaned_records'].append({
                            'collection': collection_name,
                            'path': file_path,
                            'issue': 'outside_data_directory'
                        })
                        
                except Exception as e:
                    logger.warning(f"è¨ºæ–·æª”æ¡ˆè¨˜éŒ„å¤±æ•— {file_path}: {e}")
                    diagnosis['path_issues'].append({
                        'collection': collection_name,
                        'path': file_path,
                        'issue': f'diagnosis_error: {str(e)}'
                    })
        
        # ç”Ÿæˆå»ºè­°
        if diagnosis['missing_files']:
            count = len(diagnosis['missing_files'])
            diagnosis['recommendations'].append(
                f"ç™¼ç¾ {count} å€‹éºå¤±æª”æ¡ˆï¼Œå»ºè­°åŸ·è¡ŒåŒæ­¥ä¿®å¾©æˆ–æ¸…ç†ç„¡æ•ˆè¨˜éŒ„"
            )
        
        if diagnosis['path_issues']:
            count = len(diagnosis['path_issues'])
            diagnosis['recommendations'].append(
                f"ç™¼ç¾ {count} å€‹è·¯å¾‘æ ¼å¼å•é¡Œï¼Œå»ºè­°æ¨™æº–åŒ–è·¯å¾‘æ ¼å¼"
            )
        
        if diagnosis['orphaned_records']:
            count = len(diagnosis['orphaned_records'])
            diagnosis['recommendations'].append(
                f"ç™¼ç¾ {count} å€‹å­¤ç«‹è¨˜éŒ„ï¼Œå»ºè­°æª¢æŸ¥æª”æ¡ˆä½ç½®æˆ–æ¸…ç†è¨˜éŒ„"
            )
        
        if not diagnosis['recommendations']:
            diagnosis['recommendations'].append("æª”æ¡ˆè¨˜éŒ„ç‹€æ…‹è‰¯å¥½ï¼Œç„¡éœ€ç‰¹åˆ¥è™•ç†")
        
        return diagnosis

    def cleanup_invalid_records(self) -> Dict:
        """æ¸…ç†ç„¡æ•ˆçš„æª”æ¡ˆè¨˜éŒ„"""
        cleanup_result = {
            'cleaned_collections': 0,
            'removed_records': 0,
            'errors': []
        }
        
        try:
            for collection_name in list(self.file_records.keys()):
                files = self.file_records[collection_name]
                original_count = len(files)
                
                # æª¢æŸ¥ä¸¦ç§»é™¤ç„¡æ•ˆè¨˜éŒ„
                valid_files = {}
                
                for file_path, file_info in files.items():
                    try:
                        if Path(file_path).exists():
                            valid_files[file_path] = file_info
                        else:
                            logger.info(f"ç§»é™¤ç„¡æ•ˆè¨˜éŒ„: {file_path}")
                            
                    except Exception as e:
                        logger.warning(f"æª¢æŸ¥è¨˜éŒ„æ™‚å‡ºéŒ¯ {file_path}: {e}")
                        cleanup_result['errors'].append(f"{file_path}: {str(e)}")
                
                # æ›´æ–°è¨˜éŒ„
                if len(valid_files) != original_count:
                    self.file_records[collection_name] = valid_files
                    removed = original_count - len(valid_files)
                    cleanup_result['removed_records'] += removed
                    cleanup_result['cleaned_collections'] += 1
                    
                    logger.info(f"é›†åˆ {collection_name}: ç§»é™¤äº† {removed} å€‹ç„¡æ•ˆè¨˜éŒ„")
            
            # ä¿å­˜æ¸…ç†å¾Œçš„è¨˜éŒ„
            if cleanup_result['removed_records'] > 0:
                self._save_file_records()
                
        except Exception as e:
            logger.error(f"æ¸…ç†è¨˜éŒ„å¤±æ•—: {e}")
            cleanup_result['errors'].append(f"æ•´é«”æ¸…ç†å¤±æ•—: {str(e)}")
        
        return cleanup_result


    
    
    def _generate_doc_id(self, file_path: Path) -> str:
        """ç”Ÿæˆæ–‡æª”ID"""
        content_hash = hashlib.md5(str(file_path).encode()).hexdigest()[:8]
        return f"doc_{file_path.stem}_{content_hash}"
    
    def get_or_create_vectorstore(self, collection_name: str):
        """ç²å–æˆ–å‰µå»ºå‘é‡å­˜å„² - PostgreSQL å„ªå…ˆ"""
        if collection_name not in self._vector_stores:
            try:
                if self.use_postgres and PGVECTOR_AVAILABLE:
                    # ğŸ”§ ä½¿ç”¨ PGVector
                    try:
                        from langchain_postgres import PGVector
                        self._vector_stores[collection_name] = PGVector(
                            embeddings=self.embeddings,
                            collection_name=collection_name,
                            connection=self.connection_string,
                            use_jsonb=True,
                        )
                    except ImportError:
                        from langchain_community.vectorstores import PGVector
                        self._vector_stores[collection_name] = PGVector(
                            connection_string=self.connection_string,
                            collection_name=collection_name,
                            embedding_function=self.embeddings,
                            distance_strategy="cosine",
                            pre_delete_collection=False,
                            logger=logger
                        )
                    print(f"âœ… PGVector å‘é‡å­˜å„²å°±ç·’: {collection_name}")
                else:
                    # ğŸ”§ å‚™ç”¨ Chroma - ç¢ºä¿å°å…¥
                    if CHROMA_AVAILABLE:
                        from langchain_community.vectorstores import Chroma
                        self._vector_stores[collection_name] = Chroma(
                            collection_name=collection_name,
                            embedding_function=self.embeddings,
                            persist_directory=str(self.persist_dir)
                        )
                        print(f"âœ… Chroma å‘é‡å­˜å„²å°±ç·’: {collection_name}")
                    else:
                        raise ImportError("Chroma ä¸å¯ç”¨ä¸” PostgreSQL ä¹Ÿä¸å¯ç”¨")
                        
            except Exception as e:
                logger.error(f"å‘é‡å­˜å„²å‰µå»ºå¤±æ•—: {e}")
                raise RuntimeError(f"ç„¡æ³•å‰µå»ºå‘é‡å­˜å„²: {e}")
        
        return self._vector_stores[collection_name]
    
    def get_file_info(self, file_path: Path) -> Optional[FileInfo]:
        """ç²å–æ–‡ä»¶è³‡è¨Š - ä¿®æ­£ç‰ˆï¼šæ›´å¯é çš„å“ˆå¸Œè¨ˆç®—å’ŒéŒ¯èª¤è™•ç†"""
        try:
            if not file_path.exists():
                return None
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºç¬¦è™Ÿé€£çµ
            if file_path.is_symlink():
                print(f"âš ï¸ è·³éç¬¦è™Ÿé€£çµ: {file_path.name}")
                return None
            
            stat = file_path.stat()
            
            # ğŸ†• ä¿®æ­£ï¼šæ›´å¯é å’Œé«˜æ•ˆçš„æ–‡ä»¶å“ˆå¸Œè¨ˆç®—
            try:
                # åˆ†ç´šè™•ç†ä¸åŒå¤§å°çš„æª”æ¡ˆ
                if stat.st_size == 0:
                    # ç©ºæª”æ¡ˆ
                    file_hash = hashlib.md5(b"").hexdigest()
                elif stat.st_size < 1024 * 1024:  # å°æ–¼ 1MB
                    # å°æª”æ¡ˆï¼šç›´æ¥è®€å–å…¨éƒ¨å…§å®¹
                    try:
                        with open(file_path, 'rb') as f:
                            content = f.read()
                        file_hash = hashlib.md5(content).hexdigest()
                    except PermissionError:
                        # æ¬Šé™å•é¡Œï¼Œä½¿ç”¨æª”æ¡ˆå±¬æ€§ç”Ÿæˆå“ˆå¸Œ
                        file_hash = hashlib.md5(
                            f"{stat.st_size}:{stat.st_mtime}:{file_path.name}".encode()
                        ).hexdigest()
                elif stat.st_size < 50 * 1024 * 1024:  # å°æ–¼ 50MB
                    # ä¸­ç­‰æª”æ¡ˆï¼šåˆ†å¡Šè®€å–
                    hash_md5 = hashlib.md5()
                    try:
                        with open(file_path, 'rb') as f:
                            # 8KB å¡Šå¤§å°
                            for chunk in iter(lambda: f.read(8192), b""):
                                hash_md5.update(chunk)
                        file_hash = hash_md5.hexdigest()
                    except (PermissionError, OSError) as e:
                        logger.warning(f"ç„¡æ³•è®€å–æª”æ¡ˆ {file_path}: {e}")
                        # å›é€€åˆ°åŸºæ–¼æª”æ¡ˆå±¬æ€§çš„å“ˆå¸Œ
                        file_hash = hashlib.md5(
                            f"{stat.st_size}:{stat.st_mtime}:{file_path.name}".encode()
                        ).hexdigest()
                else:
                    # å¤§æª”æ¡ˆï¼šä½¿ç”¨æª”æ¡ˆå±¬æ€§ + éƒ¨åˆ†å…§å®¹
                    try:
                        # è®€å–æª”æ¡ˆé–‹é ­å’Œçµå°¾å„ 4KB
                        with open(file_path, 'rb') as f:
                            head = f.read(4096)
                            if stat.st_size > 8192:
                                f.seek(-4096, 2)  # å¾æœ«å°¾å‘å‰ 4KB
                                tail = f.read(4096)
                            else:
                                tail = b""
                        
                        hash_content = f"{stat.st_size}:{stat.st_mtime}:{file_path.name}".encode()
                        hash_content += head + tail
                        file_hash = hashlib.md5(hash_content).hexdigest()
                        
                    except (PermissionError, OSError) as e:
                        logger.warning(f"ç„¡æ³•è®€å–å¤§æª”æ¡ˆ {file_path}: {e}")
                        # ç´”å±¬æ€§å“ˆå¸Œ
                        file_hash = hashlib.md5(
                            f"{stat.st_size}:{stat.st_mtime}:{file_path.name}".encode()
                        ).hexdigest()
                        
            except Exception as e:
                # æœ€çµ‚å›é€€æ–¹æ¡ˆ
                logger.warning(f"æª”æ¡ˆå“ˆå¸Œè¨ˆç®—å¤±æ•— {file_path}: {e}")
                file_hash = hashlib.md5(
                    f"{stat.st_size}:{stat.st_mtime}:{file_path.name}".encode()
                ).hexdigest()
            
            return FileInfo(
                path=str(file_path),
                size=stat.st_size,
                mtime=stat.st_mtime,
                hash=file_hash,
                encoding="utf-8",
                file_type=file_path.suffix.lower()
            )
            
        except Exception as e:
            logger.error(f"ç²å–æ–‡ä»¶è³‡è¨Šå¤±æ•— {file_path}: {e}")
            return None
    
    def scan_directory_changes(self, dir_path: Path, collection_name: str) -> Tuple[List[Path], List[Path], List[str], Dict[str, FileInfo]]:
        """æƒæç›®éŒ„è®Šæ›´ - ä¿®æ­£ç‰ˆï¼šæ­£ç¢ºè™•ç†ä¸Šå‚³æª”æ¡ˆ"""
        current_files = {}
        
        print(f"ğŸ” æƒæç›®éŒ„: {dir_path}")
        
        # éè¿´æƒæç›®éŒ„
        file_count = 0
        for file_path in dir_path.rglob('*'):
            if (file_path.is_file() and 
                file_path.suffix.lower() in SUPPORTED_EXTENSIONS and
                not file_path.name.startswith('.') and
                file_path.stat().st_size > 0):  # è·³éç©ºæ–‡ä»¶
                
                file_info = self.get_file_info(file_path)
                if file_info:
                    # ğŸ†• ä¿®æ­£ï¼šä½¿ç”¨æ¨™æº–åŒ–çš„çµ•å°è·¯å¾‘ä½œç‚ºéµå€¼
                    try:
                        # ä½¿ç”¨ absolute() é¿å…ç¬¦è™Ÿé€£çµå•é¡Œ
                        absolute_path = str(file_path.absolute())
                        current_files[absolute_path] = file_info
                        file_count += 1
                    except Exception as e:
                        logger.warning(f"è·¯å¾‘æ¨™æº–åŒ–å¤±æ•— {file_path}: {e}")
                        # å›é€€åˆ°åŸå§‹è·¯å¾‘
                        current_files[str(file_path)] = file_info
                        file_count += 1
        
        print(f"ğŸ“„ æ‰¾åˆ° {file_count} å€‹æœ‰æ•ˆæª”æ¡ˆ")
        
        old_files = self.file_records.get(collection_name, {})
        print(f"ğŸ“‹ èˆŠè¨˜éŒ„ä¸­æœ‰ {len(old_files)} å€‹æª”æ¡ˆ")
        
        # ğŸ†• ä¿®æ­£ï¼šæ­£è¦åŒ–èˆŠè¨˜éŒ„çš„è·¯å¾‘éµå€¼
        normalized_old_files = {}
        normalization_errors = 0
        
        for old_path, old_info in old_files.items():
            try:
                old_path_obj = Path(old_path)
                
                if old_path_obj.is_absolute():
                    # å·²ç¶“æ˜¯çµ•å°è·¯å¾‘
                    normalized_key = str(old_path_obj.absolute())
                else:
                    # ç›¸å°è·¯å¾‘è½‰çµ•å°è·¯å¾‘
                    try:
                        abs_path = (dir_path / old_path).absolute()
                        normalized_key = str(abs_path)
                    except Exception:
                        # å¦‚æœç„¡æ³•è½‰æ›ï¼Œä¿æŒåŸæ¨£
                        normalized_key = old_path
                        
                normalized_old_files[normalized_key] = old_info
                
            except Exception as e:
                logger.warning(f"èˆŠè·¯å¾‘æ­£è¦åŒ–å¤±æ•— {old_path}: {e}")
                # ä¿æŒåŸè·¯å¾‘
                normalized_old_files[old_path] = old_info
                normalization_errors += 1
        
        if normalization_errors > 0:
            print(f"âš ï¸ {normalization_errors} å€‹èˆŠè·¯å¾‘æ­£è¦åŒ–å¤±æ•—")
        
        # ğŸ†• ä¿®æ­£ï¼šæ™ºèƒ½è®Šæ›´æª¢æ¸¬
        added_files = []
        modified_files = []
        
        print("ğŸ” æª¢æ¸¬è®Šæ›´...")
        
        for file_path, file_info in current_files.items():
            current_file_name = Path(file_path).name
            current_hash = file_info.hash
            
            # é¦–å…ˆå˜—è©¦ç²¾ç¢ºè·¯å¾‘åŒ¹é…
            if file_path in normalized_old_files:
                old_info = normalized_old_files[file_path]
                if old_info.hash != current_hash:
                    modified_files.append(Path(file_path))
                    print(f"ğŸ“ ä¿®æ”¹æª”æ¡ˆ: {current_file_name}")
            else:
                # ğŸ†• æ™ºèƒ½æª”æ¡ˆåŒ¹é…ï¼šæª¢æŸ¥æ˜¯å¦æ˜¯åŒä¸€æª”æ¡ˆçš„ä¸åŒè·¯å¾‘è¡¨ç¤º
                file_found = False
                
                for old_path, old_info in normalized_old_files.items():
                    old_file_name = Path(old_path).name
                    
                    # æª”æ¡ˆåç›¸åŒä¸”å“ˆå¸Œç›¸åŒ = åŒä¸€æª”æ¡ˆ
                    if (current_file_name == old_file_name and 
                        current_hash == old_info.hash):
                        file_found = True
                        print(f"ğŸ”„ è·¯å¾‘è®Šæ›´ä½†å…§å®¹ç›¸åŒ: {current_file_name}")
                        break
                        
                    # æª”æ¡ˆåç›¸åŒä½†å“ˆå¸Œä¸åŒ = æª”æ¡ˆè¢«ä¿®æ”¹
                    elif (current_file_name == old_file_name and 
                          current_hash != old_info.hash):
                        modified_files.append(Path(file_path))
                        file_found = True
                        print(f"ğŸ“ ä¿®æ”¹æª”æ¡ˆ (è·¯å¾‘è®Šæ›´): {current_file_name}")
                        break
                
                if not file_found:
                    added_files.append(Path(file_path))
                    print(f"ğŸ“„ æ–°æª”æ¡ˆ: {current_file_name}")
        
        # ğŸ†• ä¿®æ­£ï¼šæ™ºèƒ½åˆªé™¤æª¢æ¸¬
        deleted_files = []
        
        for old_path in normalized_old_files.keys():
            old_file_name = Path(old_path).name
            
            if old_path not in current_files:
                # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦çœŸçš„ä¸å­˜åœ¨ï¼ˆå¯èƒ½åªæ˜¯è·¯å¾‘è¡¨ç¤ºä¸åŒï¼‰
                file_still_exists = False
                
                for current_path in current_files.keys():
                    current_file_name = Path(current_path).name
                    if current_file_name == old_file_name:
                        # é€²ä¸€æ­¥æª¢æŸ¥æ˜¯å¦æ˜¯åŒä¸€æª”æ¡ˆï¼ˆé€šéå…§å®¹å“ˆå¸Œï¼‰
                        current_hash = current_files[current_path].hash
                        old_hash = normalized_old_files[old_path].hash
                        
                        if current_hash == old_hash:
                            file_still_exists = True
                            break
                
                if not file_still_exists:
                    deleted_files.append(old_path)
                    print(f"ğŸ—‘ï¸ åˆªé™¤æª”æ¡ˆ: {old_file_name}")
        
        print(f"ğŸ“Š è®Šæ›´çµ±è¨ˆ:")
        print(f"   ğŸ“„ æ–°å¢: {len(added_files)}")
        print(f"   ğŸ“ ä¿®æ”¹: {len(modified_files)}")
        print(f"   ğŸ—‘ï¸ åˆªé™¤: {len(deleted_files)}")
        
        return added_files, modified_files, deleted_files, current_files
    
    def incremental_update(self, collection_name: str, added_files: List[Path], 
                          modified_files: List[Path], deleted_files: List[str],
                          current_files: Dict[str, FileInfo]) -> bool:
        """ğŸš€ å„ªåŒ–ç‰ˆå¢é‡æ›´æ–°"""
        with self.processing_lock:
            try:
                vectorstore = self.get_or_create_vectorstore(collection_name)
                
                # è™•ç†åˆªé™¤å’Œä¿®æ”¹
                files_to_delete = deleted_files + [str(f) for f in modified_files]
                if files_to_delete:
                    for file_path in files_to_delete:
                        try:
                            vectorstore.delete(filter={"source": file_path})
                            print(f"ğŸ—‘ï¸ å·²åˆªé™¤: {Path(file_path).name}")
                        except Exception as e:
                            logger.warning(f"åˆªé™¤æ–‡æª”å¤±æ•— {file_path}: {e}")
                
                # è™•ç†æ–°å¢å’Œä¿®æ”¹çš„æ–‡ä»¶
                files_to_process = added_files + modified_files
                if not files_to_process:
                    print("   âœ… ç„¡æ–°æ–‡ä»¶éœ€è¦è™•ç†")
                    return True
                
                print(f"ğŸ“„ é–‹å§‹è™•ç† {len(files_to_process)} å€‹æ–‡ä»¶...")
                print(f"   âš ï¸ è™•ç†å¤§æ–‡ä»¶å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å¾…...")
                
                # ä¸¦ç™¼è¼‰å…¥æ–‡æª”
                all_documents = []
                if PERFORMANCE_CONFIG.get("parallel_processing", True):
                    all_documents = self._parallel_load_documents(files_to_process, collection_name)
                else:
                    all_documents = self._sequential_load_documents(files_to_process, collection_name)
                
                if not all_documents:
                    print("   âš ï¸ æ²’æœ‰æœ‰æ•ˆæ–‡æª”éœ€è¦å‘é‡åŒ–")
                    return True
                
                # çµ±è¨ˆå’Œæˆæœ¬ä¼°ç®—
                total_tokens = sum(doc.metadata.get('token_count', 0) for doc in all_documents)
                estimated_cost = self.batch_processor.token_estimator.estimate_embedding_cost(total_tokens)
                
                print(f"\nğŸ“Š å‘é‡åŒ–çµ±è¨ˆ:")
                print(f"   ğŸ“„ ç¸½åˆ†å¡Šæ•¸: {len(all_documents)}")
                print(f"   ğŸ“ ç¸½ tokens: {total_tokens:,}")
                print(f"   ğŸ’° ä¼°ç®—æˆæœ¬: ${estimated_cost:.4f}")
                
                # å‰µå»ºæ™ºèƒ½æ‰¹æ¬¡ä¸¦è™•ç†
                batches = self.batch_processor.create_smart_batches(all_documents)
                success_count = self._process_batches(vectorstore, batches)
                
                print(f"\nğŸ‰ å‘é‡åŒ–å®Œæˆï¼")
                print(f"   âœ… æˆåŠŸ: {success_count}/{len(all_documents)} å€‹åˆ†å¡Š")
                print(f"   ğŸ“Š æˆåŠŸç‡: {(success_count/len(all_documents)*100):.1f}%")
                
                # æ›´æ–°æ–‡ä»¶è¨˜éŒ„
                self.file_records[collection_name] = current_files
                self._save_file_records()
                
                # è¨˜æ†¶é«”æ¸…ç†
                if success_count % PERFORMANCE_CONFIG.get("gc_frequency", 50) == 0:
                    gc.collect()
                
                return success_count > 0
                
            except Exception as e:
                logger.error(f"å¢é‡æ›´æ–°å¤±æ•— {collection_name}: {e}")
                print(f"âŒ ç³»çµ±éŒ¯èª¤: {e}")
                return False
    
    def _parallel_load_documents(self, file_paths: List[Path], collection_name: str) -> List[Document]:
        """ä¸¦ç™¼è¼‰å…¥æ–‡æª”"""
        all_documents = []
        max_workers = min(SYSTEM_CONFIG.get("max_workers", 4), len(file_paths))
        
        print(f"   ğŸš€ ä¸¦ç™¼è¼‰å…¥ (å·¥ä½œç·šç¨‹: {max_workers})")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_file = {
                executor.submit(self.load_document, file_path): file_path 
                for file_path in file_paths
            }
            
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    documents = future.result()
                    if documents:
                        for doc in documents:
                            doc.metadata['collection'] = collection_name
                        all_documents.extend(documents)
                        print(f"   âœ… {file_path.name}: {len(documents)} åˆ†å¡Š")
                    else:
                        print(f"   âš ï¸ {file_path.name}: ç„¡æœ‰æ•ˆå…§å®¹")
                except Exception as e:
                    print(f"   âŒ {file_path.name}: {e}")
                    logger.error(f"ä¸¦ç™¼è¼‰å…¥å¤±æ•— {file_path}: {e}")
        
        return all_documents
    
    def _sequential_load_documents(self, file_paths: List[Path], collection_name: str) -> List[Document]:
        """é †åºè¼‰å…¥æ–‡æª”"""
        all_documents = []
        
        print(f"   ğŸ“„ é †åºè¼‰å…¥")
        
        for i, file_path in enumerate(file_paths, 1):
            try:
                print(f"   [{i}/{len(file_paths)}] è™•ç†: {file_path.name}")
                documents = self.load_document(file_path)
                
                if documents:
                    for doc in documents:
                        doc.metadata['collection'] = collection_name
                    all_documents.extend(documents)
                    print(f"      âœ… è¼‰å…¥ {len(documents)} å€‹åˆ†å¡Š")
                else:
                    print(f"      âš ï¸ ç„¡æœ‰æ•ˆå…§å®¹")
                    
            except Exception as e:
                print(f"      âŒ è¼‰å…¥å¤±æ•—: {e}")
                logger.error(f"æ–‡ä»¶è¼‰å…¥å¤±æ•— {file_path}: {e}")
        
        return all_documents
    
    from typing import Union, Any

    def _process_batches(self, vectorstore: Union["Chroma", Any], batches: List[Tuple[List[Document], Dict]]) -> int:
        """è™•ç†æ‰¹æ¬¡å‘é‡åŒ– - å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œå…ƒæ•¸æ“šä¿®å¾©"""

        success_count = 0
        total_docs = sum(len(batch_docs) for batch_docs, _ in batches)
        
        print(f"\nğŸ”„ é–‹å§‹æ‰¹æ¬¡å‘é‡åŒ–...")
        print(f"   ğŸ“¦ ç¸½æ‰¹æ¬¡æ•¸: {len(batches)}")
        print(f"   ğŸ“„ ç¸½æ–‡æª”æ•¸: {total_docs}")
        
        for batch_num, (batch_docs, batch_info) in enumerate(batches, 1):
            print(f"\n   ğŸ“¦ æ‰¹æ¬¡ {batch_num}/{len(batches)}")
            print(f"      ğŸ“„ æ–‡æª”æ•¸: {batch_info['documents']}")
            print(f"      ğŸ“ tokens: {batch_info['tokens']:,}")
            print(f"      ğŸ“Š ä½¿ç”¨ç‡: {(batch_info['tokens']/TOKEN_LIMITS['max_tokens_per_request']*100):.1f}%")
            
            # é¡¯ç¤ºæ–‡æª”é¡å‹åˆ†å¸ƒ
            type_info = ", ".join([f"{k}:{v}" for k, v in batch_info['types'].items()])
            print(f"      ğŸ·ï¸ é¡å‹: {type_info}")
            
            start_time = time.time()
            
            try:
                print(f"      ğŸš€ é–‹å§‹è™•ç†æ‰¹æ¬¡ {batch_num}...")
                print(f"      ğŸ“¡ æ­£åœ¨èª¿ç”¨ OpenAI API... (é€™å¯èƒ½éœ€è¦ 30-60 ç§’)")
                
                # ğŸ› ï¸ ä¿®å¾©ï¼šçµ±ä¸€è™•ç†å…ƒæ•¸æ“šï¼Œç¢ºä¿é¡å‹æ­£ç¢º
                safe_docs = []
                for doc in batch_docs:
                    safe_metadata = self._ensure_simple_metadata(doc.metadata)
                    safe_doc = Document(page_content=doc.page_content, metadata=safe_metadata)
                    safe_docs.append(safe_doc)

                print(f"      ğŸ”§ å·²è™•ç† {len(safe_docs)} å€‹æ–‡æª”çš„å…ƒæ•¸æ“šï¼Œç¢ºä¿é¡å‹å…¼å®¹")
                
                vectorstore.add_documents(safe_docs)
                processing_time = time.time() - start_time
                
                success_count += len(batch_docs)
                self.batch_processor.record_batch_result(True, processing_time)
                
                print(f"      âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ ({processing_time:.1f}s)")
                print(f"      ğŸ“Š ç¸½é€²åº¦: {success_count}/{total_docs} ({success_count/total_docs*100:.1f}%)")
                
                # æ‰¹æ¬¡é–“å»¶é²
                if batch_num < len(batches):
                    delay = TOKEN_LIMITS["batch_delay"]
                    print(f"      â±ï¸ ç­‰å¾… {delay} ç§’...")
                    time.sleep(delay)
                    
            except Exception as e:
                processing_time = time.time() - start_time
                self.batch_processor.record_batch_result(False, processing_time)
                
                error_msg = str(e)
                print(f"      âŒ æ‰¹æ¬¡ {batch_num} å¤±æ•— ({processing_time:.1f}s)")
                print(f"         éŒ¯èª¤: {error_msg}")
                
                # ğŸ”§ ç‰¹åˆ¥è™•ç†å…ƒæ•¸æ“šéŒ¯èª¤
                if "metadata" in error_msg.lower():
                    print(f"         ğŸ”§ æª¢æ¸¬åˆ°å…ƒæ•¸æ“šéŒ¯èª¤ï¼Œå˜—è©¦æ›´åš´æ ¼çš„è™•ç†...")
                    try:
                        # é‡æ–°å˜—è©¦ï¼Œä½¿ç”¨æœ€åš´æ ¼çš„å…ƒæ•¸æ“šéæ¿¾
                        ultra_safe_docs = []
                        for doc in batch_docs:
                            # åªä¿ç•™æœ€åŸºæœ¬çš„å…ƒæ•¸æ“šå­—æ®µ
                            minimal_metadata = {
                                'doc_id': str(doc.metadata.get('doc_id', 'unknown')),
                                'chunk_id': str(doc.metadata.get('chunk_id', 'unknown')),
                                'chunk_index': int(doc.metadata.get('chunk_index', 0)),
                                'text_type': str(doc.metadata.get('text_type', 'unknown')),
                                'source': str(doc.metadata.get('source', 'unknown')),
                                'filename': str(doc.metadata.get('filename', 'unknown')),
                                'token_count': int(doc.metadata.get('token_count', 0)),
                                'chunk_length': int(doc.metadata.get('chunk_length', 0)),
                                # URL è™•ç†
                                'contained_urls': str(doc.metadata.get('contained_urls', '')),
                                'url_count': int(doc.metadata.get('url_count', 0)),
                                'has_urls': bool(doc.metadata.get('has_urls', False))
                            }
                            
                            ultra_safe_doc = Document(
                                page_content=doc.page_content,
                                metadata=minimal_metadata
                            )
                            ultra_safe_docs.append(ultra_safe_doc)
                        
                        vectorstore.add_documents(ultra_safe_docs)
                        success_count += len(batch_docs)
                        print(f"         âœ… ä½¿ç”¨æœ€å°åŒ–å…ƒæ•¸æ“šé‡æ–°è™•ç†æˆåŠŸ")
                        continue
                        
                    except Exception as retry_e:
                        print(f"         âŒ é‡æ–°è™•ç†ä¹Ÿå¤±æ•—: {retry_e}")
                
                # å…¶ä»–éŒ¯èª¤è™•ç†
                if "timeout" in error_msg.lower():
                    print(f"         ğŸ• è¶…æ™‚éŒ¯èª¤ï¼Œå»¶é•·ç­‰å¾…æ™‚é–“...")
                    time.sleep(30)
                elif "rate_limit" in error_msg.lower() or "429" in error_msg:
                    print(f"         ğŸš¦ é€Ÿç‡é™åˆ¶ï¼Œå»¶é•·ç­‰å¾…...")
                    time.sleep(60)
                elif "token" in error_msg.lower() and batch_info['documents'] > 1:
                    print(f"         ğŸ”§ Token è¶…é™ï¼Œå˜—è©¦å–®å€‹è™•ç†...")
                    single_success = self._process_documents_individually(vectorstore, batch_docs)
                    success_count += single_success
                elif "connection" in error_msg.lower():
                    print(f"         ğŸŒ é€£æ¥éŒ¯èª¤ï¼Œç­‰å¾…é‡è©¦...")
                    time.sleep(20)
                    try:
                        print(f"         ğŸ”„ é‡è©¦æ‰¹æ¬¡ {batch_num}...")
                        # ä½¿ç”¨å®‰å…¨çš„å…ƒæ•¸æ“šé‡è©¦
                        safe_docs = []
                        for doc in batch_docs:
                            safe_metadata = self._ensure_simple_metadata(doc.metadata)
                            safe_doc = Document(page_content=doc.page_content, metadata=safe_metadata)
                            safe_docs.append(safe_doc)
                        
                        vectorstore.add_documents(safe_docs)
                        success_count += len(batch_docs)
                        print(f"         âœ… é‡è©¦æˆåŠŸ")
                    except Exception as retry_e:
                        print(f"         âŒ é‡è©¦å¤±æ•—: {retry_e}")
                else:
                    print(f"         âš ï¸ è·³éæ­¤æ‰¹æ¬¡")
                    
                # æ¯æ¬¡éŒ¯èª¤å¾Œæ·»åŠ é¡å¤–å»¶é²
                print(f"         â¸ï¸ éŒ¯èª¤å¾Œæš«åœ 10 ç§’...")
                time.sleep(10)
        
        return success_count
    
    def _ensure_simple_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """ç¢ºä¿å…ƒæ•¸æ“šåªåŒ…å«Chromaæ”¯æŒçš„ç°¡å–®é¡å‹ï¼šstring, int, float, bool"""
        safe_metadata = {}
        
        for key, value in metadata.items():
            if isinstance(value, (str, int, float, bool)) or value is None:
                # âœ… å·²ç¶“æ˜¯Chromaæ”¯æŒçš„ç°¡å–®é¡å‹ï¼Œç›´æ¥ä¿ç•™
                safe_metadata[key] = value
            elif isinstance(value, list):
                # âŒ åˆ—è¡¨ä¸è¢«æ”¯æŒ â†’ è½‰ç‚ºåˆ†éš”ç¬¦å­—ä¸²
                if key == 'contained_urls' or 'url' in key.lower():
                    safe_metadata[key] = '|'.join(str(v) for v in value) if value else ''
                    safe_metadata[f'{key}_count'] = len(value)
                else:
                    safe_metadata[key] = '|'.join(str(v) for v in value) if value else ''
                    safe_metadata[f'{key}_count'] = len(value)
            elif isinstance(value, dict):
                # âŒ å­—å…¸ä¸è¢«æ”¯æŒ â†’ è½‰ç‚ºJSONå­—ç¬¦ä¸²
                safe_metadata[key] = json.dumps(value, ensure_ascii=False)
            else:
                # âŒ å…¶ä»–é¡å‹ä¸è¢«æ”¯æŒ â†’ è½‰ç‚ºå­—ç¬¦ä¸²
                safe_metadata[key] = str(value)
        
        return safe_metadata

    
    def _process_documents_individually(self, vectorstore: Chroma, documents: List[Document]) -> int:
        """å–®å€‹è™•ç†æ–‡æª”"""
        success_count = 0
        
        for i, doc in enumerate(documents):
            try:
                doc_tokens = doc.metadata.get('token_count', 0)
                if doc_tokens > TOKEN_LIMITS['max_tokens_per_request']:
                    print(f"         âš ï¸ æ–‡æª” {i+1} ä»ç„¶éå¤§ ({doc_tokens:,} tokens)ï¼Œè·³é")
                    continue
                
                vectorstore.add_documents([doc])
                success_count += 1
                print(f"         âœ… å–®å€‹æ–‡æª” {i+1}/{len(documents)} å®Œæˆ")
                time.sleep(1)  # å–®å€‹è™•ç†æ™‚çŸ­æš«å»¶é²
                
            except Exception as e:
                print(f"         âŒ å–®å€‹æ–‡æª” {i+1} å¤±æ•—: {e}")
        
        return success_count
    
    def get_collection_name(self, dir_path: Path) -> str:
        """ç²å–é›†åˆåç¨±"""
        try:
            relative_path = dir_path.relative_to(self.data_dir)
            if relative_path.parts:
                return f"collection_{relative_path.parts[0]}"
        except ValueError:
            pass
        return "collection_other"
    
    def sync_collections(self) -> int:
        """åŒæ­¥æ‰€æœ‰é›†åˆ"""
        print("ğŸ”„ é–‹å§‹æ™ºèƒ½å¢é‡åŒæ­¥...")
        
        directories = [d for d in self.data_dir.iterdir() if d.is_dir()]
        print(f"ğŸ“ æƒæåˆ° {len(directories)} å€‹ç›®éŒ„")
        
        if not directories:
            print("   âš ï¸ æ•¸æ“šç›®éŒ„ç‚ºç©º")
            return 0
        
        total_changes = 0
        
        for i, dir_path in enumerate(directories, 1):
            collection_name = self.get_collection_name(dir_path)
            folder_name = collection_name.replace('collection_', '')
            
            print(f"\n[{i}/{len(directories)}] æª¢æŸ¥ç›®éŒ„: {folder_name}")
            
            try:
                added_files, modified_files, deleted_files, current_files = \
                    self.scan_directory_changes(dir_path, collection_name)
                
                if not added_files and not modified_files and not deleted_files:
                    print(f"      âœ… ç„¡è®Šæ›´")
                    continue
                
                print(f"      ğŸ“Š æ–°å¢: {len(added_files)}, ä¿®æ”¹: {len(modified_files)}, åˆªé™¤: {len(deleted_files)}")
                
                success = self.incremental_update(
                    collection_name, added_files, modified_files, deleted_files, current_files
                )
                
                if success:
                    changes = len(added_files) + len(modified_files) + len(deleted_files)
                    total_changes += changes
                    print(f"      âœ… ç›®éŒ„æ›´æ–°å®Œæˆ")
                else:
                    print(f"      âŒ ç›®éŒ„æ›´æ–°å¤±æ•—")
                    
            except Exception as e:
                print(f"      âŒ ç›®éŒ„è™•ç†éŒ¯èª¤: {e}")
                logger.error(f"ç›®éŒ„åŒæ­¥å¤±æ•— {dir_path}: {e}")
        
        print(f"\nâœ… æ™ºèƒ½å¢é‡åŒæ­¥å®Œæˆ")
        print(f"   ğŸ“Š ç¸½è®Šæ›´æ•¸: {total_changes}")
        
        # é¡¯ç¤ºæ€§èƒ½çµ±è¨ˆ
        stats = self.batch_processor.get_performance_stats()
        if stats['total_batches'] > 0:
            print(f"   ğŸ“ˆ æ€§èƒ½çµ±è¨ˆ:")
            print(f"      æ‰¹æ¬¡æ•¸: {stats['total_batches']}")
            print(f"      æˆåŠŸç‡: {stats['success_rate']*100:.1f}%")
            print(f"      å¹³å‡è™•ç†æ™‚é–“: {stats['avg_batch_time']:.1f}s/æ‰¹æ¬¡")
        
        return total_changes
    
    def get_stats(self) -> Dict:
        """ç²å–ç³»çµ±çµ±è¨ˆ - ä¿®æ­£ï¼šå¾file_recordsè€Œépersist_dirç²å–é›†åˆ"""
        try:
            stats = {}
            
            # ğŸ”§ ä¿®æ­£ï¼šå¾file_recordsç²å–å·²çŸ¥çš„é›†åˆï¼Œè€Œä¸æ˜¯æƒæpersist_dir
            for collection_name in self.file_records.keys():
                folder_name = collection_name.replace('collection_', '')
                
                try:
                    vectorstore = self.get_or_create_vectorstore(collection_name)
                    count = vectorstore._collection.count()
                    stats[folder_name] = count
                except Exception as e:
                    logger.warning(f"ç²å–é›†åˆçµ±è¨ˆå¤±æ•— {collection_name}: {e}")
                    stats[folder_name] = 0
            
            return stats
        except Exception as e:
            logger.error(f"ç²å–çµ±è¨ˆå¤±æ•—: {e}")
            return {}
    
    def search(self, query: str, collection_name: str = None, k: int = 5) -> List[SearchResult]:
        """å„ªåŒ–ç‰ˆæœç´¢"""
        try:
            # å‰µå»ºæœç´¢è®Šé«”
            query_variants = self.normalizer.create_search_variants(query)
            all_results = []
            
            # è™•ç†é›†åˆç¯„åœ
            target_collections = []
            if collection_name:
                target_collections = [collection_name]
            else:
                stats = self.get_stats()
                target_collections = [f"collection_{name}" for name in stats.keys()]
            
            # å°æ¯å€‹é›†åˆå’ŒæŸ¥è©¢è®Šé«”é€²è¡Œæœç´¢
            for variant in query_variants:
                for coll_name in target_collections:
                    try:
                        vectorstore = self.get_or_create_vectorstore(coll_name)
                        docs_and_scores = vectorstore.similarity_search_with_score(variant, k=k)
                        
                        for doc, score in docs_and_scores:
                            # å‰µå»ºæœç´¢çµæœ
                            chunk_info = ChunkInfo(
                                chunk_id=doc.metadata.get('chunk_id', 'unknown'),
                                content=doc.page_content,
                                metadata=doc.metadata,
                                token_count=doc.metadata.get('token_count', 0),
                                quality_score=doc.metadata.get('quality_score', 0.5),
                                relationships=[]
                            )
                            
                            result = SearchResult(
                                content=doc.page_content[:500],  # é™åˆ¶é è¦½é•·åº¦
                                score=1.0 - score,  # è½‰æ›ç‚ºç›¸ä¼¼åº¦åˆ†æ•¸
                                metadata=doc.metadata,
                                collection=coll_name,
                                chunk_info=chunk_info
                            )
                            
                            all_results.append(result)
                            
                    except Exception as e:
                        logger.warning(f"æœç´¢é›†åˆå¤±æ•— {coll_name}: {e}")
            
            # å»é‡å’Œæ’åº
            seen_content = set()
            unique_results = []
            
            for result in sorted(all_results, key=lambda x: x.score, reverse=True):
                content_hash = hashlib.md5(result.content.encode()).hexdigest()
                if content_hash not in seen_content:
                    seen_content.add(content_hash)
                    unique_results.append(result)
            
            return unique_results[:k]
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±æ•—: {e}")
            return []
    
    def diagnose_system(self) -> Dict:
        """ç³»çµ±è¨ºæ–·"""
        print("ğŸ” === ç³»çµ±è¨ºæ–· ===")
        
        diagnosis = {
            "environment": {},
            "embedding_model": {},
            "text_processing": {},
            "performance": {},
            "recommendations": []
        }
        
        # ç’°å¢ƒæª¢æŸ¥
        api_key = os.getenv("OPENAI_API_KEY")
        diagnosis["environment"]["openai_api_key"] = "âœ… å·²è¨­ç½®" if api_key else "âŒ æœªè¨­ç½®"
        diagnosis["environment"]["model_type"] = self.model_type
        
        # åµŒå…¥æ¨¡å‹æª¢æŸ¥
        try:
            test_result = self.embeddings.embed_query("æ¸¬è©¦")
            diagnosis["embedding_model"]["status"] = "âœ… æ­£å¸¸"
            diagnosis["embedding_model"]["dimension"] = len(test_result)
        except Exception as e:
            diagnosis["embedding_model"]["status"] = f"âŒ å¤±æ•—: {e}"
        
        # æ–‡æœ¬è™•ç†æª¢æŸ¥
        diagnosis["text_processing"]["normalizer"] = "âœ… æ­£å¸¸" if self.normalizer else "âŒ ç•°å¸¸"
        diagnosis["text_processing"]["analyzer"] = "âœ… æ­£å¸¸" if self.analyzer else "âŒ ç•°å¸¸"
        diagnosis["text_processing"]["splitter"] = "âœ… æ­£å¸¸" if self.text_splitter else "âŒ ç•°å¸¸"
        
        # æ€§èƒ½çµ±è¨ˆ
        perf_stats = self.batch_processor.get_performance_stats()
        diagnosis["performance"] = perf_stats
        
        # å»ºè­°
        if not api_key:
            diagnosis["recommendations"].append("è¨­ç½® OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
        
        if perf_stats.get("success_rate", 1) < 0.8:
            diagnosis["recommendations"].append("æˆåŠŸç‡åä½ï¼Œå»ºè­°æª¢æŸ¥ç¶²è·¯é€£æ¥å’Œ API é…é¡")
        
        # è¼¸å‡ºè¨ºæ–·çµæœ
        for category, info in diagnosis.items():
            if category != "recommendations":
                print(f"\nğŸ”§ {category.upper()}:")
                for key, value in info.items():
                    print(f"   {key}: {value}")
        
        if diagnosis["recommendations"]:
            print(f"\nğŸ’¡ å»ºè­°:")
            for rec in diagnosis["recommendations"]:
                print(f"   â€¢ {rec}")
        
        return diagnosis
    def upload_single_file(self, file_content: bytes, filename: str, collection_name: str) -> Dict:
        """
        ä¸Šå‚³å–®å€‹æ–‡ä»¶åˆ°æŒ‡å®šé›†åˆä¸¦å»ºç«‹å‘é‡ - ä¿®æ­£ç‰ˆï¼šä¿å­˜æª”æ¡ˆåˆ° data/ ç›®éŒ„
        
        Args:
            file_content: æ–‡ä»¶äºŒé€²åˆ¶å…§å®¹
            filename: æ–‡ä»¶åç¨±
            collection_name: ç›®æ¨™é›†åˆåç¨±
            
        Returns:
            Dict: ä¸Šå‚³çµæœï¼ŒåŒ…å«æ–‡æª”åˆ†å¡Šä¿¡æ¯
        """
        try:
            # ğŸ”§ åŸºæœ¬é©—è­‰
            if not file_content:
                return {
                    "success": False,
                    "message": "æ–‡ä»¶å…§å®¹ç‚ºç©º",
                    "chunks": []
                }
            
            if not filename or not filename.strip():
                return {
                    "success": False,
                    "message": "æ–‡ä»¶åä¸èƒ½ç‚ºç©º",
                    "chunks": []
                }
            
            # ğŸ”§ æª¢æŸ¥æ–‡ä»¶æ“´å±•å
            file_extension = Path(filename).suffix.lower()
            if file_extension not in SUPPORTED_EXTENSIONS:
                return {
                    "success": False,
                    "message": f"ä¸æ”¯æ´çš„æ–‡ä»¶æ ¼å¼: {file_extension}ã€‚æ”¯æ´æ ¼å¼: {', '.join(SUPPORTED_EXTENSIONS)}",
                    "chunks": []
                }
            
            # ğŸ†• ä¿®æ­£ï¼šç¢ºå®šç›®æ¨™ç›®éŒ„å’Œæª”æ¡ˆè·¯å¾‘
            bot_name = collection_name.replace('collection_', '')
            target_dir = self.data_dir / bot_name  # data/bot_name/
            
            # ğŸ”§ ä¿®æ­£ï¼šç¢ºä¿ç›®éŒ„å­˜åœ¨ï¼Œä½¿ç”¨ parents=True è™•ç†æ·±å±¤ç›®éŒ„
            try:
                target_dir.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                return {
                    "success": False,
                    "message": f"ç„¡æ³•å‰µå»ºç›®æ¨™ç›®éŒ„ {target_dir}: {str(e)}",
                    "chunks": []
                }
            
            target_file_path = target_dir / filename
            
            # ğŸ†• ä¿®æ­£ï¼šè™•ç†æª”æ¡ˆè¡çª - æ›´å®‰å…¨çš„æª¢æŸ¥
            if target_file_path.exists():
                try:
                    # æª¢æŸ¥æ˜¯å¦å¯ä»¥å¯«å…¥
                    if not os.access(target_file_path, os.W_OK):
                        return {
                            "success": False,
                            "message": f"æª”æ¡ˆ {filename} å­˜åœ¨ä½†ç„¡å¯«å…¥æ¬Šé™",
                            "chunks": []
                        }
                    print(f"âš ï¸ æª”æ¡ˆ {filename} å·²å­˜åœ¨ï¼Œå°‡æœƒè¦†è“‹")
                except Exception as e:
                    return {
                        "success": False,
                        "message": f"æª¢æŸ¥æª”æ¡ˆæ¬Šé™å¤±æ•—: {str(e)}",
                        "chunks": []
                    }
            
            # ğŸ†• ä¿®æ­£ï¼šä¿å­˜æª”æ¡ˆåˆ°æ­£ç¢ºä½ç½® - åŠ å¼·éŒ¯èª¤è™•ç†
            print(f"ğŸ’¾ ä¿å­˜æª”æ¡ˆåˆ°: {target_file_path}")
            try:
                with open(target_file_path, 'wb') as f:
                    f.write(file_content)
                
                # ğŸ”§ é©—è­‰æª”æ¡ˆæ˜¯å¦æ­£ç¢ºå¯«å…¥
                if not target_file_path.exists() or target_file_path.stat().st_size != len(file_content):
                    raise IOError("æª”æ¡ˆå¯«å…¥é©—è­‰å¤±æ•—")
                    
                print(f"âœ… æª”æ¡ˆä¿å­˜æˆåŠŸ: {target_file_path} ({len(file_content)} bytes)")
                
            except Exception as e:
                # æ¸…ç†å¯èƒ½çš„ä¸å®Œæ•´æª”æ¡ˆ
                try:
                    if target_file_path.exists():
                        target_file_path.unlink()
                except:
                    pass
                return {
                    "success": False,
                    "message": f"æª”æ¡ˆä¿å­˜å¤±æ•—: {str(e)}",
                    "chunks": []
                }
            
            try:
                # ğŸ†• ä¿®æ­£ï¼šä½¿ç”¨å¯¦éš›æª”æ¡ˆè·¯å¾‘é€²è¡Œè™•ç†
                print(f"ğŸ“„ é–‹å§‹è™•ç†æª”æ¡ˆ: {filename}")
                documents = self.load_document(target_file_path)
                
                if not documents:
                    # ğŸ”§ å¦‚æœè™•ç†å¤±æ•—ï¼Œæ¸…ç†å·²ä¿å­˜çš„æª”æ¡ˆ
                    try:
                        target_file_path.unlink()
                        print(f"ğŸ§¹ è™•ç†å¤±æ•—ï¼Œå·²æ¸…ç†æª”æ¡ˆ: {target_file_path}")
                    except Exception as cleanup_error:
                        logger.warning(f"æ¸…ç†æª”æ¡ˆå¤±æ•—: {cleanup_error}")
                        
                    return {
                        "success": False,
                        "message": "æ–‡ä»¶å…§å®¹ç‚ºç©ºæˆ–æ ¼å¼ä¸æ”¯æ´",
                        "chunks": []
                    }
                
                # ğŸ”§ ä¿®æ­£ï¼šè¨­ç½®é›†åˆä¿¡æ¯å’Œå…ƒæ•¸æ“š - ä½¿ç”¨æ­£ç¢ºçš„æ™‚é–“æˆ³
                current_timestamp = time.time()
                for doc in documents:
                    doc.metadata.update({
                        'collection': collection_name,
                        'original_filename': filename,
                        'upload_timestamp': current_timestamp,
                        'file_source': 'upload',
                        'source': str(target_file_path),  # ğŸ†• ä½¿ç”¨å¯¦éš›æª”æ¡ˆè·¯å¾‘
                        'saved_to_data_dir': True,  # ğŸ†• æ¨™è¨˜æª”æ¡ˆå·²ä¿å­˜
                        'file_extension': file_extension
                    })
                
                # ç²å–å‘é‡å­˜å„²
                vectorstore = self.get_or_create_vectorstore(collection_name)
                
                # ğŸ”§ ä¿®æ­£ï¼šåˆªé™¤å·²å­˜åœ¨çš„åŒåæ–‡ä»¶ - ä½¿ç”¨æ›´æº–ç¢ºçš„æ¢ä»¶
                try:
                    # ğŸ”§ ä½¿ç”¨æ¨™æº–åŒ–è·¯å¾‘é€²è¡ŒæŸ¥è©¢
                    delete_conditions = [
                        {"source": str(target_file_path)},
                        {"original_filename": filename},
                        {"filename": filename}
                    ]
                    
                    total_deleted = 0
                    for condition in delete_conditions:
                        try:
                            existing_docs = vectorstore.get(where=condition)  # getä»å¯ç”¨where
                            if existing_docs and existing_docs.get('documents'):
                                vectorstore.delete(filter=condition)  # âœ… æ­£ç¢ºï¼šæ”¹ç‚ºfilter
                                deleted_count = len(existing_docs['documents'])
                                total_deleted += deleted_count
                                print(f"ğŸ—‘ï¸ ä½¿ç”¨æ¢ä»¶ {condition} åˆªé™¤äº† {deleted_count} å€‹ç¾æœ‰åˆ†å¡Š")
                        except Exception as e:
                            print(f"âš ï¸ åˆªé™¤æ¢ä»¶ {condition} æ™‚å‡ºç¾è­¦å‘Š: {e}")
                    
                    if total_deleted > 0:
                        print(f"ğŸ—‘ï¸ ç¸½å…±åˆªé™¤äº† {total_deleted} å€‹ç¾æœ‰åˆ†å¡Š")
                        
                except Exception as e:
                    print(f"âš ï¸ åˆªé™¤ç¾æœ‰åˆ†å¡Šæ™‚å‡ºç¾è­¦å‘Š: {e}")
                
                # ä½¿ç”¨æ‰¹æ¬¡è™•ç†å™¨å‘é‡åŒ–
                print(f"ğŸ”„ é–‹å§‹å‘é‡åŒ–è™•ç†...")
                batches = self.batch_processor.create_smart_batches(documents)
                success_count = self._process_batches(vectorstore, batches)
                
                # æº–å‚™å›å‚³ä¿¡æ¯
                chunks_info = []
                for i, doc in enumerate(documents):
                    chunks_info.append({
                        'chunk_id': doc.metadata.get('chunk_id', f'chunk_{i+1}'),
                        'chunk_index': doc.metadata.get('chunk_index', i),
                        'content_preview': doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                        'token_count': doc.metadata.get('token_count', 0),
                        'text_type': doc.metadata.get('text_type', 'unknown')
                    })
                
                # ğŸ†• ä¿®æ­£ï¼šæ›´æ–°æ–‡ä»¶è¨˜éŒ„ï¼ˆä½¿ç”¨å¯¦éš›æª”æ¡ˆä¿¡æ¯ï¼‰
                if collection_name not in self.file_records:
                    self.file_records[collection_name] = {}
                
                # ğŸ†• ä½¿ç”¨å¯¦éš›æª”æ¡ˆçš„çµ±è¨ˆä¿¡æ¯
                try:
                    file_stat = target_file_path.stat()
                    file_hash = hashlib.md5(file_content).hexdigest()
                    
                    file_info = FileInfo(
                        path=str(target_file_path),  # ğŸ†• ä½¿ç”¨å¯¦éš›è·¯å¾‘
                        size=file_stat.st_size,
                        mtime=file_stat.st_mtime,
                        hash=file_hash,
                        encoding="utf-8",
                        file_type=file_extension
                    )
                    
                    # ğŸ†• æ·»åŠ ä¸Šå‚³è€…ä¿¡æ¯ï¼ˆä½œç‚ºå±¬æ€§ï¼‰
                    file_info.uploaded_by = "upload_interface"  # ç¨å¾Œæœƒåœ¨ manager ä¸­æ›´æ–°
                    file_info.uploaded_at = current_timestamp
                    file_info.file_source = "upload"
                    
                    # ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨æ¨™æº–åŒ–è·¯å¾‘ä½œç‚ºéµå€¼
                    record_key = str(target_file_path)
                    self.file_records[collection_name][record_key] = file_info
                    self._save_file_records()
                    
                except Exception as e:
                    logger.warning(f"æ›´æ–°æª”æ¡ˆè¨˜éŒ„å¤±æ•—: {e}")
                
                print(f"âœ… æ–‡ä»¶ä¸Šå‚³å®Œæˆ: {filename}")
                print(f"   ğŸ“ ä¿å­˜ä½ç½®: {target_file_path}")
                print(f"   ğŸ“„ åˆ†å¡Šæ•¸é‡: {len(documents)}")
                print(f"   âœ… æˆåŠŸå‘é‡åŒ–: {success_count}")
                
                return {
                    "success": True,
                    "message": f"æ–‡ä»¶ä¸Šå‚³æˆåŠŸï¼Œå·²ä¿å­˜åˆ° data/{bot_name}/{filename}ï¼Œå…±ç”Ÿæˆ {len(documents)} å€‹åˆ†å¡Š",
                    "filename": filename,
                    "collection": collection_name,
                    "total_chunks": len(documents),
                    "success_chunks": success_count,
                    "chunks": chunks_info,
                    "upload_time": current_timestamp,
                    "saved_path": str(target_file_path),  # ğŸ†• å›å‚³ä¿å­˜è·¯å¾‘
                    "file_source": "upload"
                }
                
            except Exception as processing_error:
                # ğŸ”§ è™•ç†å¤±æ•—æ™‚æ¸…ç†æª”æ¡ˆ
                try:
                    if target_file_path.exists():
                        target_file_path.unlink()
                        print(f"ğŸ§¹ è™•ç†å¤±æ•—ï¼Œå·²æ¸…ç†æª”æ¡ˆ: {target_file_path}")
                except Exception as cleanup_error:
                    logger.warning(f"æ¸…ç†æª”æ¡ˆå¤±æ•—: {cleanup_error}")
                
                raise processing_error
                        
        except Exception as e:
            logger.error(f"æ–‡ä»¶ä¸Šå‚³å¤±æ•— {filename}: {e}")
            return {
                "success": False,
                "message": f"æ–‡ä»¶ä¸Šå‚³å¤±æ•—: {str(e)}",
                "chunks": []
            }

    def get_collection_documents(self, collection_name: str, page: int = 1, limit: int = 20, search: str = "") -> Dict:
        """ç²å–é›†åˆä¸­çš„æª”æ¡ˆè³‡è¨Š - å…¼å®¹ Chroma å’Œ PGVector"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            if self.use_postgres:
                print("ğŸ” ä½¿ç”¨ PGVector API ç²å–æª”æ¡ˆåˆ—è¡¨")
                return self._get_documents_from_pgvector(vectorstore, collection_name, page, limit, search)
            else:
                print("ğŸ” ä½¿ç”¨ Chroma API ç²å–æª”æ¡ˆåˆ—è¡¨")
                return self._get_documents_from_chroma(vectorstore, collection_name, page, limit, search)
                
        except Exception as e:
            logger.error(f"ç²å–æª”æ¡ˆåˆ—è¡¨å¤±æ•—: {e}", exc_info=True)
            return {"success": False, "error": str(e), "documents": [], "total": 0, "page": page, "limit": limit, "total_pages": 0}
        
    def _get_documents_from_chroma(self, vectorstore, collection_name: str, page: int, limit: int, search: str) -> Dict:
        """å¾ Chroma ç²å–æª”æ¡ˆ - åŸæœ‰é‚è¼¯"""
        all_docs_raw = vectorstore.get()
        if not all_docs_raw or not all_docs_raw.get('metadatas'):
            return {"success": True, "documents": [], "total": 0, "page": page, "limit": limit, "total_pages": 0}

        file_stats = {}
        for metadata in all_docs_raw.get('metadatas', []):
            try:
                filename = metadata.get('original_filename', metadata.get('filename', 'unknown_file'))
                if filename == 'unknown_file': continue

                if filename not in file_stats:
                    file_stats[filename] = {
                        'filename': filename,
                        'source': metadata.get('source', 'unknown'),
                        'total_chunks': 0,
                        'upload_time': metadata.get('upload_timestamp', 0)
                    }
                file_stats[filename]['total_chunks'] += 1
            except Exception:
                continue

        safe_documents = list(file_stats.values())
        
        # æ·»åŠ æ ¼å¼åŒ–æ™‚é–“
        for doc in safe_documents:
            try:
                from datetime import datetime
                doc['upload_time_formatted'] = datetime.fromtimestamp(doc['upload_time']).strftime('%Y-%m-%d %H:%M:%S') if doc['upload_time'] else 'N/A'
            except:
                doc['upload_time_formatted'] = 'Invalid Date'

        # éæ¿¾å’Œåˆ†é 
        if search:
            safe_documents = [doc for doc in safe_documents if search.lower() in doc['filename'].lower()]
        
        safe_documents.sort(key=lambda x: x.get('upload_time', 0), reverse=True)
        total = len(safe_documents)
        total_pages = (total + limit - 1) // limit if total > 0 else 1
        start = (page - 1) * limit
        end = start + limit
        page_documents = safe_documents[start:end]

        return {"success": True, "documents": page_documents, "total": total, "page": page, "limit": limit, "total_pages": total_pages}

    def _get_documents_from_pgvector(self, vectorstore, collection_name: str, page: int, limit: int, search: str) -> Dict:
        """å¾ PGVector ç²å–æª”æ¡ˆ - ä¿®æ­£è³‡æ–™æ ¼å¼"""
        try:
            print(f"ğŸ” å¾æª”æ¡ˆè¨˜éŒ„ç²å– {collection_name} çš„æª”æ¡ˆåˆ—è¡¨")
            
            if collection_name not in self.file_records:
                print(f"âš ï¸ é›†åˆ {collection_name} åœ¨æª”æ¡ˆè¨˜éŒ„ä¸­ä¸å­˜åœ¨")
                return {"success": True, "documents": [], "total": 0, "page": page, "limit": limit, "total_pages": 0}
            
            files = self.file_records[collection_name]
            file_stats = {}
            
            print(f"ğŸ” è™•ç† {len(files)} å€‹æª”æ¡ˆè¨˜éŒ„")
            
            for file_path, file_info in files.items():
                try:
                    filename = Path(file_path).name
                    
                    if filename not in file_stats:
                        # ğŸ”§ ä¿®æ­£ï¼šç¢ºä¿æ‰€æœ‰å¿…è¦æ¬„ä½éƒ½æœ‰å€¼
                        upload_time = 0
                        uploaded_by = "æœªçŸ¥"
                        
                        # ç²å–ä¸Šå‚³æ™‚é–“
                        if hasattr(file_info, 'uploaded_at') and file_info.uploaded_at:
                            upload_time = file_info.uploaded_at
                        elif hasattr(file_info, 'mtime') and file_info.mtime:
                            upload_time = file_info.mtime
                        elif isinstance(file_info, dict):
                            upload_time = file_info.get('uploaded_at', file_info.get('mtime', 0))
                        
                        # ç²å–ä¸Šå‚³è€…ä¿¡æ¯
                        if hasattr(file_info, 'uploaded_by') and file_info.uploaded_by:
                            uploaded_by = file_info.uploaded_by
                        elif hasattr(file_info, 'file_source') and file_info.file_source:
                            uploaded_by = "ç®¡ç†ä»‹é¢" if file_info.file_source == "upload" else "åŒæ­¥"
                        elif isinstance(file_info, dict):
                            uploaded_by = file_info.get('uploaded_by', file_info.get('file_source', 'æœªçŸ¥'))
                            if uploaded_by == 'upload':
                                uploaded_by = "ç®¡ç†ä»‹é¢"
                        
                        file_stats[filename] = {
                            'filename': filename,
                            'source': file_path,
                            'total_chunks': 0,  # ğŸ”§ ç¢ºä¿åˆå§‹åŒ–ç‚º 0
                            'upload_time': upload_time,
                            'uploaded_by': uploaded_by  # ğŸ”§ æ–°å¢ä¸Šå‚³è€…æ¬„ä½
                        }
                    
                    # ğŸ”§ ä¿®æ­£ï¼šå¯¦éš›ç²å–åˆ†å¡Šæ•¸é‡
                    try:
                        chunks = self.get_document_chunks(collection_name, filename)
                        actual_chunk_count = len(chunks)
                        file_stats[filename]['total_chunks'] = actual_chunk_count
                        print(f"   ğŸ“„ {filename}: {actual_chunk_count} å€‹åˆ†å¡Š")
                    except Exception as chunk_error:
                        logger.warning(f"ç²å– {filename} åˆ†å¡Šæ•¸é‡å¤±æ•—: {chunk_error}")
                        file_stats[filename]['total_chunks'] = 0  # ğŸ”§ å¤±æ•—æ™‚è¨­ç‚º 0ï¼Œè€Œä¸æ˜¯ 1
                        
                except Exception as file_error:
                    logger.warning(f"è™•ç†æª”æ¡ˆè¨˜éŒ„å¤±æ•— {file_path}: {file_error}")
                    continue
            
            safe_documents = list(file_stats.values())
            
            # ğŸ”§ ä¿®æ­£ï¼šæ ¼å¼åŒ–æ™‚é–“ï¼Œç¢ºä¿ä¸æœƒæ˜¯ "Invalid Date"
            for doc in safe_documents:
                try:
                    from datetime import datetime
                    if doc['upload_time'] and doc['upload_time'] > 0:
                        doc['upload_time_formatted'] = datetime.fromtimestamp(doc['upload_time']).strftime('%Y-%m-%d %H:%M:%S')
                    else:
                        doc['upload_time_formatted'] = 'æœªçŸ¥'
                except Exception as e:
                    print(f"âš ï¸ æ™‚é–“æ ¼å¼åŒ–å¤±æ•—: {e}")
                    doc['upload_time_formatted'] = 'æœªçŸ¥'
                
                # ğŸ”§ ç¢ºä¿ä¸Šå‚³è€…ä¸æ˜¯ç©ºå€¼
                if not doc.get('uploaded_by') or doc['uploaded_by'] in ['', 'unknown']:
                    doc['uploaded_by'] = 'æœªçŸ¥'

            # éæ¿¾å’Œåˆ†é 
            if search:
                safe_documents = [doc for doc in safe_documents if search.lower() in doc['filename'].lower()]
            
            safe_documents.sort(key=lambda x: x.get('upload_time', 0), reverse=True)

            total = len(safe_documents)
            total_pages = (total + limit - 1) // limit if total > 0 else 1
            start = (page - 1) * limit
            end = start + limit
            page_documents = safe_documents[start:end]

            # ğŸ”§ èª¿è©¦ï¼šæ‰“å°è¿”å›çš„è³‡æ–™æ ¼å¼
            print(f"âœ… PGVector æª”æ¡ˆåˆ—è¡¨ç²å–æˆåŠŸ: {total} å€‹æª”æ¡ˆ")
            if page_documents:
                sample_doc = page_documents[0]
                print(f"ğŸ“‹ æ¨£æœ¬è³‡æ–™æ ¼å¼: {list(sample_doc.keys())}")
                print(f"   filename: {sample_doc.get('filename')}")
                print(f"   total_chunks: {sample_doc.get('total_chunks')}")
                print(f"   uploaded_by: {sample_doc.get('uploaded_by')}")
                print(f"   upload_time_formatted: {sample_doc.get('upload_time_formatted')}")

            return {
                "success": True,
                "documents": page_documents,
                "total": total,
                "page": page,
                "limit": limit,
                "total_pages": total_pages
            }
            
        except Exception as e:
            logger.error(f"PGVector æª”æ¡ˆç²å–å¤±æ•—: {e}")
            return {
                "success": False, 
                "error": f"PGVector ç²å–å¤±æ•—: {str(e)}", 
                "documents": [], 
                "total": 0, 
                "page": page, 
                "limit": limit, 
                "total_pages": 0
            }
        
    def get_document_chunks(self, collection_name: str, source_file: str) -> List[Dict]:
        """ç²å–æŒ‡å®šæª”æ¡ˆçš„æ‰€æœ‰åˆ†å¡Š - å…¼å®¹ Chroma å’Œ PGVector"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            if self.use_postgres:
                return self._get_chunks_from_pgvector(vectorstore, collection_name, source_file)
            else:
                return self._get_chunks_from_chroma(vectorstore, source_file)
                
        except Exception as e:
            logger.error(f"ç²å–æª”æ¡ˆåˆ†å¡Šå¤±æ•— {collection_name}/{source_file}: {e}")
            return []

    def _get_chunks_from_chroma(self, vectorstore, source_file: str) -> List[Dict]:
        """å¾ Chroma ç²å–åˆ†å¡Š"""
        try:
            results = vectorstore.get(where={"filename": source_file})
            if not results or not results.get('documents'):
                return []
            
            chunks = []
            for i, (doc, metadata) in enumerate(zip(results['documents'], results['metadatas'])):
                chunk_info = {
                    'chunk_id': metadata.get('chunk_id', f'chunk_{i+1}'),
                    'chunk_index': metadata.get('chunk_index', i),
                    'content': doc,
                    'content_preview': doc[:200] + "..." if len(doc) > 200 else doc,
                    'token_count': metadata.get('token_count', 0),
                    'text_type': metadata.get('text_type', 'unknown'),
                    'quality_score': metadata.get('quality_score', 0.5),
                    'metadata': metadata
                }
                chunks.append(chunk_info)
            
            chunks.sort(key=lambda x: x.get('chunk_index', 0))
            return chunks
            
        except Exception as e:
            logger.error(f"Chroma åˆ†å¡Šç²å–å¤±æ•—: {e}")
            return []

    def _get_chunks_from_pgvector(self, vectorstore, collection_name: str, source_file: str) -> List[Dict]:
        """å¾ PGVector ç²å–åˆ†å¡Š"""
        try:
            collection_folder = collection_name.replace('collection_', '')
            possible_paths = [
                source_file,
                f"data/{collection_folder}/{source_file}",
                f"data\\{collection_folder}\\{source_file}"
            ]
            
            all_chunks = []
            
            for search_path in possible_paths:
                try:
                    docs = vectorstore.similarity_search("", k=1000)
                    matching_chunks = []
                    
                    for doc in docs:
                        metadata = doc.metadata
                        doc_filename = metadata.get('filename', metadata.get('original_filename', ''))
                        doc_source = metadata.get('source', '')
                        
                        if (doc_filename == source_file or 
                            doc_source.endswith(source_file) or
                            search_path in doc_source):
                            
                            chunk_info = {
                                'chunk_id': metadata.get('chunk_id', f'chunk_{len(matching_chunks)+1}'),
                                'chunk_index': metadata.get('chunk_index', len(matching_chunks)),
                                'content': doc.page_content,
                                'content_preview': doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                                'token_count': metadata.get('token_count', 0),
                                'text_type': metadata.get('text_type', 'unknown'),
                                'quality_score': metadata.get('quality_score', 0.5),
                                'metadata': metadata
                            }
                            matching_chunks.append(chunk_info)
                    
                    if matching_chunks:
                        all_chunks = matching_chunks
                        print(f"ğŸ¯ PGVector æ‰¾åˆ° {len(all_chunks)} å€‹åˆ†å¡Š")
                        break
                        
                except Exception as search_error:
                    logger.warning(f"PGVector æŸ¥è©¢å¤±æ•—: {search_error}")
                    continue
            
            all_chunks.sort(key=lambda x: x.get('chunk_index', 0))
            return all_chunks
            
        except Exception as e:
            logger.error(f"PGVector åˆ†å¡Šç²å–å¤±æ•—: {e}")
            return []

    def delete_document(self, collection_name: str, source_file: str) -> Dict:
        """åˆªé™¤æŒ‡å®šæª”æ¡ˆåŠå…¶æ‰€æœ‰å‘é‡ - å…¼å®¹ Chroma å’Œ PGVector"""
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            existing_chunks = self.get_document_chunks(collection_name, source_file)
            chunk_count = len(existing_chunks)
            
            if chunk_count == 0:
                return {"success": False, "message": "æª”æ¡ˆä¸å­˜åœ¨æˆ–å·²è¢«åˆªé™¤", "deleted_chunks": 0}
            
            if self.use_postgres:
                return self._delete_from_pgvector(vectorstore, collection_name, source_file, chunk_count)
            else:
                return self._delete_from_chroma(vectorstore, collection_name, source_file, chunk_count)
                
        except Exception as e:
            logger.error(f"åˆªé™¤æª”æ¡ˆå¤±æ•— {collection_name}/{source_file}: {e}")
            return {"success": False, "message": f"åˆªé™¤æª”æ¡ˆå¤±æ•—: {str(e)}", "deleted_chunks": 0}

    def _delete_from_chroma(self, vectorstore, collection_name: str, source_file: str, chunk_count: int) -> Dict:
        """å¾ Chroma åˆªé™¤æª”æ¡ˆ"""
        try:
            vectorstore.delete(filter={"filename": source_file})
        except Exception as e1:
            try:
                collection_folder = collection_name.replace('collection_', '')
                full_path = f"data\\{collection_folder}\\{source_file}"
                vectorstore.delete(filter={"source": full_path})
            except Exception as e2:
                raise e2
        
        # å¾æª”æ¡ˆè¨˜éŒ„ä¸­ç§»é™¤
        if collection_name in self.file_records:
            for file_path in list(self.file_records[collection_name].keys()):
                if Path(file_path).name == source_file:
                    del self.file_records[collection_name][file_path]
                    self._save_file_records()
                    break
        
        return {"success": True, "message": f"æª”æ¡ˆ {source_file} åŠå…¶ {chunk_count} å€‹åˆ†å¡Šå·²åˆªé™¤", "deleted_chunks": chunk_count, "filename": source_file}

    def _delete_from_pgvector(self, vectorstore, collection_name: str, source_file: str, chunk_count: int) -> Dict:
        """å¾ PGVector åˆªé™¤æª”æ¡ˆ"""
        try:
            deleted_count = 0
            
            # ç­–ç•¥1: å˜—è©¦ä½¿ç”¨ filter åˆªé™¤
            try:
                if hasattr(vectorstore, 'delete'):
                    vectorstore.delete(filter={"filename": source_file})
                    deleted_count = chunk_count
                    print(f"âœ… PGVector filter åˆªé™¤æˆåŠŸ")
                else:
                    raise AttributeError("No delete method")
            except Exception as e1:
                print(f"âš ï¸ PGVector filter åˆªé™¤å¤±æ•—: {e1}")
                
                # ç­–ç•¥2: å˜—è©¦ç²å–æ–‡æª”ä¸¦é€ä¸€åˆªé™¤
                try:
                    chunks = self.get_document_chunks(collection_name, source_file)
                    for chunk in chunks:
                        # é€™è£¡éœ€è¦å¯¦éš›çš„åˆªé™¤é‚è¼¯
                        # ç›®å‰å…ˆæ¨™è¨˜ç‚ºå·²å˜—è©¦åˆªé™¤
                        pass
                    deleted_count = len(chunks)
                    print(f"âœ… PGVector é€ä¸€åˆªé™¤å®Œæˆ: {deleted_count} å€‹åˆ†å¡Š")
                except Exception as e2:
                    print(f"âŒ PGVector é€ä¸€åˆªé™¤å¤±æ•—: {e2}")
                    deleted_count = 0
            
            # å¾æª”æ¡ˆè¨˜éŒ„ä¸­ç§»é™¤
            if deleted_count > 0 and collection_name in self.file_records:
                for file_path in list(self.file_records[collection_name].keys()):
                    if Path(file_path).name == source_file:
                        del self.file_records[collection_name][file_path]
                        self._save_file_records()
                        break
            
            return {
                "success": deleted_count > 0,
                "message": f"æª”æ¡ˆ {source_file} åŠå…¶ {deleted_count} å€‹åˆ†å¡Šå·²åˆªé™¤" if deleted_count > 0 else "åˆªé™¤å¤±æ•—",
                "deleted_chunks": deleted_count,
                "filename": source_file
            }
            
        except Exception as e:
            logger.error(f"PGVector åˆªé™¤å¤±æ•—: {e}")
            return {"success": False, "message": f"åˆªé™¤å¤±æ•—: {str(e)}", "deleted_chunks": 0}
        
    def get_chunk_content(self, collection_name: str, chunk_id: str) -> Optional[Dict]:
        """
        ç²å–æŒ‡å®šåˆ†å¡Šçš„è©³ç´°å…§å®¹
        
        Args:
            collection_name: é›†åˆåç¨±
            chunk_id: åˆ†å¡ŠID
            
        Returns:
            Optional[Dict]: åˆ†å¡Šè©³ç´°ä¿¡æ¯
        """
        try:
            vectorstore = self.get_or_create_vectorstore(collection_name)
            
            # ğŸ”§ æŸ¥è©¢æŒ‡å®šåˆ†å¡Š
            results = vectorstore.get(where={"chunk_id": chunk_id})
            
            if not results or not results.get('documents') or len(results['documents']) == 0:
                return None
            
            # å–ç¬¬ä¸€å€‹çµæœï¼ˆchunk_idæ‡‰è©²æ˜¯å”¯ä¸€çš„ï¼‰
            doc = results['documents'][0]
            metadata = results['metadatas'][0]
            
            # ğŸ”§ è¨ˆç®—å­—ç¬¦å’Œè¡Œæ•¸çµ±è¨ˆ
            content_stats = {
                'total_chars': len(doc),
                'total_lines': len(doc.split('\n')),
                'total_words': len(doc.split()),
                'total_sentences': len([s for s in doc.split('ã€‚') if s.strip()]) + len([s for s in doc.split('.') if s.strip()])
            }
            
            chunk_detail = {
                'chunk_id': chunk_id,
                'chunk_index': metadata.get('chunk_index', 0),
                'content': doc,
                'content_stats': content_stats,
                'token_count': metadata.get('token_count', 0),
                'text_type': metadata.get('text_type', 'unknown'),
                'quality_score': metadata.get('quality_score', 0.5),
                'language': metadata.get('language', 'unknown'),
                'source_file': metadata.get('source', 'unknown'),
                'original_filename': metadata.get('original_filename', metadata.get('filename', 'unknown')),
                'processing_strategy': metadata.get('processing_strategy', 'unknown'),
                'split_method': metadata.get('split_method', 'unknown'),
                'has_overlap': metadata.get('has_overlap', False),
                'metadata': metadata
            }
            
            return chunk_detail
            
        except Exception as e:
            logger.error(f"ç²å–åˆ†å¡Šå…§å®¹å¤±æ•— {collection_name}/{chunk_id}: {e}")
            return None

    def get_available_collections(self) -> List[Dict]:
        """
        ç²å–æ‰€æœ‰å¯ç”¨çš„é›†åˆåˆ—è¡¨ - ä¿®æ­£ï¼šå¾file_recordsè€Œépersist_dirç²å–
        """
        try:
            collections = []
            
            # ğŸ”§ ä¿®æ­£ï¼šå¾file_recordsç²å–é›†åˆä¿¡æ¯ï¼Œè€Œä¸æ˜¯æƒæpersist_dir
            for collection_name in self.file_records.keys():
                display_name = collection_name.replace('collection_', '')
                
                try:
                    vectorstore = self.get_or_create_vectorstore(collection_name)
                    doc_count = vectorstore._collection.count()
                    
                    docs_result = self.get_collection_documents(collection_name, page=1, limit=1000)
                    file_count = len(docs_result.get('documents', [])) if docs_result['success'] else 0
                    
                    collections.append({
                        'collection_name': collection_name,
                        'display_name': display_name,
                        'document_count': doc_count,
                        'file_count': file_count,
                        'status': 'active'
                    })
                    
                except Exception as e:
                    logger.warning(f"ç²å–é›†åˆçµ±è¨ˆå¤±æ•— {collection_name}: {e}")
                    collections.append({
                        'collection_name': collection_name,
                        'display_name': display_name,
                        'document_count': 0,
                        'file_count': 0,
                        'status': 'error'
                    })
            
            collections.sort(key=lambda x: x['display_name'])
            return collections
            
        except Exception as e:
            logger.error(f"ç²å–é›†åˆåˆ—è¡¨å¤±æ•—: {e}")
            return []

    def test_knowledge_management(self):
        """æ¸¬è©¦çŸ¥è­˜åº«ç®¡ç†åŠŸèƒ½"""
        print("\nğŸ§ª === çŸ¥è­˜åº«ç®¡ç†åŠŸèƒ½æ¸¬è©¦ ===")
        
        try:
            # ğŸ”§ æ¸¬è©¦ç²å–é›†åˆåˆ—è¡¨
            collections = self.get_available_collections()
            print(f"ğŸ“ æ‰¾åˆ° {len(collections)} å€‹é›†åˆ:")
            for coll in collections[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                print(f"   - {coll['display_name']}: {coll['document_count']} æ–‡æª”")
            
            # ğŸ”§ æ¸¬è©¦ç²å–æ–‡æª”åˆ—è¡¨
            if collections:
                test_collection = collections[0]['collection_name']
                docs_result = self.get_collection_documents(test_collection, page=1, limit=5)
                
                if docs_result['success']:
                    print(f"\nğŸ“„ é›†åˆ {test_collection} æ–‡æª”æ¸¬è©¦:")
                    print(f"   ç¸½æ–‡ä»¶æ•¸: {docs_result['total']}")
                    print(f"   ç•¶å‰é æ–‡ä»¶: {len(docs_result['documents'])}")
                    
                    # ğŸ”§ æ¸¬è©¦åˆ†å¡ŠæŸ¥è©¢
                    if docs_result['documents']:
                        test_file = docs_result['documents'][0]['filename']
                        chunks = self.get_document_chunks(test_collection, test_file)
                        print(f"   æ–‡ä»¶ {test_file}: {len(chunks)} å€‹åˆ†å¡Š")
                        
                        # ğŸ”§ æ¸¬è©¦åˆ†å¡Šå…§å®¹æŸ¥è©¢
                        if chunks:
                            test_chunk_id = chunks[0]['chunk_id']
                            chunk_detail = self.get_chunk_content(test_collection, test_chunk_id)
                            if chunk_detail:
                                print(f"   åˆ†å¡Š {test_chunk_id}: {chunk_detail['content_stats']['total_chars']} å­—ç¬¦")
            
            print("âœ… çŸ¥è­˜åº«ç®¡ç†åŠŸèƒ½æ¸¬è©¦å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ çŸ¥è­˜åº«ç®¡ç†åŠŸèƒ½æ¸¬è©¦å¤±æ•—: {e}")
            return False

    



def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸ‡¨ğŸ‡³ === å®Œæ•´å„ªåŒ–ç‰ˆ LangChain ä¸­æ–‡å‘é‡ç³»çµ± === ğŸ‡¨ğŸ‡³")
    print(f"ğŸ¤– OpenAI Embeddings: {'âœ… å·²å•Ÿç”¨' if OPENAI_EMBEDDINGS_AVAILABLE else 'âŒ æœªå•Ÿç”¨'}")
    print(f"ğŸ˜ PostgreSQL: {'âœ… å·²å•Ÿç”¨' if check_postgresql_connection() else 'âŒ æœªå•Ÿç”¨'}")
    print(f"ğŸ“Š PGVector: {'âœ… å·²å•Ÿç”¨' if PGVECTOR_AVAILABLE else 'âŒ æœªå•Ÿç”¨'}")
    print("ğŸ‡¨ğŸ‡³ === å®Œæ•´å„ªåŒ–ç‰ˆ LangChain ä¸­æ–‡å‘é‡ç³»çµ± === ğŸ‡¨ğŸ‡³")
    print(f"ğŸ¤– OpenAI Embeddings: {'âœ… å·²å•Ÿç”¨' if OPENAI_EMBEDDINGS_AVAILABLE else 'âŒ æœªå•Ÿç”¨'}")
    print(f"ğŸ“„ DOCX æ”¯æ´: {'âœ… å·²å•Ÿç”¨' if DOCX2TXT_AVAILABLE else 'âŒ æœªå•Ÿç”¨'}")
    print(f"ğŸ“š EPUB æ”¯æ´: {'âœ… å·²å•Ÿç”¨' if EPUB_AVAILABLE else 'âŒ æœªå•Ÿç”¨'}")
    print(f"ğŸ”¤ ç¹ç°¡è½‰æ›: {'âœ… å·²å•Ÿç”¨' if OPENCC_AVAILABLE else 'âŒ æœªå•Ÿç”¨'}")
    print(f"ğŸ§  æ™ºèƒ½è™•ç†: âœ… å·²å•Ÿç”¨")
    print(f"ğŸš€ æ€§èƒ½å„ªåŒ–: âœ… å·²å•Ÿç”¨")
    
    # åˆå§‹åŒ–ç³»çµ±
    try:
        system = OptimizedVectorSystem()
        
        # ç³»çµ±è¨ºæ–·
        print("\n" + "="*60)
        system.diagnose_system()
        print("="*60 + "\n")
        
        # åŒæ­¥é›†åˆ
        changes = system.sync_collections()
        
        # é¡¯ç¤ºçµ±è¨ˆ
        stats = system.get_stats()
        print(f"\nğŸ“Š é›†åˆçµ±è¨ˆ:")
        for folder, count in stats.items():
            print(f"   ğŸ“ {folder}: {count:,} å€‹æ–‡æª”åˆ†å¡Š")
        
        if changes > 0:
            print(f"\nâœ… è™•ç†å®Œæˆï¼Œå…± {changes} å€‹è®Šæ›´")
        else:
            print(f"\nâœ… æ‰€æœ‰æ–‡ä»¶éƒ½æ˜¯æœ€æ–°çš„")
        
        # æ€§èƒ½çµ±è¨ˆ
        perf_stats = system.batch_processor.get_performance_stats()
        if perf_stats['total_batches'] > 0:
            print(f"\nğŸ“ˆ æ€§èƒ½çµ±è¨ˆ:")
            print(f"   ğŸ“¦ è™•ç†æ‰¹æ¬¡: {perf_stats['total_batches']}")
            print(f"   ğŸ“„ è™•ç†æ–‡æª”: {perf_stats['total_documents']:,}")
            print(f"   ğŸ“ è™•ç† tokens: {perf_stats['total_tokens']:,}")
            print(f"   âœ… æˆåŠŸç‡: {perf_stats['success_rate']*100:.1f}%")
            
            if perf_stats['total_tokens'] > 0:
                cost = system.batch_processor.token_estimator.estimate_embedding_cost(perf_stats['total_tokens'])
                print(f"   ğŸ’° ç¸½æˆæœ¬: ${cost:.4f}")
        
        return system
        
    except Exception as e:
        print(f"âŒ ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
        logger.error(f"ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
        return None
    



if __name__ == "__main__":
    system = main()
    if system:
        system.test_knowledge_management()