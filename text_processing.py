from core_config import *
import re
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set, Any
from collections import defaultdict

# LangChainç›¸é—œ
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

# EPUBè™•ç†ç›¸é—œ (æ¢ä»¶å°å…¥)
if EPUB_AVAILABLE:
    import ebooklib
    from ebooklib import epub
    from bs4 import BeautifulSoup

# ä¸­æ–‡è™•ç†ç›¸é—œ (æ¢ä»¶å°å…¥)
if JIEBA_AVAILABLE:
    import jieba
if OPENCC_AVAILABLE:
    import opencc

# é…ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)

class AdvancedTokenEstimator:
    """ğŸ”§ é«˜ç´š Token ä¼°ç®—å™¨"""
    
    def __init__(self):
        # ä¸åŒèªè¨€å’Œå…§å®¹é¡å‹çš„ Token ä¿‚æ•¸
        self.token_ratios = {
            "chinese": 2.5,      # ä¸­æ–‡å­—ç¬¦/token
            "english": 4.0,      # è‹±æ–‡å­—ç¬¦/token
            "mixed": 3.0,        # ä¸­è‹±æ··åˆ
            "code": 3.5,         # ç¨‹å¼ç¢¼
            "punctuation": 1.0,  # æ¨™é»ç¬¦è™Ÿ
            "numbers": 2.0       # æ•¸å­—
        }
        self.safety_margin = TOKEN_LIMITS.get("token_safety_margin", 0.15)
    
    def analyze_text_composition(self, text: str) -> Dict[str, int]:
        """åˆ†ææ–‡æœ¬çµ„æˆ"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        numbers = len(re.findall(r'\d', text))
        punctuation = len(re.findall(r'[^\w\s\u4e00-\u9fff]', text))
        other_chars = len(text) - chinese_chars - english_chars - numbers - punctuation
        
        return {
            "chinese": chinese_chars,
            "english": english_chars,
            "numbers": numbers,
            "punctuation": punctuation,
            "other": other_chars,
            "total": len(text)
        }
    
    def estimate_tokens(self, text: str, content_type: str = "mixed") -> int:
        """ç²¾ç¢ºä¼°ç®— Token æ•¸é‡"""
        if not text:
            return 0
        
        composition = self.analyze_text_composition(text)
        
        # æ ¹æ“šæ–‡æœ¬çµ„æˆè¨ˆç®— token
        estimated_tokens = (
            composition["chinese"] / self.token_ratios["chinese"] +
            composition["english"] / self.token_ratios["english"] +
            composition["numbers"] / self.token_ratios["numbers"] +
            composition["punctuation"] / self.token_ratios["punctuation"] +
            composition["other"] / self.token_ratios["mixed"]
        )
        
        # æ ¹æ“šå…§å®¹é¡å‹èª¿æ•´
        if content_type == "code":
            estimated_tokens *= 1.2  # ç¨‹å¼ç¢¼é€šå¸¸ token å¯†åº¦æ›´é«˜
        elif content_type == "academic":
            estimated_tokens *= 1.1  # å­¸è¡“æ–‡æœ¬å°ˆæ¥­è©å½™å¤š
        
        # åŠ ä¸Šå®‰å…¨é‚Šéš›
        final_estimate = int(estimated_tokens * (1 + self.safety_margin))
        
        return max(final_estimate, 1)  # è‡³å°‘1å€‹token
    
    def estimate_embedding_cost(self, total_tokens: int, model: str = "text-embedding-3-small") -> float:
        """ä¼°ç®— Embedding æˆæœ¬"""
        cost_per_1k_tokens = {
            "text-embedding-3-small": 0.00002,
            "text-embedding-3-large": 0.00013,
            "text-embedding-ada-002": 0.0001
        }
        
        rate = cost_per_1k_tokens.get(model, 0.00002)
        return (total_tokens / 1000) * rate

class ChineseTextNormalizer:
    """å„ªåŒ–ç‰ˆä¸­æ–‡æ–‡æœ¬æ¨™æº–åŒ–è™•ç†å™¨"""
    
    def __init__(self):
        self.s2t_converter = None
        self.t2s_converter = None
        
        if OPENCC_AVAILABLE:
            try:
                self.s2t_converter = opencc.OpenCC('s2t')
                self.t2s_converter = opencc.OpenCC('t2s')
                print("ğŸ”¤ ç¹ç°¡è½‰æ›å™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.warning(f"OpenCC åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # ä¸­æ–‡æ–‡æœ¬æ­£è¦åŒ–è¦å‰‡
        self.normalization_rules = [
            (r'[\u3000]+', ' '),
            (r'\r\n|\r', '\n'),                       # çµ±ä¸€æ›è¡Œ
            (r'\n{3,}', '\n\n'),                      # é™åˆ¶é€£çºŒæ›è¡Œ
            (r'[\u201C\u201D\u2018\u2019\u201E\u201A\u2033\u2032]', '"'),  # çµ±ä¸€å¼•è™Ÿ
            (r'[â€”â€”â€“âˆ¶]', '-'),                          # çµ±ä¸€ç ´æŠ˜è™Ÿ
            (r'[â€¦â‹¯]', '...'),                         # çµ±ä¸€çœç•¥è™Ÿ
        ]
    
    def normalize_text(self, text: str) -> Tuple[str, Dict]:
        """æ¨™æº–åŒ–æ–‡æœ¬ä¸¦è¿”å›è™•ç†è³‡è¨Š"""
        if not text:
            return "", {}
        
        original_length = len(text)
        processed_text = text
        
        # æ‡‰ç”¨æ­£è¦åŒ–è¦å‰‡
        for pattern, replacement in self.normalization_rules:
            processed_text = re.sub(pattern, replacement, processed_text)
        
        # ç§»é™¤é¦–å°¾ç©ºç™½
        processed_text = processed_text.strip()
        
        # æª¢æ¸¬å’Œè½‰æ›ç¹ç°¡
        variant, confidence = self.detect_chinese_variant(processed_text)
        if variant == 'simplified' and self.s2t_converter:
            try:
                processed_text = self.s2t_converter.convert(processed_text)
                variant = 'traditional_converted'
            except Exception as e:
                logger.warning(f"ç¹ç°¡è½‰æ›å¤±æ•—: {e}")
        
        processing_info = {
            'original_length': original_length,
            'processed_length': len(processed_text),
            'variant': variant,
            'confidence': confidence,
            'normalized': original_length != len(processed_text)
        }
        
        return processed_text, processing_info
    
    def detect_chinese_variant(self, text: str) -> Tuple[str, float]:
        """æª¢æ¸¬ç¹é«”æˆ–ç°¡é«”ä¸­æ–‡"""
        if not text:
            return 'unknown', 0.0
        
        simplified_chars = set('å›½å‘ä¼šå­¦ä¹ è®ºé—®é¢˜ä¸šä¸“é•¿æ—¶é—´ç»æµ')
        traditional_chars = set('åœ‹ç™¼æœƒå­¸ç¿’è«–å•é¡Œæ¥­å°ˆé•·æ™‚é–“ç¶“æ¿Ÿ')
        
        simplified_count = sum(1 for char in text if char in simplified_chars)
        traditional_count = sum(1 for char in text if char in traditional_chars)
        total_chinese = len(re.findall(r'[\u4e00-\u9fff]', text))
        
        if total_chinese == 0:
            return 'unknown', 0.0
        
        simplified_ratio = simplified_count / total_chinese
        traditional_ratio = traditional_count / total_chinese
        
        if simplified_ratio > traditional_ratio:
            return 'simplified', simplified_ratio
        elif traditional_ratio > simplified_ratio:
            return 'traditional', traditional_ratio
        else:
            return 'mixed', max(simplified_ratio, traditional_ratio)
    
    def create_search_variants(self, query: str) -> List[str]:
        """å‰µå»ºç¹ç°¡æœç´¢è®Šé«”"""
        variants = [query]
        
        if not self.s2t_converter or not self.t2s_converter:
            return variants
        
        try:
            traditional = self.s2t_converter.convert(query)
            if traditional != query:
                variants.append(traditional)
            
            simplified = self.t2s_converter.convert(query)
            if simplified != query:
                variants.append(simplified)
        except Exception as e:
            logger.warning(f"å‰µå»ºæœç´¢è®Šé«”å¤±æ•—: {e}")
        
        return list(set(variants))

class EpubProcessor:
    """ğŸ“š EPUB æ–‡ä»¶è™•ç†å™¨"""
    
    def __init__(self):
        self.normalizer = ChineseTextNormalizer()
        
    def extract_epub_content(self, file_path: Path) -> str:
        """æå– EPUB å…§å®¹"""
        try:
            if not EPUB_AVAILABLE:
                raise ImportError("EPUB è™•ç†åº«æœªå®‰è£")
            
            print(f"ğŸ“š æ­£åœ¨è™•ç† EPUB æ–‡ä»¶: {file_path.name}")
            
            # è®€å– EPUB æ–‡ä»¶
            book = epub.read_epub(str(file_path))
            
            # æå–æ‰€æœ‰æ–‡æœ¬å…§å®¹
            content_parts = []
            chapter_count = 0
            
            # ç²å–æ›¸ç±ä¿¡æ¯
            title = book.get_metadata('DC', 'title')
            author = book.get_metadata('DC', 'creator')
            
            book_info = []
            if title:
                book_info.append(f"æ›¸å: {title[0][0] if title else 'æœªçŸ¥'}")
            if author:
                book_info.append(f"ä½œè€…: {author[0][0] if author else 'æœªçŸ¥'}")
            
            if book_info:
                content_parts.append("\n".join(book_info) + "\n\n")
            
            # æŒ‰é †åºè™•ç†ç« ç¯€
            spine_items = book.get_items_of_type(ebooklib.ITEM_DOCUMENT)
            
            for item in spine_items:
                try:
                    # è§£æ HTML å…§å®¹
                    soup = BeautifulSoup(item.get_content(), 'html.parser')
                    
                    # ç§»é™¤è…³æœ¬å’Œæ¨£å¼
                    for script in soup(["script", "style"]):
                        script.decompose()
                    
                    # æå–æ–‡æœ¬
                    text = soup.get_text()
                    
                    if text and len(text.strip()) > 50:  # éæ¿¾å¤ªçŸ­çš„å…§å®¹
                        # æ¸…ç†æ–‡æœ¬
                        cleaned_text = self._clean_epub_text(text)
                        
                        if cleaned_text.strip():
                            chapter_count += 1
                            
                            # æ·»åŠ ç« ç¯€æ¨™è¨˜
                            chapter_title = self._extract_chapter_title(soup, cleaned_text)
                            if chapter_title:
                                content_parts.append(f"\n\n=== {chapter_title} ===\n")
                            else:
                                content_parts.append(f"\n\n=== ç¬¬ {chapter_count} ç«  ===\n")
                            
                            content_parts.append(cleaned_text)
                            
                except Exception as e:
                    print(f"   âš ï¸ ç« ç¯€è™•ç†å¤±æ•—: {e}")
                    continue
            
            if not content_parts:
                raise ValueError("EPUB æ–‡ä»¶ä¸­æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆå…§å®¹")
            
            full_content = "".join(content_parts)
            
            print(f"   âœ… EPUB è™•ç†å®Œæˆ: {chapter_count} å€‹ç« ç¯€, {len(full_content):,} å­—ç¬¦")
            
            return full_content
            
        except Exception as e:
            print(f"   âŒ EPUB è™•ç†å¤±æ•—: {e}")
            raise
    
    def _clean_epub_text(self, text: str) -> str:
        """æ¸…ç† EPUB æ–‡æœ¬"""
        if not text:
            return ""
        
        # æ¨™æº–åŒ–æ–‡æœ¬
        normalized_text, _ = self.normalizer.normalize_text(text)
        
        # EPUB ç‰¹å®šæ¸…ç†
        epub_cleaning_rules = [
            (r'\n{4,}', '\n\n'),                    # é™åˆ¶é€£çºŒæ›è¡Œ
            (r'[ \t]{3,}', ' ' ),                    # é™åˆ¶é€£çºŒç©ºæ ¼
            (r'^[ \t]+', '', re.MULTILINE),          # ç§»é™¤è¡Œé¦–ç©ºç™½
            (r'[ \t]+$', '', re.MULTILINE),          # ç§»é™¤è¡Œå°¾ç©ºç™½
            (r'\n[ \t]*\n', '\n\n'),                # æ¸…ç†ç©ºè¡Œ
            (r'[\x00-\x08\x0B\x0C\x0E-\x1F]', ''),  # ç§»é™¤æ§åˆ¶å­—ç¬¦
        ]
        
        cleaned_text = normalized_text
        for pattern, replacement, *flags in epub_cleaning_rules:
            flag = flags[0] if flags else 0
            cleaned_text = re.sub(pattern, replacement, cleaned_text, flags=flag)
        
        return cleaned_text.strip()
    
    def _extract_chapter_title(self, soup, text: str) -> Optional[str]:
        """å˜—è©¦æå–ç« ç¯€æ¨™é¡Œ"""
        try:
            # å¾ HTML æ¨™ç±¤ä¸­æŸ¥æ‰¾æ¨™é¡Œ
            for tag in ['h1', 'h2', 'h3', 'title']:
                title_element = soup.find(tag)
                if title_element and title_element.get_text().strip():
                    title = title_element.get_text().strip()
                    if 5 <= len(title) <= 100:  # åˆç†çš„æ¨™é¡Œé•·åº¦
                        return title
            
            # å¾æ–‡æœ¬é–‹é ­æŸ¥æ‰¾æ¨™é¡Œ
            lines = text.split('\n')[:5]  # æª¢æŸ¥å‰5è¡Œ
            for line in lines:
                line = line.strip()
                if line and 5 <= len(line) <= 100:
                    # æª¢æŸ¥æ˜¯å¦åŒ…å«ç« ç¯€é—œéµè©
                    chapter_keywords = ['ç« ', 'Chapter', 'ç¬¬', 'å·', 'Part']
                    if any(keyword in line for keyword in chapter_keywords):
                        return line
                    # æˆ–è€…æ˜¯çŸ­è¡Œä¸”çœ‹èµ·ä¾†åƒæ¨™é¡Œ
                    elif len(line) <= 50 and not line.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
                        return line
            
            return None
            
        except Exception:
            return None

class SmartTextAnalyzer:
    """ğŸ§  æ™ºèƒ½æ–‡æœ¬åˆ†æå™¨"""
    
    def __init__(self):
        self.normalizer = ChineseTextNormalizer()
        self.token_estimator = AdvancedTokenEstimator()
        
    def analyze_text(self, text: str, source_info: Dict = None) -> TextAnalysis:
        """ç¶œåˆåˆ†ææ–‡æœ¬"""
        if not text:
            return TextAnalysis(
                length=0, text_type="empty", language="unknown",
                encoding="utf-8", structure_info={}, quality_score=0.0,
                processing_strategy="skip"
            )
        
        # æ¨™æº–åŒ–æ–‡æœ¬
        normalized_text, norm_info = self.normalizer.normalize_text(text)
        length = len(normalized_text)
        
        # åˆ†é¡æ–‡æœ¬é•·åº¦
        text_type = self._classify_text_length(length)
        
        # åˆ†ææ–‡æœ¬çµæ§‹
        structure_info = self._analyze_structure(normalized_text)
        
        # æª¢æ¸¬èªè¨€
        language = self._detect_language(normalized_text)
        
        # è©•ä¼°è³ªé‡
        quality_score = self._evaluate_quality(normalized_text, structure_info)
        
        # æ±ºå®šè™•ç†ç­–ç•¥
        processing_strategy = self._determine_strategy(text_type, structure_info, quality_score)
        
        return TextAnalysis(
            length=length,
            text_type=text_type,
            language=language,
            encoding=norm_info.get('encoding', 'utf-8'),
            structure_info=structure_info,
            quality_score=quality_score,
            processing_strategy=processing_strategy
        )
    
    def _classify_text_length(self, length: int) -> str:
        """åˆ†é¡æ–‡æœ¬é•·åº¦"""
        config = SMART_TEXT_CONFIG
        
        if length < config["micro_text_threshold"]:
            return "micro_text"
        elif length < config["short_text_threshold"]:
            return "short_text"
        elif length < config["medium_text_threshold"]:
            return "medium_text"
        elif length < config["long_text_threshold"]:
            return "long_text"
        elif length < config["ultra_long_threshold"]:
            return "ultra_long"
        else:
            return "mega_text"  # è¶…å¤§æ–‡æœ¬
    
    def _analyze_structure(self, text: str) -> Dict:
        """åˆ†ææ–‡æœ¬çµæ§‹"""
        structure_info = {
            'paragraphs': len(text.split('\n\n')),
            'lines': len(text.split('\n')),
            'sentences': len(re.findall(r'[ã€‚ï¼ï¼Ÿ.!?]+', text)),
            'has_chapters': False,
            'has_sections': False,
            'has_lists': False,
            'has_tables': False,
            'chapter_count': 0,
            'section_count': 0
        }
        
        # æª¢æ¸¬ç« ç¯€çµæ§‹
        chapter_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ',
            r'Chapter\s+\d+',
            r'\d+\.\s*[^\n]{1,50}\n'
        ]
        
        for pattern in chapter_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                structure_info['has_chapters'] = True
                structure_info['chapter_count'] = len(matches)
                break
        
        # æª¢æ¸¬å°ç¯€çµæ§‹
        section_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç¯€',
            r'\d+\.\d+',
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€'
        ]
        
        for pattern in section_patterns:
            matches = re.findall(pattern, text)
            if matches:
                structure_info['has_sections'] = True
                structure_info['section_count'] = len(matches)
                break
        
        # æª¢æ¸¬åˆ—è¡¨
        if re.search(r'^\s*[â€¢\-*]\s+', text, re.MULTILINE) or \
           re.search(r'^\s*\d+\.\s+', text, re.MULTILINE):
            structure_info['has_lists'] = True
        
        # æª¢æ¸¬è¡¨æ ¼
        if '|' in text and text.count('|') > 4:
            structure_info['has_tables'] = True
        
        return structure_info
    
    def _detect_language(self, text: str) -> str:
        """æª¢æ¸¬æ–‡æœ¬ä¸»è¦èªè¨€"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_chars = len(text)
        
        if total_chars == 0:
            return "unknown"
        
        chinese_ratio = chinese_chars / total_chars
        english_ratio = english_chars / total_chars
        
        if chinese_ratio > 0.3:
            if english_ratio > 0.2:
                return "mixed_zh_en"
            else:
                return "chinese"
        elif english_ratio > 0.5:
            return "english"
        else:
            return "mixed"
    
    def _evaluate_quality(self, text: str, structure_info: Dict) -> float:
        """è©•ä¼°æ–‡æœ¬è³ªé‡ï¼ˆ0-1åˆ†æ•¸ï¼‰"""
        score = 0.5  # åŸºç¤åˆ†æ•¸
        
        # é•·åº¦åˆç†æ€§ï¼ˆå¤ªçŸ­æˆ–å¤ªé•·éƒ½æ‰£åˆ†ï¼‰
        length = len(text)
        if 50 <= length <= 10000:
            score += 0.2
        elif length < 20:
            score -= 0.3
        
        # çµæ§‹å®Œæ•´æ€§
        if structure_info['paragraphs'] > 1:
            score += 0.1
        if structure_info['sentences'] > 2:
            score += 0.1
        
        # å…§å®¹è±å¯Œåº¦
        unique_chars = len(set(text))
        if unique_chars > 20:
            score += 0.1
        
        # é¿å…é‡è¤‡å…§å®¹
        lines = text.split('\n')
        unique_lines = len(set(lines))
        if len(lines) > 0:
            uniqueness_ratio = unique_lines / len(lines)
            score += uniqueness_ratio * 0.1
        
        return min(max(score, 0.0), 1.0)
    
    def _determine_strategy(self, text_type: str, structure_info: Dict, quality_score: float) -> str:
        """æ±ºå®šè™•ç†ç­–ç•¥"""
        if quality_score < 0.3:
            return "low_quality_skip"
        
        if text_type in ["micro_text", "short_text"]:
            return "whole_document"
        elif text_type == "medium_text":
            if structure_info['has_chapters'] or structure_info['paragraphs'] > 5:
                return "paragraph_aware"
            else:
                return "simple_split"
        elif text_type == "long_text":
            if structure_info['has_chapters']:
                return "chapter_aware"
            elif structure_info['has_sections']:
                return "section_aware"
            else:
                return "fine_split"
        elif text_type in ["ultra_long", "mega_text"]:
            return "hierarchical_split"
        else:
            # å›é€€ç­–ç•¥
            return "hierarchical_split"

class OptimizedTextSplitter:
    """ğŸ”§ å„ªåŒ–ç‰ˆæ–‡æœ¬åˆ†å‰²å™¨"""
    
    def __init__(self):
        self.analyzer = SmartTextAnalyzer()
        self.token_estimator = AdvancedTokenEstimator()
        self.config = SMART_TEXT_CONFIG
        
        # åˆ†å‰²å™¨å¯¦ä¾‹æ± 
        self._splitter_cache = {}
        
        print("ğŸ”§ å„ªåŒ–ç‰ˆæ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“ æ”¯æŒ {len(self.config)} ç¨®æ–‡æœ¬é¡å‹")
        print(f"   ğŸ§  æ™ºèƒ½ç­–ç•¥é¸æ“‡")
        print(f"   âš¡ æ€§èƒ½å„ªåŒ–")
    
    def get_splitter(self, chunk_size: int, chunk_overlap: int) -> RecursiveCharacterTextSplitter:
        """ç²å–åˆ†å‰²å™¨å¯¦ä¾‹ï¼ˆå¸¶å¿«å–ï¼‰"""
        cache_key = f"{chunk_size}_{chunk_overlap}"
        
        if cache_key not in self._splitter_cache:
            self._splitter_cache[cache_key] = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=[
                    "\n\nç¬¬", "\nç¬¬",      # ç« ç¯€æ¨™é¡Œ
                    "\n\n", "\n",          # æ®µè½
                    "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›",  # å¥å­çµæŸ
                    "ï¼Œ", "ã€",            # çŸ­èªåˆ†éš”
                    " ", ""
                ],
                keep_separator=True,
                length_function=len
            )
        
        return self._splitter_cache[cache_key]
    
    def smart_split_documents(self, text: str, doc_id: str, source_info: Dict = None) -> List[Document]:
        """ğŸ§  æ™ºèƒ½æ–‡æª”åˆ†å‰²ä¸»å…¥å£"""
        if not text or not text.strip():
            return []
        
        # åˆ†ææ–‡æœ¬
        analysis = self.analyzer.analyze_text(text, source_info)
        
        print(f"ğŸ“Š æ–‡æœ¬åˆ†æ: {analysis.text_type} ({analysis.length:,} å­—ç¬¦)")
        print(f"   ğŸ¯ è™•ç†ç­–ç•¥: {analysis.processing_strategy}")
        print(f"   ğŸ“ˆ è³ªé‡åˆ†æ•¸: {analysis.quality_score:.2f}")
        print(f"   ğŸ—£ï¸ ä¸»è¦èªè¨€: {analysis.language}")
        
        # æ ¹æ“šç­–ç•¥é¸æ“‡è™•ç†æ–¹æ³•
        documents = []
        if analysis.processing_strategy == "low_quality_skip":
            print("   âš ï¸ æ–‡æœ¬è³ªé‡éä½ï¼Œè·³éè™•ç†")
            return []
        elif analysis.processing_strategy == "whole_document":
            documents = self._process_whole_document(text, doc_id, analysis)
        elif analysis.processing_strategy == "paragraph_aware":
            documents = self._process_paragraph_aware(text, doc_id, analysis)
        elif analysis.processing_strategy == "simple_split":
            documents = self._process_simple_split(text, doc_id, analysis)
        elif analysis.processing_strategy == "chapter_aware":
            documents = self._process_chapter_aware(text, doc_id, analysis)
        elif analysis.processing_strategy == "section_aware":
            documents = self._process_section_aware(text, doc_id, analysis)
        elif analysis.processing_strategy == "fine_split":
            documents = self._process_fine_split(text, doc_id, analysis)
        elif analysis.processing_strategy == "hierarchical_split":
            documents = self._process_hierarchical_split(text, doc_id, analysis)
        else:
            # å›é€€åˆ°ç°¡å–®åˆ†å‰²
            documents = self._process_simple_split(text, doc_id, analysis)

        # æœ€çµ‚é©—è­‰å’Œéæ¿¾ï¼Œç¢ºä¿æ²’æœ‰ç©ºå…§å®¹çš„æ–‡æª”
        final_documents = [doc for doc in documents if doc.page_content and doc.page_content.strip()]
        if len(final_documents) != len(documents):
            logger.warning(f"éæ¿¾æ‰ {len(documents) - len(final_documents)} å€‹ç©ºå…§å®¹çš„å€«å¡Š")
        
        return final_documents
    
    def _process_whole_document(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """è™•ç†æ•´å€‹æ–‡æª”ï¼ˆä¸åˆ†å‰²ï¼‰"""
        config = self.config[analysis.text_type]
        
        if len(text.strip()) < config.get("min_length", 20):
            print(f"   âš ï¸ æ–‡æª”éçŸ­ï¼Œè·³éè™•ç†")
            return []
        
        print(f"   ğŸ“ {config['description']}")
        
        # è¨ˆç®— token æ•¸
        token_count = self.token_estimator.estimate_tokens(text)
        
        metadata = {
            'doc_id': doc_id,
            'chunk_id': f"{doc_id}_whole",
            'chunk_index': 0,
            'total_chunks': 1,
            'text_type': analysis.text_type,
            'processing_strategy': analysis.processing_strategy,
            'is_complete_document': True,
            'token_count': token_count,
            'quality_score': analysis.quality_score,
            'language': analysis.language,
            'structure_info': analysis.structure_info
        }
        
        return [Document(page_content=text, metadata=metadata)]
    
    def _process_paragraph_aware(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """æ®µè½æ„ŸçŸ¥è™•ç†"""
        config = self.config[analysis.text_type]
        chunk_size = config["chunk_size"]
        chunk_overlap = config["chunk_overlap"]
        
        print(f"   ğŸ“’ {config['description']} (ç›®æ¨™å¤§å°: {chunk_size})")
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        if not paragraphs:
            return self._process_simple_split(text, doc_id, analysis)
        
        documents = []
        current_chunk = ""
        chunk_index = 0
        
        for para_idx, paragraph in enumerate(paragraphs):
            # æª¢æŸ¥æ˜¯å¦å¯ä»¥åŠ å…¥ç•¶å‰åˆ†å¡Š
            potential_chunk = current_chunk + ("\n\n" if current_chunk else "") + paragraph
            
            if len(potential_chunk) <= chunk_size or not current_chunk:
                current_chunk = potential_chunk
            else:
                # ç•¶å‰åˆ†å¡Šå·²æ»¿ï¼Œå‰µå»ºæ–‡æª”
                if current_chunk:
                    doc = self._create_chunk_document(
                        current_chunk, doc_id, chunk_index, analysis, "paragraph_aware"
                    )
                    documents.append(doc)
                    chunk_index += 1
                
                current_chunk = paragraph
        
        # è™•ç†æœ€å¾Œä¸€å€‹åˆ†å¡Š
        if current_chunk:
            doc = self._create_chunk_document(
                current_chunk, doc_id, chunk_index, analysis, "paragraph_aware"
            )
            documents.append(doc)
        
        # è™•ç†é‡ç–Š
        if chunk_overlap > 0 and len(documents) > 1:
            documents = self._add_overlap_to_documents(documents, chunk_overlap)
        
        return documents
    
    def _process_simple_split(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """ç°¡å–®åˆ†å‰²è™•ç†"""
        config = self.config[analysis.text_type]
        chunk_size = config["chunk_size"]
        chunk_overlap = config["chunk_overlap"]
        
        print(f"   âœ‚ï¸ {config['description']} (å¤§å°: {chunk_size}, é‡ç–Š: {chunk_overlap})")
        
        splitter = self.get_splitter(chunk_size, chunk_overlap)
        chunks = splitter.split_text(text)
        
        documents = []
        for i, chunk in enumerate(chunks):
            if len(chunk.strip()) >= config.get("min_length", 20):
                doc = self._create_chunk_document(
                    chunk, doc_id, i, analysis, "simple_split"
                )
                documents.append(doc)
        
        return documents
    
    def _process_chapter_aware(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """ç« ç¯€æ„ŸçŸ¥è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   ğŸ“š {config['description']} - æª¢æ¸¬åˆ° {analysis.structure_info.get('chapter_count', 0)} å€‹ç« ç¯€")
        
        # æŒ‰ç« ç¯€åˆ†å‰²
        chapters = self._split_by_chapters(text)
        
        if len(chapters) <= 1:
            # æ²’æœ‰æª¢æ¸¬åˆ°ç« ç¯€ï¼Œå›é€€åˆ°æ®µè½æ„ŸçŸ¥
            return self._process_paragraph_aware(text, doc_id, analysis)
        
        all_documents = []
        
        for chapter_idx, chapter_text in enumerate(chapters):
            chapter_doc_id = f"{doc_id}_ch{chapter_idx+1:02d}"
            
            # æ¯å€‹ç« ç¯€å…§éƒ¨ä½¿ç”¨æ®µè½æ„ŸçŸ¥åˆ†å‰²
            chapter_analysis = analysis  # ä½¿ç”¨ç›¸åŒçš„åˆ†æçµæœ
            chapter_docs = self._process_paragraph_aware(chapter_text, chapter_doc_id, chapter_analysis)
            
            # æ·»åŠ ç« ç¯€ä¿¡æ¯åˆ°å…ƒæ•¸æ“š
            for doc in chapter_docs:
                doc.metadata.update({
                    'chapter_index': chapter_idx,
                    'chapter_total': len(chapters),
                    'parent_doc_id': doc_id,
                    'split_method': 'chapter_aware'
                })
            
            all_documents.extend(chapter_docs)
        
        return all_documents
    
    def _process_section_aware(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """å°ç¯€æ„ŸçŸ¥è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   ğŸ“‹ {config['description']} - æª¢æ¸¬åˆ° {analysis.structure_info.get('section_count', 0)} å€‹å°ç¯€")
        
        # æŒ‰å°ç¯€åˆ†å‰²
        sections = self._split_by_sections(text)
        
        if len(sections) <= 1:
            return self._process_simple_split(text, doc_id, analysis)
        
        all_documents = []
        
        for section_idx, section_text in enumerate(sections):
            section_doc_id = f"{doc_id}_sec{section_idx+1:02d}"
            
            # å°ç¯€å…§éƒ¨ç°¡å–®åˆ†å‰²
            section_docs = self._process_simple_split(section_text, section_doc_id, analysis)
            
            # æ·»åŠ å°ç¯€ä¿¡æ¯
            for doc in section_docs:
                doc.metadata.update({
                    'section_index': section_idx,
                    'section_total': len(sections),
                    'parent_doc_id': doc_id,
                    'split_method': 'section_aware'
                })
            
            all_documents.extend(section_docs)
        
        return all_documents
    
    def _process_fine_split(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """ç²¾ç´°åˆ†å‰²è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   ğŸ”¬ {config['description']}")
        
        return self._process_simple_split(text, doc_id, analysis)
    
    def _process_hierarchical_split(self, text: str, doc_id: str, analysis: TextAnalysis) -> List[Document]:
        """éšå±¤å¼åˆ†å‰²è™•ç†"""
        config = self.config[analysis.text_type]
        
        print(f"   ğŸ—ï¸ {config['description']}")
        
        # ç¬¬ä¸€å±¤ï¼šç« ç¯€åˆ†å‰²
        chapters = self._split_by_chapters(text)
        
        if len(chapters) > 1:
            return self._process_chapter_aware(text, doc_id, analysis)
        
        # ç¬¬äºŒå±¤ï¼šæ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        if len(paragraphs) > 10:  # æ®µè½è¼ƒå¤šï¼Œç”¨æ®µè½æ„ŸçŸ¥
            return self._process_paragraph_aware(text, doc_id, analysis)
        
        # ç¬¬ä¸‰å±¤ï¼šç²¾ç´°åˆ†å‰²
        return self._process_fine_split(text, doc_id, analysis)
    
    def _split_by_chapters(self, text: str) -> List[str]:
        """æŒ‰ç« ç¯€åˆ†å‰²æ–‡æœ¬"""
        chapter_patterns = [
            r'\n(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« [^\n]*)',
            r'\n(Chapter\s+\d+[^\n]*)',
            r'\n(\d+\.\s*[^\n]{5,50})\n',
            r'\n([A-Z][^\n]{10,50})\n(?=[A-Z]|$)'
        ]
        
        best_split = None
        best_count = 0
        
        for pattern in chapter_patterns:
            matches = list(re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE))
            
            if len(matches) >= 2:
                splits = []
                start = 0
                
                for match in matches:
                    if start < match.start():
                        splits.append(text[start:match.start()].strip())
                    start = match.start()
                
                splits.append(text[start:].strip())
                splits = [s for s in splits if s and len(s) > 50]
                
                if len(splits) > best_count:
                    best_split = splits
                    best_count = len(splits)
        
        return best_split if best_split else [text]
    
    def _split_by_sections(self, text: str) -> List[str]:
        """æŒ‰å°ç¯€åˆ†å‰²æ–‡æœ¬"""
        section_patterns = [
            r'\n(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç¯€[^\n]*)',
            r'\n(\d+\.\d+[^\n]*)',
            r'\n([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€[^\n]*)',
            r'\n([A-Z]\.[^\n]*)'
        ]
        
        for pattern in section_patterns:
            matches = list(re.finditer(pattern, text, re.MULTILINE))
            
            if len(matches) >= 2:
                splits = []
                start = 0
                
                for match in matches:
                    if start < match.start():
                        splits.append(text[start:match.start()].strip())
                    start = match.start()
                
                splits.append(text[start:].strip())
                return [s for s in splits if s and len(s) > 30]
        
        return [text]
    
    def _add_overlap_to_documents(self, documents: List[Document], overlap_size: int) -> List[Document]:
        """ç‚ºæ–‡æª”æ·»åŠ é‡ç–Šéƒ¨åˆ†"""
        if len(documents) <= 1:
            return documents
        
        for i in range(1, len(documents)):
            # å¾å‰ä¸€å€‹æ–‡æª”æœ«å°¾å–é‡ç–Šå…§å®¹
            prev_content = documents[i-1].page_content
            current_content = documents[i].page_content
            
            if len(prev_content) > overlap_size:
                overlap_text = prev_content[-overlap_size:]
                # å°‹æ‰¾å®Œæ•´çš„å¥å­é‚Šç•Œ
                sentence_end = overlap_text.rfind('ã€‚')
                if sentence_end == -1:
                    sentence_end = overlap_text.rfind('.')
                
                if sentence_end > overlap_size // 2:
                    overlap_text = overlap_text[sentence_end+1:]
                
                documents[i].page_content = overlap_text + "\n" + current_content
                documents[i].metadata['has_overlap'] = True
                documents[i].metadata['overlap_length'] = len(overlap_text)
        
        return documents
    
    def _create_chunk_document(self, content: str, doc_id: str, chunk_index: int, 
                          analysis: TextAnalysis, split_method: str) -> Document:
        """å‰µå»ºåˆ†å¡Šæ–‡æª” - çµ±ä¸€å…ƒæ•¸æ“šæ ¼å¼"""
        token_count = self.token_estimator.estimate_tokens(content)
        
        # æœå°‹URL
        url_regex = r'https?://[\w\-./?#&%=]+'
        found_urls = re.findall(url_regex, content)

        # åŸºæœ¬å…ƒæ•¸æ“šï¼ˆç¢ºä¿éƒ½æ˜¯ç°¡å–®é¡å‹ï¼‰
        metadata = {
            'doc_id': str(doc_id),
            'chunk_id': f"{doc_id}_{chunk_index+1:03d}",
            'chunk_index': int(chunk_index),
            'text_type': str(analysis.text_type),
            'processing_strategy': str(analysis.processing_strategy),
            'split_method': str(split_method),
            'chunk_length': int(len(content)),
            'token_count': int(token_count),
            'quality_score': float(analysis.quality_score),
            'language': str(analysis.language),
            'has_overlap': False
        }

        # ğŸ¯ æ­£ç¢ºæ–¹æ¡ˆï¼šURLåˆ—è¡¨ â†’ åˆ†éš”ç¬¦å­—ä¸² (é€™æ˜¯å”¯ä¸€å¯è¡Œçš„æ–¹æ¡ˆ)
        if found_urls:
            metadata['contained_urls'] = '|'.join(found_urls)  # âœ… å­—ç¬¦ä¸²é¡å‹
            metadata['url_count'] = len(found_urls)           # âœ… æ•´æ•¸é¡å‹
            metadata['has_urls'] = True                       # âœ… å¸ƒæ—é¡å‹
        else:
            metadata['contained_urls'] = ''                   # âœ… ç©ºå­—ç¬¦ä¸²
            metadata['url_count'] = 0                        # âœ… æ•´æ•¸ 0
            metadata['has_urls'] = False                     # âœ… å¸ƒæ— False

        # ğŸ¯ æ­£ç¢ºæ–¹æ¡ˆï¼šè¤‡é›œçµæ§‹ â†’ JSONå­—ä¸²
        if analysis.structure_info:
            metadata['structure_info'] = json.dumps(analysis.structure_info, ensure_ascii=False)  # âœ… JSONå­—ç¬¦ä¸²
            # åŒæ™‚ä¿ç•™é—œéµä¿¡æ¯ä½œç‚ºç°¡å–®å­—æ®µï¼ˆä¾¿æ–¼æŸ¥è©¢ï¼‰
            metadata['has_chapters'] = bool(analysis.structure_info.get('has_chapters', False))    # âœ… å¸ƒæ—
            metadata['has_sections'] = bool(analysis.structure_info.get('has_sections', False))    # âœ… å¸ƒæ—  
            metadata['paragraph_count'] = int(analysis.structure_info.get('paragraphs', 0))       # âœ… æ•´æ•¸
            metadata['sentence_count'] = int(analysis.structure_info.get('sentences', 0))         # âœ… æ•´æ•¸
        else:
            metadata['structure_info'] = '{}'        # âœ… ç©ºJSONå­—ç¬¦ä¸²
            metadata['has_chapters'] = False         # âœ… å¸ƒæ—
            metadata['has_sections'] = False         # âœ… å¸ƒæ—
            metadata['paragraph_count'] = 0          # âœ… æ•´æ•¸
            metadata['sentence_count'] = 0           # âœ… æ•´æ•¸
        
        return Document(page_content=content, metadata=metadata)