import time
import json
import shutil
import time
import hashlib
import re
import logging
import os
import gc
import threading
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set, Any, Union
from dataclasses import dataclass, asdict
import random
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import tempfile
# ğŸ”§ å¼·åˆ¶è¼‰å…¥ç’°å¢ƒè®Šæ•¸
from dotenv import load_dotenv
from database_adapter import DatabaseFactory, PostgreSQLAdapter
load_dotenv('.env', override=True)
logger = logging.getLogger(__name__)

import urllib.parse
from urllib.parse import quote_plus

import sys
from pathlib import Path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
from config import app_config  # â­ çµ±ä¸€å°å…¥



# ğŸ†• 1. æ·»åŠ  Railway ç’°å¢ƒæª¢æ¸¬å‡½æ•¸ï¼ˆæ”¾åœ¨æ–‡ä»¶é ‚éƒ¨ï¼Œå°å…¥èªå¥ä¹‹å¾Œï¼‰
def detect_railway_environment():
    """æª¢æ¸¬æ˜¯å¦åœ¨ Railway ç’°å¢ƒä¸­é‹è¡Œ"""
    railway_indicators = [
        os.getenv("RAILWAY_PROJECT_ID"),
        os.getenv("RAILWAY_SERVICE_ID"), 
        os.getenv("DATABASE_URL"),
        "railway.internal" in os.getenv("POSTGRES_HOST", "")
    ]
    
    is_railway = any(railway_indicators)
    if is_railway:
        print("ğŸš‚ æª¢æ¸¬åˆ° Railway éƒ¨ç½²ç’°å¢ƒ")
        # é¡¯ç¤º Railway ç‰¹å®šä¿¡æ¯
        project_id = os.getenv("RAILWAY_PROJECT_ID", "unknown")
        service_id = os.getenv("RAILWAY_SERVICE_ID", "unknown")  
        print(f"   é …ç›®ID: {project_id[:8]}***")
        print(f"   æœå‹™ID: {service_id[:8]}***")
    
    return is_railway


# ğŸ”§ å„ªåŒ–ç‰ˆç³»çµ±é…ç½®
SYSTEM_CONFIG = {
    "persist_dir": "./chroma_langchain_db",
    "data_dir": "./data", 
    "device": "cpu",  
    "log_level": "INFO",
    "max_workers": 4,  # ä¸¦ç™¼è™•ç†æ•¸
    "cache_embeddings": True,  # å¿«å–åµŒå…¥å‘é‡
    "backup_enabled": True  # è‡ªå‹•å‚™ä»½
}

# ğŸ§  æ™ºèƒ½æ–‡æœ¬åˆ†é¡é…ç½®
SMART_TEXT_CONFIG = {
    # æ–‡æœ¬é•·åº¦é–¾å€¼ï¼ˆå­—ç¬¦æ•¸ï¼‰
    "micro_text_threshold": 100,      # å¾®æ–‡æœ¬ï¼šæ¨™é¡Œã€æ‘˜è¦ç­‰
    "short_text_threshold": 500,      # çŸ­æ–‡æœ¬ï¼šçŸ­æ–‡ç« ã€æ®µè½
    "medium_text_threshold": 2000,    # ä¸­ç­‰æ–‡æœ¬ï¼šä¸­ç¯‡æ–‡ç« 
    "long_text_threshold": 8000,      # é•·æ–‡æœ¬ï¼šé•·ç¯‡æ–‡ç« 
    "ultra_long_threshold": 20000,    # è¶…é•·æ–‡æœ¬ï¼šæ›¸ç±ç« ç¯€
    
    # å„é¡æ–‡æœ¬çš„è™•ç†ç­–ç•¥
    "micro_text": {
        "min_length": 20,
        "process_whole": True,
        "merge_with_next": True,  # å˜—è©¦èˆ‡ä¸‹ä¸€å€‹ç‰‡æ®µåˆä½µ
        "chunk_size": 0,
        "description": "å¾®æ–‡æœ¬æ•´é«”è™•ç†"
    },
    
    "short_text": {
        "min_length": 50,
        "process_whole": True,
        "preserve_structure": True,
        "chunk_size": 0,
        "allow_merge": True,  # å…è¨±åˆä½µç›¸é„°çŸ­æ–‡æœ¬
        "description": "çŸ­æ–‡æœ¬æ•´é«”ä¿å­˜"
    },
    
    "medium_text": {
        "chunk_size": 800,
        "chunk_overlap": 120,
        "preserve_paragraphs": True,
        "smart_boundaries": True,  # æ™ºèƒ½é‚Šç•Œæª¢æ¸¬
        "min_chunk_ratio": 0.3,   # æœ€å°ç‰‡æ®µæ¯”ä¾‹
        "description": "ä¸­ç­‰æ–‡æœ¬æ®µè½æ„ŸçŸ¥åˆ†å‰²"
    },
    
    "long_text": {
        "chunk_size": 500,
        "chunk_overlap": 80,
        "hierarchical_split": True,
        "section_aware": True,    # ç« ç¯€æ„ŸçŸ¥
        "quality_check": True,    # è³ªé‡æª¢æŸ¥
        "description": "é•·æ–‡æœ¬ç²¾ç´°åŒ–åˆ†å‰²"
    },
    
    "ultra_long": {
        "chunk_size": 600,
        "chunk_overlap": 100,
        "multi_level_split": True,  # å¤šå±¤ç´šåˆ†å‰²
        "chapter_detection": True,  # ç« ç¯€æª¢æ¸¬
        "summary_chunks": True,     # ç”Ÿæˆæ‘˜è¦ç‰‡æ®µ
        "description": "è¶…é•·æ–‡æœ¬éšå±¤å¼è™•ç†"
    },
    
    "mega_text": {
        "chunk_size": 500,
        "chunk_overlap": 80,
        "multi_level_split": True,  # å¤šå±¤ç´šåˆ†å‰²
        "chapter_detection": True,  # ç« ç¯€æª¢æ¸¬
        "aggressive_split": True,   # ç©æ¥µåˆ†å‰²
        "quality_filter": True,     # è³ªé‡éæ¿¾
        "description": "è¶…å¤§æ–‡æœ¬ç©æ¥µåˆ†å‰²è™•ç†"
    }
}

# ğŸ”§ å„ªåŒ–ç‰ˆ Token é™åˆ¶é…ç½®
TOKEN_LIMITS = {
    "max_tokens_per_request": 150000,  # æ›´ä¿å®ˆçš„é™åˆ¶
    "max_batch_size": 8,               # æ¸›å°æ‰¹æ¬¡å¤§å°
    "min_batch_size": 1,
    "batch_delay": 5.0,                # å¢åŠ æ‰¹æ¬¡é–“å»¶é²
    "retry_delay": 10,
    "max_retries": 3,
    "token_safety_margin": 0.2,        # 20% å®‰å…¨é‚Šéš›
    "adaptive_batching": True           # è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°
}

# ğŸ”§ æ€§èƒ½å„ªåŒ–é…ç½®
PERFORMANCE_CONFIG = {
    "embedding_batch_size": 12,
    "parallel_processing": True,
    "memory_limit_mb": 1024,      # è¨˜æ†¶é«”é™åˆ¶
    "chunk_cache_size": 1000,     # åˆ†å¡Šå¿«å–å¤§å°
    "preload_models": True,       # é è¼‰å…¥æ¨¡å‹
    "gc_frequency": 50            # åƒåœ¾å›æ”¶é »ç‡
}

# æ”¯æ´çš„æ–‡ä»¶æ ¼å¼æ“´å±•
SUPPORTED_EXTENSIONS = {
    '.txt', '.md', '.pdf', '.csv', '.json', '.py', '.js', 
    '.docx', '.doc', '.epub', '.rst', '.org'
}

from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from langchain_community.vectorstores import Chroma

try:
    from langchain_community.vectorstores import Chroma
    CHROMA_AVAILABLE = True
except ImportError:
    CHROMA_AVAILABLE = False
    # å‰µå»ºä¸€å€‹è™›æ“¬çš„ Chroma é¡å‹ç”¨æ–¼é¡å‹è¨»è§£
    class Chroma:
        pass




# ğŸ”§ æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
def check_openai_api_key():
    """æª¢æŸ¥ OpenAI API Key"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸  æœªè¨­ç½® OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
        print("   è«‹åœ¨ .env æ–‡ä»¶ä¸­è¨­ç½®: OPENAI_API_KEY=sk-your-api-key")
        return False
    
    if api_key.startswith("sk-") and len(api_key) > 20:
        print("âœ… OpenAI API Key æ ¼å¼æ­£ç¢º")
        print(f"   API Key: {api_key[:8]}***{api_key[-4:]}")
        return True
    else:
        print("âš ï¸  OpenAI API Key æ ¼å¼å¯èƒ½ä¸æ­£ç¢º")
        return False

# LangChain æ ¸å¿ƒçµ„ä»¶å°å…¥
try:
    from langchain_postgres import PGVector
    PGVECTOR_AVAILABLE = True
    print("âœ… PGVector å¯ç”¨")
except ImportError:
    try:
        from langchain_community.vectorstores import PGVector
        PGVECTOR_AVAILABLE = True
        print("âœ… PGVector (community) å¯ç”¨")
    except ImportError:
        PGVECTOR_AVAILABLE = False
        print("âŒ PGVector ä¸å¯ç”¨")
        # å¯ä»¥å›é€€åˆ° Chroma
        from langchain_community.vectorstores import Chroma

# OpenAI embeddings å°å…¥
try:
    from langchain_openai import OpenAIEmbeddings
    OPENAI_EMBEDDINGS_AVAILABLE = True
    print("âœ… OpenAI Embeddings å¯ç”¨")
    
    if check_openai_api_key():
        print("âœ… OpenAI ç’°å¢ƒæª¢æŸ¥é€šé")
    else:
        print("âš ï¸ OpenAI ç’°å¢ƒé…ç½®å¯èƒ½æœ‰å•é¡Œ")
        
except ImportError as e:
    OPENAI_EMBEDDINGS_AVAILABLE = False
    print(f"âš ï¸ OpenAI Embeddings ä¸å¯ç”¨: {e}")

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    TextLoader, PyPDFLoader, Docx2txtLoader, 
    CSVLoader, JSONLoader
)

# ğŸ”§ PostgreSQL ä¾è³´æª¢æŸ¥
try:
    import psycopg2
    PSYCOPG2_AVAILABLE = True
except ImportError:
    PSYCOPG2_AVAILABLE = False
    print("âš ï¸ è­¦å‘Š: psycopg2 æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install psycopg2-binary")
